{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome \ud83d\ude00 This site contains self-attempted solutions to exercises in the great textbook The Elements of Statistical Learning by Prof. Trevor Hastie , Prof. Robert Tibshirani and Prof. Jerome Friedman . May I ask you for a Github \u2b50 if you find this repo helpful? Usage \u00b6 Start browsing here . To quickly navigate to a particular solution, please search exercise number in \ud83d\udd0d in the top. Version \u00b6 The version of the textbook this site is using is Second Edition (corrected 12th printing Jan 2017) and book PDF can be downloaded here . Programming Language \u00b6 For programming exercises, relevant Python scripts have been provided. Similar Resources \u00b6 An excellent solution manual with notes from John L. Weatherwax, PhD and Prof. David Epstein . A chinese version of ESL by Wei Ya. Contributing \u00b6 Due to my limited ability, my solutions are not complete and may contain errors. Please feel free to create new Issues for discussion. Thank you in advance \ud83d\ude00","title":"Home"},{"location":"#introduction","text":"Welcome \ud83d\ude00 This site contains self-attempted solutions to exercises in the great textbook The Elements of Statistical Learning by Prof. Trevor Hastie , Prof. Robert Tibshirani and Prof. Jerome Friedman . May I ask you for a Github \u2b50 if you find this repo helpful?","title":"Introduction"},{"location":"#usage","text":"Start browsing here . To quickly navigate to a particular solution, please search exercise number in \ud83d\udd0d in the top.","title":"Usage"},{"location":"#version","text":"The version of the textbook this site is using is Second Edition (corrected 12th printing Jan 2017) and book PDF can be downloaded here .","title":"Version"},{"location":"#programming-language","text":"For programming exercises, relevant Python scripts have been provided.","title":"Programming Language"},{"location":"#similar-resources","text":"An excellent solution manual with notes from John L. Weatherwax, PhD and Prof. David Epstein . A chinese version of ESL by Wei Ya.","title":"Similar Resources"},{"location":"#contributing","text":"Due to my limited ability, my solutions are not complete and may contain errors. Please feel free to create new Issues for discussion. Thank you in advance \ud83d\ude00","title":"Contributing"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-1/","text":"Ex. 2.1 Suppose each of \\(K\\) -classes has associated target \\(t_k\\) , which is a vector of all zeros, except a one in the \\(k\\) -th position. Show that classifying to the largest of \\(\\hat y\\) amounts to choosing the closet target, \\(\\min_k\\|t_k-\\hat y\\|\\) , if the elements of \\(\\hat y\\) sum to one. Soln. 2.1 We need to prove: \\[\\begin{equation} \\underset{k}{\\operatorname{argmax}} \\hat y_k = \\underset{k}{\\operatorname{argmin}} \\|t_k-\\hat y\\|^2 \\label{eq:2-1a} \\end{equation}\\] By definition of \\(t_k\\) , we have \\[\\begin{align} \\|t_k-\\hat y\\|^2 &= (1-\\hat y_k)^2 + \\sum_{l \\neq k }(0 - \\hat y_l)^2\\nonumber\\\\ &= (1-\\hat y_k)^2 + \\sum_{l \\neq k }\\hat y_l^2\\nonumber\\\\ &= 1 - 2\\hat y_k + \\sum\\hat y_l^2 \\label{eq:2-1b} \\end{align}\\] Given \\(\\eqref{eq:2-1b}\\) , it's straightforward to see that \\(\\eqref{eq:2-1a}\\) indeed holds because only \\(-2\\hat y_k\\) depends on \\(k\\) . Remark The assumption \\(\\sum_{k=1}^K\\hat y_k=1\\) is actually not required.","title":"Ex. 2.1"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-2/","text":"Ex. 2.2 Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5. Soln. 2.2 Let's first recall how the data is generated (starting from the bottom of page 16 in the text). First we generated 10 means \\(m_k\\) from a bivariate Gaussian \\(N((1,0)^T, \\textbf{I})\\) and labeled this class BLUE. Similarly we generate 10 more means, denoted as \\(o_k\\) , from \\(N((0,1)^T, \\textbf{I})\\) and labeled this class ORANGE. We regard \\(m_k\\) and \\(o_k\\) as fixed for this problem. Next, for each color (class), we generate 100 observations in the following way. For each observation, we picked an \\(m_k\\) ( \\(o_k\\) , respectively) at random with probability \\(1/10\\) , and generate a variable with distribution \\(N(m_k, \\textbf{I}/5)\\) ( \\(N(o_k, \\textbf{I}/5)\\) , respectively), thus leading to a mixture of Gaussian clusters for each class. Therefore we have \\[\\begin{equation} \\label{eq:2-2blue} P(X=x|\\text{BLUE}) = \\sum_{i=1}^{10}\\frac{1}{10}\\phi(x;m_i, \\textbf{I}/5)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\end{equation}\\] where \\(\\phi\\) is the density of \\(N(m_i, \\textbf{I}/5)\\) . Similarly \\[\\begin{equation} \\label{eq:2-2organge} P(X=x|\\text{ORANGE}) = \\sum_{i=1}^{10}\\frac{1}{10}\\phi(x;o_i, \\textbf{I}/5)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\end{equation}\\] The Bayes decision boundary is determined by \\[\\begin{equation} \\label{eq:2-2bound} P\\left(\\text{BLUE}|X=x\\right) = P\\left(\\text{ORANGE}|X=x\\right)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\end{equation}\\] From Bayes formula we have \\[\\begin{eqnarray} P\\left(\\text{BLUE}|X=x\\right) &=& \\frac{P\\left(\\text{BLUE}, X=x\\right)}{P(X=x)}\\nonumber\\\\ &=&\\frac{P(X=x|\\text{BLUE})P(\\text{BLUE})}{P(X=x)}.\\nonumber \\end{eqnarray}\\] Similarly we have \\[\\begin{eqnarray} P\\left(\\text{ORANGE}|X=x\\right) &=&\\frac{P(X=x|\\text{ORANGE})P(\\text{ORANGE})}{P(X=x)}.\\nonumber \\end{eqnarray}\\] There the boundary equation \\(\\eqref{eq:2-2bound}\\) reduces to \\[\\begin{equation} P(X=x|\\text{BLUE})P(\\text{BLUE}) = P(X=x|\\text{ORANGE})P(\\text{ORANGE}).\\nonumber \\end{equation}\\] Note that in this example \\(P(\\text{BLUE}) = P(\\text{ORANGE}) = 1/2\\) , so we have \\[\\begin{equation} P(X=x|\\text{BLUE}) = P(X=x|\\text{ORANGE}).\\nonumber \\end{equation}\\] Recall \\(\\eqref{eq:2-2blue}\\) - \\(\\eqref{eq:2-2organge}\\) , it's now easy to see how to calculate the Bayes decision boundary.","title":"Ex. 2.2"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-3/","text":"Ex. 2.3 Derive equation (2.24). Soln. 2.3 Let \\(\\nu_p\\cdot r^p\\) be the volume of the sphere of radius \\(r\\) in \\(p\\) dimension. Consider the unit ball and a point \\(a\\) uniformly sampled from it. The probability that \\(a\\) falls outside of the superball \\(b\\) which centers at origin and has radius \\(0<r<1\\) is \\[\\begin{equation} \\frac{\\nu_p\\cdot 1^p-\\nu_p\\cdot r^p}{\\nu_p\\cdot 1^p} = 1-r^p.\\nonumber \\end{equation}\\] Now for \\(N\\) independently and uniformly distributed data points, the probability of the point that is the closest to the origin falls outside of the superball \\(b\\) is \\[\\begin{equation} (1- r^p)^N.\\nonumber \\end{equation}\\] To find the median of the radius of the closest point of origin, we set the probability above equal to \\(\\frac{1}{2}\\) : \\[\\begin{equation} (1-d(p,N)^p)^N = \\frac{1}{2}.\\nonumber \\end{equation}\\] Solving for \\(d(p,N)\\) we get \\[\\begin{equation} d(p, N) = \\left(1-\\frac{1}{2}^{1/N}\\right)^{1/p}.\\nonumber \\end{equation}\\]","title":"Ex. 2.3"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-4/","text":"Ex. 2.4 The edge effect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution \\(X\\sim N(0, \\textbf{I}_p)\\) . The squared distance from any sample point to the origin has a \\(\\chi_p^2\\) distribution with mean \\(p\\) . Consider a prediction point \\(x_0\\) drawn from this distribution, and let \\(a=x_0/\\|x_0\\|\\) be an associated unit vector. Let \\(z_i=a^Tx_i\\) be the projection of each of the training points on this direction. Show that the \\(z_i\\) are distributed \\(N(0,1)\\) with expected squared distance from origin 1, while the target point has expected squared distance \\(p\\) from the origin. Hence for \\(p=10\\) , a randomly drawn test point is about 3.1 standard deviations from the origin, while all the training points are on average one standard deviation along direction \\(a\\) . So most prediction points see themselves as lying on the edge of the training set. Soln. 2.4 Since \\(z_i = a^Tx_i\\) , \\(z_i\\) is a linear combination of standard normal random variables, thus \\(z_i\\) is normal. It's easy to see that \\(E[z_i] = 0\\) and \\[\\begin{equation} \\text{Var}(z_i) = \\|a\\|^2\\text{Var}(x_i) = \\text{Var}(x_i) = 1.\\nonumber \\end{equation}\\] There we know \\(z_i\\sim N(0,1)\\) and the expected squared distance from origin is just its variance, which is 1. As for the target point \\(x_t\\) , its squared distance to origin follows a \\(\\chi_p^2\\) distribution and thus has mean \\(p\\) . For \\(p=10\\) , we have \\[\\begin{equation} \\text{sd}(x_t) = \\sqrt{\\text{Var}(x_t)} = \\sqrt{10} \\approx 3.16.\\nonumber \\end{equation}\\]","title":"Ex. 2.4"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-5/","text":"Ex. 2.5 (a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument. (b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace( \\(AB\\) ) = trace( \\(BA\\) )], and its linearity (which allows us to interchange the order of trace and expectation). Soln. 2.5 (a) Since \\(X\\) and \\(\\epsilon\\) are independent, we have \\(E_{\\mathcal{T}}(\\epsilon) = 0\\) . Since \\(\\hat \\beta = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^Ty = \\beta + (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\epsilon\\) , we have \\(E_{\\mathcal{T}}(\\hat \\beta) = \\beta\\) and \\[\\begin{equation} \\label{eq:2-5a} \\text{Var}_\\mathcal{T}(\\hat\\beta) = E_\\mathcal{T}(\\hat \\beta^T\\hat\\beta) = E_\\mathcal{T}(\\textbf{X}^T\\textbf{X})^{-1}\\sigma^2.\\ \\ \\ \\ \\ \\ \\ \\end{equation}\\] Note that \\(y_0\\) is constant for the distribution \\(\\mathcal{T}\\) . We have \\[\\begin{eqnarray} E_\\mathcal{T}(y_0-\\hat y_0)^2 &=& y_0^2 + E_\\mathcal{T}\\hat y_0^2 - 2 y_0E_\\mathcal{T}\\hat y_0\\nonumber\\\\ &=& [y_0^2-E_{y_0|x_0}(y_0)^2] + [E_\\mathcal{T}\\hat y_0^2 - (E_\\mathcal{T}\\hat y_0)^2]\\nonumber\\\\ && + [ (E_\\mathcal{T}\\hat y_0)^2 - 2 y_0E_\\mathcal{T}\\hat y_0 + E_{y_0|x_0}(y_0)^2 ]\\nonumber\\\\ &=& [y_0^2-E_{y_0|x_0}(y_0)^2] + [E_\\mathcal{T}\\hat y_0^2 - (E_\\mathcal{T}\\hat y_0)^2]\\nonumber\\\\ && + (E_\\mathcal{T}\\hat y_0 - y_0)^2\\nonumber \\end{eqnarray}\\] Therefore, by \\(\\eqref{eq:2-5a}\\) , we have \\[\\begin{eqnarray} E_{y_0|x_0}E_\\mathcal{T}(y_0-\\hat y_0)^2 &=&\\text{Var}(y_0|x_0) + \\text{Var}_\\mathcal{T}(\\hat y_0) + \\text{Bias}^2(\\hat y_0)\\nonumber\\\\ &=&\\sigma^2 + E_\\mathcal{T} x_0^T(\\textbf{X}^T\\textbf{X})^{-1}x_0\\sigma^2 + 0^2.\\nonumber \\end{eqnarray}\\] (b) First, we have \\[\\begin{equation} E_{x_0}\\text{EPE}(x_0) \\sim E_{x_0}x_0^T\\text{Cov}(X)^{-1}x_0\\sigma^2/N + \\sigma^2.\\nonumber \\end{equation}\\] Note that \\(x_0^T\\text{Cov}(X)^{-1}x_0\\) is scalar and equal to its own trace, we have \\[\\begin{eqnarray} E_{x_0}\\text{EPE}(x_0) &\\sim& E_{x_0}x_0^T\\text{Cov}(X)^{-1}x_0\\sigma^2/N + \\sigma^2\\nonumber\\\\ &=&\\text{trace}\\left(E_{x_0}x_0x_0^T\\text{Cov}(X)^{-1}\\right)\\sigma^2/N + \\sigma^2\\nonumber\\\\ &=&\\text{trace}\\left(\\textbf{I}_p\\right)\\sigma^2/N + \\sigma^2\\nonumber\\\\ &=&\\sigma^2(p/N) + \\sigma^2.\\nonumber \\end{eqnarray}\\]","title":"Ex. 2.5"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-6/","text":"Ex. 2.6 Consider a regression problem with inputs \\(x_i\\) and outputs \\(y_i\\) , and a parameterized model \\(f_\\theta(x)\\) to be fit by least squares. Show that if there are observations with tied or identical values of \\(x\\) , then the fit can be obtained from a reduced weighted least squares problem. Soln. 2.6 We use the notation in the text on page 32, consider multiple observation pairs \\(x_i, y_{il}, l=1,...,N_i\\) at each values of \\(x_i\\) for \\(i=1,...,N\\) . Our goal is to minimize \\[\\begin{equation} \\text{RSS}(\\theta) = \\sum_{i=1}^N\\sum_{l=1}^{N_i}(y_{il} - f_\\theta(x_i))^2.\\nonumber \\end{equation}\\] Let \\(\\bar y_i = \\frac{1}{N_i}\\sum_{l=1}^{N_i}y_{il}\\) be the average of \\(y_{ij}\\) for \\(i\\) th class. Expanding RSS above we get \\[\\begin{eqnarray} \\text{RSS}(\\theta) &=& \\sum_{i=1}^N\\sum_{l=1}^{N_i}(y_{il}^2 - 2 y_{il} f_\\theta(x_i) + f_\\theta(x_i)^2)\\nonumber\\\\ &=&\\sum_{i=1}^N N_i\\left(\\frac{\\sum_{l=1}^{N_i}y_{il}^2}{N_i} -2\\bar y_if_\\theta(x_i)+ f_\\theta(x_i)^2\\right)\\nonumber\\\\ &=&\\sum_{i=1}^N N_i\\left(\\bar y_i - f_\\theta(x_i)\\right)^2 + \\text{Terms independent of }\\theta.\\nonumber \\end{eqnarray}\\] Therefore, we are essentially minimizing \\[\\begin{equation} \\text{RSS}(\\theta)' = \\sum_{i=1}^N N_i\\left(\\bar y_i - f_\\theta(x_i)\\right)^2,\\nonumber \\end{equation}\\] which is known as a weighted least squares problem.","title":"Ex. 2.6"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-7/","text":"Ex. 2.7 Suppose we have a sample of \\(N\\) pairs \\(x_i, y_i\\) drawn i.i.d. from the distribution characterized as follows: \\[\\begin{eqnarray} &&x_i\\sim h(x), \\text{ the design density}\\nonumber\\\\ &&y_i = f(x_i) + \\epsilon_i, f \\text{ is the regression function}\\nonumber\\\\ &&\\epsilon_i\\sim (0, \\sigma^2)\\ (\\text{mean zero, variance } \\sigma^2)\\nonumber \\end{eqnarray}\\] We construct an estimate for \\(f\\) linear in the \\(y_i\\) , \\[\\begin{equation} \\label{eq:2-7linear} \\hat f(x_0) = \\sum_{i=1}^N\\ell_i(x_0;\\mathcal{X})y_i, \\end{equation}\\] where the weights \\(\\ell_i(x_0;\\mathcal{X})\\) do not depend on the \\(y_i\\) , but do depend on the entire training sequence of \\(x_i\\) , denoted here by \\(\\mathcal{X}\\) . (a) Show that linear regression and \\(k\\) -nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights \\(\\ell_i(x_0;\\mathcal{X})\\) in each of those cases. (b) Decompose the conditional mean-squared error \\[\\begin{equation} E_{\\mathcal{Y}|\\mathcal{X}}(f(x_0)-\\hat f(x_0))^2\\nonumber \\end{equation}\\] into a conditional squared bias and a conditional variance component. Like \\(\\mathcal{X}, \\mathcal{Y}\\) represents the entire training sequence of \\(y_i\\) . (c) Decompose the (unconditional) mean-squared error \\[\\begin{equation} E_{\\mathcal{Y}, \\mathcal{X}}(f(x_0)-\\hat f(x_0))^2\\nonumber \\end{equation}\\] into a squared bias and a variance component. (d) Establish a relationship between the squared biases and variances in the above two cases. Remark A smoother \\(\\hat f\\) is called a linear smoother (see Section 5.4 in the text for more details) if it has the form \\[\\begin{equation} {\\hat f} = \\textbf{S}\\textbf{y}.\\nonumber \\end{equation}\\] Note that the linearity implies that \\(\\textbf{S}\\) does not depend on \\(\\textbf{y}\\) . For linear regression, \\(\\textbf{S} = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\) . As for the bias and variance, we have \\[\\begin{eqnarray} \\text{Bias}({\\hat f}) &=& \\textbf{f} - \\textbf{S}\\textbf{f},\\nonumber\\\\ \\text{Cov}({\\hat f}) &=&\\textbf{S}\\text{Cov}(\\textbf{y})\\textbf{S}^T.\\nonumber \\end{eqnarray}\\] Soln. 2.7 (a) For linear regression, we have \\[\\begin{equation} \\hat f(x_0) = [x_0, 1](\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^Ty,\\nonumber \\end{equation}\\] so that \\[\\begin{equation} \\ell_i(x_0;\\mathcal{X}) = [x_0, 1](\\textbf{X}^T\\textbf{X})^{-1}\\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}.\\nonumber \\end{equation}\\] For \\(k\\) -nearest-neighbor regression, we have \\[\\begin{equation} \\hat f(x_0) = \\frac{1}{k}\\sum_{x_i\\in N_k(x_0)}y_i,\\nonumber \\end{equation}\\] where \\(N_k(x_0)\\) is the neighborhood of \\(x_0\\) defined by the \\(k\\) closest points \\(x_i\\) in the training sample. Therefore, \\[\\begin{equation} \\ell_i(x_0;\\mathcal{X}) = \\begin{cases} \\frac{1}{k}, & \\text{if } x_i \\in N_k(x_0)\\\\ 0, & \\text{otherwise.}\\nonumber \\end{cases} \\end{equation}\\] (b) Note that \\(\\mathcal{X}\\) is fixed and randomness comes from \\(\\mathcal{Y}\\) only. We have \\[\\begin{eqnarray} E_{\\mathcal{Y}|\\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 &=& f(x_0)^2 - 2f(x_0)E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)) + E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)^2)\\nonumber\\\\ &=& \\left(f(x_0)-E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))\\right)^2\\nonumber\\\\ && + E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)^2) - \\left(E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))\\right)^2\\nonumber\\\\ &=& \\text{Bias}_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))^2 + \\text{Var}_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)).\\nonumber \\end{eqnarray}\\] (c) The calculation logic is the same as (b), we have \\[\\begin{eqnarray} E_{\\mathcal{Y}, \\mathcal{X}}\\left(f(x_0) - \\hat f(x_0)\\right)^2 &=& f(x_0)^2 - 2f(x_0)E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0)) + E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0)^2)\\nonumber\\\\ &=& \\left(f(x_0)-E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0))\\right)^2\\nonumber\\\\ && + E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0)^2) - \\left(E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0))\\right)^2\\nonumber\\\\ &=& \\text{Bias}(\\hat f(x_0))^2 + \\text{Var}(\\hat f(x_0)).\\nonumber \\end{eqnarray}\\] (d) From (b) we already see that \\(Bias_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))\\) can be written as \\[\\begin{eqnarray} && f(x_0) - E_{\\mathcal{Y}|\\mathcal{X}}\\hat f(x_0)\\nonumber\\\\ &=& f(x_0) - \\sum_{i=1}^NE_{\\mathcal{Y}|\\mathcal{X}}\\ell_i(x_0;\\mathcal{X})(f(x_i) + \\epsilon_i)\\nonumber\\\\ &=& f(x_0) - \\sum_{i=1}^N\\ell_i(x_0;\\mathcal{X})f(x_i)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\label{eq:2-7biasb} \\end{eqnarray}\\] Also, we write \\(Var_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))\\) as \\[\\begin{eqnarray} &&E_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)^2) - \\left(E_{\\mathcal{Y}, \\mathcal{X}}(\\hat f(x_0))\\right)^2\\nonumber\\\\ &=& E_{\\mathcal{Y}|\\mathcal{X}}\\left[\\sum_{i=1}^N\\sum_{j=1}^N\\ell_i(x_0;\\mathcal{X})\\ell_j(x_0;\\mathcal{X})(f(x_0)+\\epsilon_i)(f(x_0)+\\epsilon_j)\\right] \\nonumber\\\\ && - \\left(\\sum_{i=1}^N\\ell_i(x_0;\\mathcal{X})f(x_i)\\right)^2\\nonumber\\\\ &=&\\sum_{i=1}^N\\sum_{j=1}^N\\ell_i(x_0;\\mathcal{X})\\ell_j(x_0;\\mathcal{X})f(x_i)f(x_j)\\nonumber\\\\ && + \\sigma^2\\sum_{i=1}^N\\ell^2_i(x_0;\\mathcal{X})\\nonumber\\\\ && - \\sum_{i=1}^N\\sum_{j=1}^N\\ell_i(x_0;\\mathcal{X})\\ell_j(x_0;\\mathcal{X})f(x_i)f(x_j)\\nonumber\\\\ &=&\\sigma^2\\sum_{i=1}^N\\ell^2_i(x_0;\\mathcal{X})\\nonumber \\end{eqnarray}\\] Denote \\(S = (\\ell_1(x_0;\\mathcal{X}), ..., \\ell_N(x_0;\\mathcal{X}))^T\\) and \\(f = (f(x_1),...,f(x_N))^T\\) . By \\(\\eqref{eq:2-7biasb}\\) and the equation above, we have \\[\\begin{eqnarray} Bias_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)) &=& f(x_0) - S^Tf,\\nonumber\\\\ Var_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)) &=& \\sigma^2S^TS.\\nonumber \\end{eqnarray}\\] Assume that \\(SS^T\\) is non-singular, note that \\(S^TS\\) is a scalar, we have \\[\\begin{eqnarray} [Bias_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))]^2 &=& (f(x_0) - S(x_0)^Tf)^T(f(x_0) - S(x_0)^Tf)\\nonumber\\\\ &=&f(x_0)^2 + 2f(x_0)S^Tf - f^TSS^Tf\\nonumber\\\\ &=&f(x_0)^2 + 2f(x_0)S^Tf - f^TSS^TSS^T(SS^T)^{-1}f\\nonumber\\\\ &=&f(x_0)^2 + 2f(x_0)S^Tf - \\frac{Var_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))}{\\sigma^2}f^Tf\\nonumber\\\\ &=&f(x_0)^2 + 2f(x_0)\\Big(f(x_0)-Bias_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0))\\Big)\\nonumber\\\\ && -\\frac{f^Tf}{\\sigma^2}Var_{\\mathcal{Y}|\\mathcal{X}}(\\hat f(x_0)).\\nonumber \\end{eqnarray}\\] That is the relationship between the squared biases and variances. For (c) , similar arguments follow by integrating terms above by joint density of \\(x_1,...,x_N\\) , that is, \\(h(x_1)\\cdots h(x_N)dx_1\\cdots dx_N\\) .","title":"Ex. 2.7"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-8/","text":"Ex. 2.8 Compare the classification performance of linear regression and \\(k\\) -nearest neighbor classification on the zipcode data. In particular, consider only the 2's and 3's, and \\(k=1,3,5,7\\) and 15. Show both the training and test error for each choice. The zipcode data are available from the website . Soln. 2.8 We summarize error rates obtained from our simulation experiments in the table below. Model Train Error Rate Test Error Rate Linear Regression 0.58% 4.12% 1-NN 0.00% 2.47% 3-NN 0.50% 3.02% 5-NN 0.58% 3.02% 7-NN 0.65% 3.30% 15-NN 0.94% 3.85% We see that as the number of neighbors \\(k\\) increases, both the train and test error rates tend to increase. In the extreme case when \\(k=1\\) , the train error is 0 as expected. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 import pathlib import numpy as np from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsClassifier # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # get original data train = np . genfromtxt ( DATA_PATH . joinpath ( \"zip_train_2and3.csv\" ), dtype = float , delimiter = ',' , skip_header = True ) test = np . genfromtxt ( DATA_PATH . joinpath ( \"zip_test_2and3.csv\" ), dtype = float , delimiter = ',' , skip_header = True ) # prepare training and testing data x_train , y_train = train [:, 1 :], train [:, 0 ] x_test , y_test = test [:, 1 :], test [:, 0 ] # for classification purpose # we assign 1 to digit '3' and 0 to '2' y_train [ y_train == 3 ] = 1 y_train [ y_train == 2 ] = 0 y_test [ y_test == 3 ] = 1 y_test [ y_test == 2 ] = 0 # a utility function to assign prediction def assign ( arr ): arr [ arr >= 0.5 ] = 1 arr [ arr < 0.5 ] = 0 # a utility function to calculate error rate # of predictions def getErrorRate ( a , b ): if a . size != b . size : raise ValueError ( 'Expect input arrays have equal size, a has {} , b has {} ' . format ( a . size , b . size )) if a . size == 0 : raise ValueError ( 'Expect non-empty input arrays' ) return np . sum ( a != b ) / a . size # Linear Regression reg = LinearRegression () . fit ( x_train , y_train ) pred_test = reg . predict ( x_test ) assign ( pred_test ) print ( 'Test error rate of Linear Regression is {:.2%} ' . format ( getErrorRate ( pred_test , y_test ))) pred_train = reg . predict ( x_train ) assign ( pred_train ) print ( 'Train error rate of Linear Regression is {:.2%} ' . format ( getErrorRate ( pred_train , y_train ))) # run separate K-NN classifiers for k in range ( 1 , 16 ): # fit the model neigh = KNeighborsClassifier ( n_neighbors = k ) neigh . fit ( x_train , y_train ) # test error pred_knn_test = neigh . predict ( x_test ) assign ( pred_knn_test ) test_error_rate = getErrorRate ( pred_knn_test , y_test ) # train error pred_knn_train = neigh . predict ( x_train ) assign ( pred_knn_train ) train_error_rate = getErrorRate ( pred_knn_train , y_train ) print ( 'k-NN Model: k is {} , train/test error rates are {:.2%} and {:.2%} ' . format ( k , train_error_rate , test_error_rate ))","title":"Ex. 2.8"},{"location":"ESL-Solution/2-Overview-of-Supervised-Learning/ex2-9/","text":"Ex. 2.9 Consider a linear regression model with \\(p\\) parameters, fit by least squares to a set of training data \\((x_1, y_1), ..., (x_N, y_N)\\) drawn at random from a population. Let \\(\\hat\\beta\\) be the least squares estimate. Suppose we have some test data \\((\\tilde x_1, \\tilde y_1),...,(\\tilde x_M, \\tilde y_M)\\) drawn at random from the same population as the training data. If \\(R_{tr}(\\beta) = \\frac{1}{N}\\sum_1^N(y_i-\\beta^Tx_i)^2\\) and \\(R_{te}(\\beta) = \\frac{1}{M}\\sum_1^M(\\tilde y_i-\\beta^T\\tilde x_i)^2\\) , prove that \\[\\begin{equation} E[R_{tr}(\\hat\\beta)]\\le E[R_{te}(\\hat\\beta)],\\nonumber \\end{equation}\\] where the expectations are over all that is random in each expression. [This exercise was brought to our attention by Ryan Tibshirani, from a homework assignment given by Andrew Ng.] Soln. 2.9 Note that both \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are considered random. When \\(\\textbf{X}^T\\textbf{X}\\) is non-singular, we know that \\[\\begin{equation} \\hat \\beta = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}\\textbf{Y},\\nonumber \\end{equation}\\] which is also random . When \\(\\textbf{X}^T\\textbf{X}\\) is singular, the simple expression above does not hold, however, there exists a measurable function \\(\\phi\\) such that \\[\\begin{equation} \\hat \\beta = \\phi(\\textbf{X}, \\textbf{Y}).\\nonumber \\end{equation}\\] Recall the definition of \\(\\hat\\beta\\) and IID assumption of \\((x_i, y_i)\\) for \\(i=1,...,N\\) . For any \\(\\beta\\) and \\(i=1,...,N\\) , we have \\[\\begin{eqnarray} E[R_{tr}(\\hat\\beta)] &=& E\\left[\\frac{1}{N}\\sum_{k=1}^N(y_k-\\hat\\beta^T x_k)^2\\right]\\nonumber\\\\ &\\le& E\\left[\\frac{1}{N}\\sum_{k=1}^N(y_k-\\beta^T x_k)^2\\right]\\nonumber\\\\ &=&E[(y_i-\\beta^T x_i)^2]. \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\label{eq:2-9a} \\end{eqnarray}\\] Assume \\(x_1\\neq 0\\) almost surely, let \\[\\begin{equation} \\beta = \\frac{y_1-\\tilde y_1+\\hat\\beta \\tilde x_1}{x_1}.\\nonumber \\end{equation}\\] Plug equation above into \\(\\eqref{eq:2-9a}\\) for \\(i=1\\) , by IID assumption of \\((\\tilde x_i, \\tilde y_i)\\) for \\(i=1,...,M\\) , we have \\[\\begin{eqnarray} E[R_{tr}(\\hat\\beta)] &\\le& E[(\\tilde y_1-\\hat\\beta^T \\tilde x_1)^2]\\nonumber\\\\ &=&E\\left[\\frac{1}{M}\\sum_{k=1}^M(\\tilde y_k-\\hat\\beta^T \\tilde x_k)^2\\right]\\nonumber\\\\ &=&E[R_{te}(\\hat\\beta)].\\nonumber \\end{eqnarray}\\]","title":"Ex. 2.9"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-01/","text":"Ex. 3.1 Show that the \\(F\\) statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding \\(z\\) -score (3.12). Soln. 3.1 First recall that (see the paragraph below (3.8) in the text) \\[\\begin{equation} \\hat\\sigma^2 = RSS_1/(N-p_1-1).\\nonumber \\end{equation}\\] Note that \\(z_j=\\hat\\beta_j/\\hat\\sigma\\sqrt{\\nu_j}\\) ((3.12) in the text), it suffices to show that \\[\\begin{equation} RSS_0 - RSS_1 = \\frac{\\hat\\beta_j^2}{\\nu_{jj}}.\\nonumber \\end{equation}\\] We already know how to find \\(RSS_1\\) , the residual sum-of-squares for the original least square model. To find \\(RSS_0\\) , when \\(j\\) -th coefficient is dropped from the original model, denote \\(e_j = (0,..., 1, ..., 0)^T\\in\\mathbb{R}^{(p+1)\\times 1}\\) , we are going to solve \\[\\begin{eqnarray} \\min_{\\beta\\in \\mathbb{R}^{(p+1)\\times 1}}&& (\\textbf{y}-\\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta)\\nonumber\\\\ \\text{s.t.}&& e_j^T\\beta=0.\\nonumber \\end{eqnarray}\\] The Lagrangian multiplier of the problem above is \\[\\begin{equation} L(\\beta,\\lambda) = (\\textbf{y}-\\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j^T\\beta.\\nonumber \\end{equation}\\] Denote the optimal solution by \\((\\hat\\beta^{\\text{new}}, \\hat\\lambda^{\\text{new}})\\) . Taking derivative w.r.t. \\(\\beta\\) and setting it zero we get \\[\\begin{equation} \\frac{\\partial L(\\beta,\\lambda)}{\\partial \\beta} = -2\\textbf{X}^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda e_j = 0.\\nonumber \\end{equation}\\] So we have \\[\\begin{equation} \\hat\\beta^{\\text{new}} = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y} - \\frac{\\hat\\lambda^\\text{new}}{2}(\\textbf{X}^T\\textbf{X})^{-1}e_j.\\nonumber \\end{equation}\\] Recall the constraint \\(e_j^T\\beta=0\\) , we obtain \\[\\begin{equation} \\hat\\lambda^{\\text{new}} = 2\\frac{e_j^T(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}}{e_j^T(\\textbf{X}^T\\textbf{X})^{-1}e_j}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\label{eq:3-1a} \\end{equation}\\] So we have \\[\\begin{equation} \\hat\\beta^{\\text{new}} = \\hat\\beta - \\frac{\\hat\\lambda}{2}(\\textbf{X}^T\\textbf{X})^{-1}e_j.\\nonumber \\end{equation}\\] Therefore, \\[\\begin{eqnarray} RSS_0 &=& (\\textbf{y}-\\textbf{X}\\hat\\beta^{\\text{new}})^T(\\textbf{y}-\\textbf{X}\\hat\\beta^{\\text{new}})\\nonumber\\\\ &=&(\\textbf{y}-\\textbf{X}\\hat\\beta)^T(\\textbf{y}-\\textbf{X}\\hat\\beta)\\nonumber\\\\ && +2 (\\textbf{y}-\\textbf{X}\\hat\\beta)^T\\cdot \\textbf{X}\\cdot\\frac{\\hat\\lambda}{2}(\\textbf{X}^T\\textbf{X})^{-1}e_j\\nonumber\\\\ && + \\frac{\\hat\\lambda^2}{4}e_j^T(\\textbf{X}^T\\textbf{X})^{-1}e_j\\nonumber\\\\ &=&RSS_1 + \\frac{\\hat\\lambda^2}{4}e_j^T(\\textbf{X}^T\\textbf{X})^{-1}e_j,\\nonumber \\end{eqnarray}\\] where the second summand above vanishes because \\[\\begin{equation} (\\textbf{y}-\\textbf{X}\\hat\\beta)^T\\cdot \\textbf{X} = \\textbf{y}^T\\textbf{X} - \\textbf{y}^T\\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{X} = \\textbf{0}.\\nonumber \\end{equation}\\] Then, by \\(\\eqref{eq:3-1a}\\) , we have \\[\\begin{eqnarray} RSS_0 - RSS_1 &=& \\frac{\\left(e_j^T(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\right)^2}{e_j^T(\\textbf{X}^T\\textbf{X})^{-1}e_j}\\nonumber\\\\ &=&\\frac{\\hat\\beta^2_j}{\\nu_{jj}}.\\nonumber \\end{eqnarray}\\] The proof is now complete.","title":"Ex. 3.1"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-02/","text":"Ex. 3.2 Given data on two variables \\(X\\) and \\(Y\\) , consider fitting a cubic polynomial regression model \\(f(X)=\\sum_{j=0}^3\\beta_jX^j\\) . In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches: (a) At each point \\(x_0\\) , form a 95% confidence interval for the linear function \\(a^T\\beta=\\sum_{j=0}^3\\beta_jx_0^j\\) . (b) Form a 95% confidence set for \\(\\beta\\) as in (3.15), which in turn generates confidence intervals for \\(f(x_0)\\) . How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods. Soln. 3.2 For the first method, at each point \\(\\textbf{x}_0\\) , the variance of \\(\\hat y_0 = \\textbf{x}_0\\hat\\beta\\) is \\[\\begin{equation} \\text{Var}(\\hat y_0) = \\textbf{x}_0 \\text{Var}(\\hat\\beta) \\textbf{x}_0^T = \\textbf{x}_0 (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{x}_0^T.\\nonumber \\end{equation}\\] Then at each sample \\((\\textbf{x}_0, y_0)\\) , the confidence interval is calculated as \\[\\begin{equation} \\hat y_0\\pm 1.96 \\sqrt{\\textbf{x}_0 (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{x}_0^T}.\\nonumber \\end{equation}\\] For the second method, denote \\(\\beta\\) as the true value of coefficients, we have \\[\\begin{equation} C_\\beta = \\{\\beta|(\\hat\\beta-\\beta)^T\\textbf{X}^T\\textbf{X}(\\hat\\beta-\\beta)\\le \\sigma^2\\chi^2_{4, 0.05}\\}.\\nonumber \\end{equation}\\] The confidence region for \\(\\hat\\beta\\) is found by inverting the inequality above with respect to \\(\\hat\\beta\\) . To do that, we first make Cholesky decomposition of \\(\\textbf{X}^T\\textbf{X}=\\textbf{U}^T\\textbf{U}\\) where \\(\\textbf{U}\\) is upper triangular. Therefore, for vectors \\(\\hat\\beta\\) at the boundary of the confidence region, \\(\\textbf{U}(\\hat\\beta-\\beta)\\) lies on the 4-dimensional ball with radius \\(r = \\sigma \\sqrt{\\chi^2_{4, 0.05}}\\) . Hence for any vector \\(\\gamma\\in\\mathbb{R}^4\\) , we can first normalize it to the ball with radius \\(r\\) and the arrive \\(\\hat\\beta\\) by \\[\\begin{equation} \\hat\\beta = \\textbf{U}^{-1}\\left(\\gamma \\cdot \\frac{r}{\\|\\gamma\\|}\\right) + \\beta.\\nonumber \\end{equation}\\] Note that the confidence region from the first method is expected to be wider than that from the second method. The reason is that the region from the first method is a simultaneous confidence region , that is, each of the coordinate \\(\\beta_i\\) falls into the region with the given probability. While the one from the second is an elliptical confidence region , it's less strict than the simultaneous one thus it's narrower. This is also observed from Figure 1 below. Figure 1: Confidence Regions from Two Different Methods Figure 1 is drawn from a numerical simulation experiment. In the simulation we first draw 10 values, \\(\\{x_i, i=1,...,10\\}\\) ,uniformly from \\([0,1]\\) . Then we generate \\(y_i = 1 + x_i + 2x_i^2 + 3x_i^3 + \\epsilon_i\\) with \\(\\epsilon_i\\sim N(0, 0.5)\\) . That is, the true value for \\(\\beta\\) is \\(\\beta = (1,1,2,3)^T\\in \\mathbb{R}^4\\) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np # prepare X ones = np . ones ( n ) x = np . random . uniform ( 0 , 1 , n ) x = np . sort ( x ) x_square = np . square ( x ) x_cubic = np . power ( x , 3 ) X = np . column_stack (( ones , x , x_square , x_cubic )) X_T = X . transpose () # prepare Y epsilon = np . random . normal ( 0 , sigma , n ) beta = np . array ([ 1 , 1 , 2 , 3 ]) y_theory = X @ beta y_realized = y_theory + epsilon We then calculate confidence regions following the procedures described above. 1 2 3 4 5 6 7 8 9 10 11 # method 1 from numpy.linalg import inv from scipy.stats import chi2 var_beta_hat = inv ( X_T @ X ) * ( sigma ** 2 ) tmp = X @ var_beta_hat tmp = tmp @ X_T width = np . diag ( tmp ) width = np . sqrt ( width ) width_upper = y_estimated + 1.96 * width width_lower = y_estimated - 1.96 * width Note that in the second method, we sample 100 different vectors \\(\\gamma\\) and thus obtain 100 different \\(\\hat\\beta\\) (of course, finally 100 different `curves' via \\(\\textbf{X}\\hat\\beta\\) ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # method 2 U_T = np . linalg . cholesky ( X_T @ X ) U = U_T . transpose () U_inv = inv ( U ) p = 0.95 df = 4 num = 100 region_arr = [] for i in range ( num ): a = np . random . normal ( 0 , 1 , df ) a = U_inv @ a a_norm = np . linalg . norm ( a , ord = 2 ) r = sigma * np . sqrt ( chi2 . ppf ( p , df )) a = a * ( r / a_norm ) beta2 = beta + a region = np . dot ( X , beta2 ) region_arr . append ( region ) The blue line is calculated from \\(\\hat\\beta^{\\text{OLS}}\\) , the two green lines are the 95% confidence region band from the first method, and the red lines are sampled from the boundary of the 95% confidence set from the second method. Note that most red ones lie in the confidence region formed by the green boundaries, however, they have the freedom to jump out. 1 2 3 4 5 6 7 8 9 10 11 12 # plot import plotly.graph_objects as go fig = go . Figure () fig . add_trace ( go . Scatter ( x = x , y = y_estimated , mode = 'lines+markers' , name = 'estimated' , line_color = '#0066ff' )) fig . add_trace ( go . Scatter ( x = x , y = width_upper , mode = 'lines+markers' , name = 'upper1' , line_color = '#009933' )) fig . add_trace ( go . Scatter ( x = x , y = width_lower , mode = 'lines+markers' , name = 'lower1' , line_color = '#009933' )) for i in range ( num ): fig . add_trace ( go . Scatter ( x = x , y = region_arr [ i ], mode = 'lines' , line_color = '#cc3300' )) fig . show () Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import numpy as np import plotly.graph_objects as go from numpy.linalg import inv from scipy.stats import chi2 n = 10 sigma = np . sqrt ( 0.5 ) # prepare data ones = np . ones ( n ) x = np . random . uniform ( 0 , 1 , n ) x = np . sort ( x ) x_square = np . square ( x ) x_cubic = np . power ( x , 3 ) X = np . column_stack (( ones , x , x_square , x_cubic )) X_T = X . transpose () epsilon = np . random . normal ( 0 , sigma , n ) beta = np . array ([ 1 , 1 , 2 , 3 ]) y_theory = X @ beta y_realized = y_theory + epsilon beta_hat = inv ( X_T @ X ) @ X_T @ y_realized y_estimated = X @ beta_hat # method 1 var_beta_hat = inv ( X_T @ X ) * ( sigma ** 2 ) tmp = X @ var_beta_hat tmp = tmp @ X_T width = np . diag ( tmp ) width = np . sqrt ( width ) width_upper = y_estimated + 1.96 * width width_lower = y_estimated - 1.96 * width # method 2 U_T = np . linalg . cholesky ( X_T @ X ) U = U_T . transpose () U_inv = inv ( U ) p = 0.95 df = 4 num = 100 region_arr = [] for i in range ( num ): a = np . random . normal ( 0 , 1 , df ) a = U_inv @ a a_norm = np . linalg . norm ( a , ord = 2 ) r = sigma * np . sqrt ( chi2 . ppf ( p , df )) a = a * ( r / a_norm ) beta2 = beta + a region = np . dot ( X , beta2 ) region_arr . append ( region ) # plot fig = go . Figure () fig . add_trace ( go . Scatter ( x = x , y = y_estimated , mode = 'lines+markers' , name = 'estimated' , line_color = '#0066ff' )) fig . add_trace ( go . Scatter ( x = x , y = width_upper , mode = 'lines+markers' , name = 'upper1' , line_color = '#009933' )) fig . add_trace ( go . Scatter ( x = x , y = width_lower , mode = 'lines+markers' , name = 'lower1' , line_color = '#009933' )) for i in range ( num ): fig . add_trace ( go . Scatter ( x = x , y = region_arr [ i ], mode = 'lines' , line_color = '#cc3300' )) fig . show ()","title":"Ex. 3.2"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-03/","text":"Ex. 3.3 Gauss-Markov theorem: (a) Prove the Gauss-Markov theorem: the least squares estimate of a parameter \\(a^T\\beta\\) has variance no bigger than that of any other linear unbiased estimate of \\(a^T\\beta\\) (Section 3.2.2). (b) The matrix inequality \\(\\textbf{B} \\preceq \\textbf{A}\\) holds if \\(\\textbf{A} - \\textbf{B}\\) is positive semidefinite. Show that if \\(\\hat{\\textbf{V}}\\) is the variance-covariance matrix of the least squares estimate of \\(\\beta\\) and \\(\\tilde{\\textbf{V}}\\) is the variance-covariance matrix of any other linear unbiased estimate, then \\(\\hat{\\textbf{V}} \\preceq \\tilde{\\textbf{V}}\\) . Remark Note the linear estimate explicitly specified in (a). See Section 5.4 in the text or Ex 2.7 Soln. 3.3 (a) Let \\(\\tilde \\theta = c^Ty\\) be another unbiased linear estimate of \\(a^T\\beta\\) with \\(c=a(X^TX)^{-1}X^T + d\\) . We have \\[\\begin{equation} E[c^Ty] = a^T\\beta + dX\\beta.\\nonumber \\end{equation}\\] Since \\(c^Ty\\) is unbiased, we have \\(dX=0\\) . Therefore, \\[\\begin{eqnarray} \\text{Var}(c^Ty) &=& \\sigma^2\\left(a(X^TX)^{-1}X^T+d\\right)\\left(a(X^TX)^{-1}X^T+d\\right)^T\\nonumber\\\\ &=&\\sigma^2(a^T(X^TX)^{-1}a + d^Td)\\nonumber\\\\ &=&\\sigma^2\\left(\\text{Var}(a^T\\hat\\beta) + d^Td\\right).\\nonumber \\end{eqnarray}\\] The proof is therefore complete by noting \\(d^Td\\ge 0\\) . (b) This is almost like a matrix version of (a). Let \\(C\\) be a \\(p\\times N\\) matrix and \\(Cy\\) is another linear unbiased estimate of \\(\\beta\\) . We write \\(C = (X^TX)^{-1}X^T + D\\) , similarly we have \\(DX=0\\) and \\[\\begin{eqnarray} \\tilde{\\textbf{V}} &=& ((X^TX)^{-1}X^T + D)((X^TX)^{-1}X^T + D)^T\\nonumber\\\\ &=&\\hat{\\textbf{V}} + DD^T.\\nonumber \\end{eqnarray}\\] The result follows because \\(DD^T\\) is positive semidefinite.","title":"Ex. 3.3"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-04/","text":"Ex. 3.4 Show how the vector of least squares coefficients can be obtained from a single pass of the Gram-Schmidt procedure (Algorithm 3.1). Represent your solution in terms of the QR decomposition of \\(\\textbf{X}\\) . Soln. 3.4 Given the QR decomposition \\(\\textbf{X}=\\textbf{Q}\\textbf{R}\\) , we know that (see (3.32) in the text) \\[\\begin{equation} \\label{eq:3-4a} \\hat\\beta = \\textbf{R}^{-1}\\textbf{Q}^T\\textbf{y}. \\end{equation}\\] Since \\(\\textbf{Q}\\) and \\(\\textbf{R}\\) are computed from the Gram-Schmidt procedure on \\(\\textbf{X}\\) , as we are producing \\(\\textbf{q}_k\\) for \\(\\textbf{Q}\\) , we can calculate \\(\\textbf{q}_k^T\\textbf{y}\\) so that \\(\\textbf{Q}^T\\textbf{y}\\) is sequentially filled. After \\(\\textbf{R}\\) is computed, we can solve \\(\\textbf{R}^{-1}\\) in a backward way since its upper triangular. Then we are able to compute \\(\\beta\\) as given in \\(\\eqref{eq:3-4a}\\) .","title":"Ex. 3.4"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-05/","text":"Ex. 3.5 Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem \\[\\begin{equation} \\hat\\beta^c = \\underset{\\beta^c}{\\operatorname{arg min}}\\left\\{\\sum_{i=1}^N[y_i-\\beta_0^c - \\sum_{j=1}^p(x_{ij}-\\bar x_j)\\beta^c_j]^2 + \\lambda \\sum_{j=1}^p(\\beta_j^c)^2\\right\\}.\\nonumber \\end{equation}\\] Given the correspondence between \\(\\beta^c\\) and the original \\(\\beta\\) in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso. Soln. 3.5 We center each \\(x_{ij}\\) by replacing \\(x_{ij}\\) with \\(x_{ij}-\\bar x_j\\) , then (3.41) becomes \\[\\begin{equation} \\hat\\beta^{\\text{ridge}} = \\underset{\\beta}{\\operatorname{arg min}}\\left\\{\\sum_{i=1}^N[y_i - \\beta_0 - \\sum_{j=1}^p\\bar x_j\\beta_j - \\sum_{j=1}^p(x_{ij}-\\bar x_j)\\beta_j]^2 + \\lambda \\sum_{j=1}^p\\beta_j^2\\right\\}.\\nonumber \\end{equation}\\] Looking at the two problems, we can see \\(\\beta^c\\) can be transformed from original \\(\\beta\\) as \\[\\begin{eqnarray} \\beta_0^c &=& \\beta_0 + \\sum_{j=1}^p\\bar x_j\\beta_j\\nonumber\\\\ \\beta_j^c &=& \\beta_j \\ \\text{ for } j =1,...,p.\\nonumber \\end{eqnarray}\\] It's easy to see that exact same centering technique applies to the lasso. To characterize the solution, we first take derivative w.r.t \\(\\beta_0^c\\) and set it equal to 0, which yields \\[\\begin{equation} \\sum_{i=1}^N\\left(y_i-\\beta_0^c - \\sum_{j=1}^p(x_{ij}-\\bar x_j)\\beta_j\\right) = 0,\\nonumber \\end{equation}\\] which further implies that \\(\\beta_0^c=\\bar y\\) . Next we set \\[\\begin{eqnarray} \\tilde y_i &=& y_i -\\beta_0^c,\\nonumber\\\\ \\tilde x_{ij} &=& x_{ij} - \\bar x_j,\\nonumber \\end{eqnarray}\\] the problem, in matrix form, becomes \\[\\begin{equation} \\min_{\\beta^c} (\\tilde{\\textbf{y}} - \\tilde{\\textbf{X}}\\beta^c)^T(\\tilde{\\textbf{y}} - \\tilde{\\textbf{X}}\\beta^c) + \\lambda\\beta_c^T\\beta_c.\\nonumber \\end{equation}\\] It's easy to see the solution is \\[\\begin{equation} \\hat\\beta_c = (\\tilde{\\textbf{X}}^T\\tilde{\\textbf{X}}+\\lambda\\textbf{I})^{-1}\\tilde{\\textbf{X}}^T\\textbf{y}.\\nonumber \\end{equation}\\]","title":"Ex. 3.5"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-06/","text":"Ex. 3.6 Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior \\(\\beta\\sim N(0,\\tau\\textbf{I})\\) , and Gaussian sampling model \\(\\textbf{y}\\sim N(\\textbf{X}\\beta, \\sigma^2\\textbf{I})\\) . Find the relationship between the regularization parameter \\(\\lambda\\) in the ridge formula, and the variances \\(\\tau\\) and \\(\\sigma^2\\) . Soln. 3.6 By Bayes' theorem we have \\[\\begin{eqnarray} P(\\beta|\\textbf{y}) &=& \\frac{P(\\textbf{y}|\\beta)P(\\beta)}{P(\\textbf{y})}\\nonumber\\\\ &=&\\frac{ N(\\textbf{X}\\beta, \\sigma^2\\textbf{I})N(0, \\tau\\textbf{I}) }{P(\\textbf{y})}.\\nonumber \\end{eqnarray}\\] Taking logarithm on both sides we get \\[\\begin{eqnarray} \\ln(P(\\beta|\\textbf{y})) = -\\frac{1}{2}\\left(\\frac{(\\textbf{y}-\\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta)}{\\sigma^2} + \\frac{\\beta^T\\beta}{\\tau}\\right) + C,\\nonumber \\end{eqnarray}\\] where \\(C\\) is a constant independent of \\(\\beta\\) . Therefore, if we let \\(\\lambda = \\frac{\\sigma^2}{\\tau}\\) , then maximizing over \\(\\beta\\) for \\(P(\\beta|\\textbf{y})\\) is equivalent to \\[\\begin{equation} \\min_{\\beta} (\\textbf{y}-\\textbf{X}\\beta)^T(\\textbf{y}-\\textbf{X}\\beta) + \\lambda \\beta^T\\beta,\\nonumber \\end{equation}\\] which is essentially solving a ridge regression.","title":"Ex. 3.6"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-07/","text":"Ex. 3.7 Assume \\(y_i\\sim N(\\beta_0+x_i^T\\beta, \\sigma^2)\\) , \\(i=1, 2,..., N\\) , and the parameters \\(\\beta_j\\) are each distributed as \\(N(0, \\tau^2)\\) , independently of one another. Assuming \\(\\sigma^2\\) and \\(\\tau^2\\) are known, show that the (minus) log-posterior density of \\(\\beta\\) is proportional to \\(\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2\\) where \\(\\lambda = \\sigma^2/\\tau^2\\) . Warning The claim above does not seem to be correct. Soln. 3.7 By Bayes' theorem we have \\[\\begin{eqnarray} P(\\beta|\\textbf{y}) &=& \\frac{P(\\textbf{y}|\\beta)P(\\beta)}{P(\\textbf{y})}.\\label{eq:3-7a} \\end{eqnarray}\\] By assumptions here we have \\[\\begin{eqnarray} P(\\textbf{y}|\\beta) &=&\\frac{1}{(2\\pi)^{N/2} \\sigma^N} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j}x_{ij}\\beta_j)^2\\right),\\nonumber\\\\ P(\\beta) &=& \\frac{1}{(2\\pi)^{p/2} \\sigma^p}\\exp\\left(-\\frac{1}{\\tau^2}\\sum_{j=1}^p\\beta_j^2\\right).\\nonumber \\end{eqnarray}\\] Therefore, with \\(\\lambda = \\sigma^2/\\tau^2\\) , from \\(\\eqref{eq:3-7a}\\) we have \\[\\begin{eqnarray} -\\ln(P(\\beta|\\textbf{y})) &=&\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j}x_{ij}\\beta_j)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2\\right) + C,\\nonumber \\end{eqnarray}\\] where \\(C\\) is a constant independent of \\(\\beta\\) . The claim is true if and only if \\(C = 0\\) , which is not the case here.","title":"Ex. 3.7"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-08/","text":"Ex. 3.8 Consider the \\(QR\\) decomposition of the uncentered \\(N\\times (p+1)\\) matrix \\(\\textbf{X}\\) (whose first column is all ones), and the SVD of the \\(N\\times p\\) centered matrix \\(\\tilde{\\textbf{X}}\\) . Show that \\(\\textbf{Q}_2\\) and \\(\\textbf{U}\\) span the same subspace, where \\(\\textbf{Q}_2\\) is the sub-matrix of \\(\\textbf{Q}\\) with the first column removed. Under what circumstances will they be the same, up to sign flips? Soln. 3.8 Let \\(\\textbf{x}_i=(x\\_{1i}, ..., x\\_{Ni})^T\\in \\mathbb{R}^{N\\times 1}\\) and \\(\\textbf{1} = (1,...,1)^T\\in \\mathbb{R}^{N\\times 1}\\) be the column vectors of \\(\\textbf{X}\\) . Let \\(\\textbf{q}_i = (q\\_{1i}, ..., q\\_{Ni})^T\\in \\mathbb{R}^{N\\times 1}\\) be the column vectors of \\(\\textbf{Q}\\) . We can write the QR decomposition of \\(X\\) as \\[\\begin{equation} \\label{eq:3-8qr} \\textbf{X} = \\begin{pmatrix} \\textbf{1} & \\textbf{x}_1 & \\cdots & \\textbf{x}_p\\\\ \\end{pmatrix} = \\begin{pmatrix} \\textbf{q}_0 & \\textbf{q}_1 & \\cdots & \\textbf{q}_p\\\\ \\end{pmatrix}\\textbf{R} =\\textbf{Q}\\textbf{R} \\in \\mathbb{R}^{N\\times (p+1)}. \\end{equation}\\] Similarly, let \\(r_{ij}\\) be the entry of \\(\\textbf{R}\\) at \\(i\\) -th row and \\(j\\) -th column. Note that \\(\\textbf{Q}\\in\\mathbb{R}^{N\\times (p+1)}\\) and is orthogonal. The matrix \\(\\textbf{R}\\in\\mathbb{R}^{(p+1)\\times(p+1)}\\) is upper triangular. By matrix multiplication we can verify that \\[\\begin{equation} \\textbf{1} = r_{00}\\textbf{q}_0.\\nonumber \\end{equation}\\] Since \\(\\textbf{Q}\\) is orthogonal, we obtain \\(q_{10}=q_{20} =...=q_{N0}=\\frac{1}{\\sqrt N}\\) , i.e., \\(\\textbf{q}_0=\\frac{1}{\\sqrt N}\\textbf{1}\\) , and \\(r_{00}=\\sqrt N\\) . Therefore, for \\(j=1,...,p\\) , we have \\[\\begin{equation} \\label{eq:3-8a} \\bar{q}_j = \\sum_{k=1}^Nq_{ij}/N = \\frac{1}{N}\\textbf{1}^T\\textbf{q}_j = \\frac{1}{\\sqrt N}\\textbf{q}_0^T\\textbf{q}_j = 0. \\end{equation}\\] Let \\(\\bar x_i = \\frac{1}{N}\\sum_{k=1}^Nx_{ki}\\) and \\(\\tilde{\\textbf{x}}_i = \\textbf{x}_i - \\bar x_i \\textbf{1}\\) . By QR decomposition of \\(X\\) we can verify that for \\(1\\le j \\le p\\) , \\[\\begin{equation} \\textbf{x}_j = \\sum_{k=0}^jr_{kj}\\textbf{q}_k,\\nonumber \\end{equation}\\] so that \\[\\begin{equation} \\bar x_j = r_{0j}\\bar q_0 + \\sum_{k=1}^jr_{kj}\\bar q_k=r_{0j}\\bar q_0 = \\frac{r_{0j}}{\\sqrt N}.\\nonumber \\end{equation}\\] Now, we can see for \\(1\\le j\\le p\\) , \\[\\begin{eqnarray} \\tilde{\\textbf{x}}_j &=& \\textbf{x}_j -\\bar x_j\\textbf{1}\\nonumber\\\\ &=& \\sum_{k=0}^jr_{kj}\\textbf{q}_k - r_{0j}\\cdot\\frac{1}{\\sqrt N}\\textbf{1}\\nonumber\\\\ &=& \\sum_{k=0}^jr_{kj}\\textbf{q}_k - r_{0j}\\textbf{q}_0\\nonumber\\\\ &=& \\sum_{k=1}^jr_{kj}\\textbf{q}_k.\\label{eq:3-8b} \\end{eqnarray}\\] Now we are ready to finish the proof. Let \\[\\begin{equation} \\textbf{Q}_2 = \\begin{pmatrix} \\textbf{q}_1 & \\cdots & \\textbf{q}_p\\\\ \\end{pmatrix}. \\nonumber \\end{equation}\\] We write the SVD decomposition of \\(\\tilde{\\textbf{X}}\\) as \\[\\begin{equation} \\tilde{\\textbf{X}} = \\begin{pmatrix} \\tilde{\\textbf{x}}_1 & \\cdots & \\tilde{\\textbf{x}}_p\\\\ \\end{pmatrix}=\\textbf{U}\\textbf{D}\\textbf{V}^T\\in\\mathbb{R}^{N\\times p}.\\nonumber \\end{equation}\\] Here \\(\\textbf{U}\\) and \\(\\textbf{V}\\) are \\(N\\times p\\) and \\(p\\times p\\) orthogonal matrices, with columns of \\(\\textbf{U}\\) spanning the column space of \\(\\tilde{\\textbf{X}}\\) , and the columns of \\(\\textbf{V}\\) spanning the row space. \\(\\textbf{D}\\) is a \\(p\\times p\\) diagonal matrix, with diagonal entries \\(d_1\\ge d_2\\ge ... \\ge d_p\\ge 0\\) called the singular values of \\(\\tilde{\\textbf{X}}\\) (see page 65 and 66 in the text). Therefore, it suffices to prove that \\(\\textbf{Q}_2\\) spans the column space of \\(\\tilde{\\textbf{X}}\\) (because \\(\\textbf{U}\\) does). However, note that columns in \\(\\textbf{Q}_2\\) are orthogonal, and \\(\\eqref{eq:3-8b}\\) , the proof is complete.","title":"Ex. 3.8"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-09/","text":"Ex. 3.9 Forward stepwise regression . Suppose we have the QR decomposition for the \\(N\\times q\\) matrix \\(\\textbf{X}_1\\) in a multiple regression problem with response \\(\\textbf{y}\\) , and we have an additional \\(p-q\\) predictors in the matrix \\(\\textbf{X}_2\\) . Denote the current residual by \\(\\textbf{r}\\) . We wish to establish which one of these additional variables will reduce the residual sum-of-squares the most when included with those in \\(\\textbf{X}_1\\) . Describe an efficient procedure for doing this. Soln. 3.9 Without loss of generality we assume \\(\\textbf{X}_1=(\\textbf{x}_1, ...,\\textbf{x}_q)\\in\\mathbb{R}^{N\\times q}\\) . Write the QR decomposition for \\(\\textbf{X}_1\\) as \\[\\begin{equation} \\textbf{X}_1 = \\textbf{Q}_1\\textbf{R}_1,\\nonumber \\end{equation}\\] where \\(\\textbf{Q}_1=(\\textbf{q}_1, ..., \\textbf{q}_q)\\) is a \\(N\\times q\\) orthogonal matrix and \\(\\textbf{R}_1\\) is a \\(q\\times a\\) upper triangular matrix. We know that \\(\\textbf{Q}_1\\) spans the column space of \\(\\textbf{X}_1\\) . Now we consider choosing an additional predictor \\(\\textbf{x}_k\\) , where \\(q < k\\le p\\) . The projection of \\(\\textbf{x}_k\\) onto the column space of \\(\\textbf{X}_1\\) , denoted by \\(\\mathcal{P}_{\\textbf{X}_1}(\\textbf{x}_k)\\) , is \\[\\begin{equation} \\mathcal{P}_{\\textbf{X}_1}(\\textbf{x}_k) = \\sum_{j=1}^q(\\textbf{x}_k^T\\textbf{q}_j) \\textbf{q}_j.\\nonumber \\end{equation}\\] Define \\[\\begin{eqnarray} \\textbf{r}_k &=& \\textbf{x}_k - \\mathcal{P}_{\\textbf{X}_1}(\\textbf{x}_k),\\nonumber\\\\ \\textbf{q}_k &=& \\textbf{r}_k/\\|\\textbf{r}_k\\|.\\nonumber \\end{eqnarray}\\] If we add \\(\\textbf{x}_k\\) into \\(\\textbf{X}_1\\) , the newly fitted value, which is the projection of \\(\\textbf{y}\\) onto newly spanned space by \\(\\textbf{X}_1\\) and \\(\\textbf{x}_k\\) , becomes \\[\\begin{equation} \\hat{\\textbf{y}} + (\\textbf{q}_k^T\\textbf{y})\\textbf{q}_k = \\hat{\\textbf{y}} + (\\textbf{q}_k^T\\textbf{r})\\textbf{q}_k,\\nonumber \\end{equation}\\] and thus the residual sum-of-squares is reduced by \\[\\begin{equation} (\\textbf{q}_k^T\\textbf{r})^2.\\nonumber \\end{equation}\\] Therefore, at each step \\(q\\) we are going to choose \\(k\\) such that \\[\\begin{equation} k = \\underset{q< k\\le p}{\\operatorname{argmax}} \\textbf{q}_k^T\\textbf{r}.\\nonumber \\end{equation}\\] Note the calculations above require the values of \\(\\textbf{q}_1, ..., \\textbf{q}_q\\) , which are already available from \\(\\textbf{Q}_1\\) in QR decomposition. After we get \\(\\textbf{q}_k\\) , we rename it to \\(\\textbf{q}_{q+1}\\) , and then we can augment \\(\\textbf{Q}_1\\) to \\(\\tilde{\\textbf{Q}}_1=(\\textbf{q}_1, ..., \\textbf{q}_q, \\textbf{q}_{q+1})\\) and continue to next iteration \\(q+1\\) . Remark The value of \\(\\textbf{R}_1\\) here is less relevant.","title":"Ex. 3.9"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-10/","text":"Ex. 3.10 Backward stepwise regression . Suppose we have the multiple regression fit of \\(\\textbf{y}\\) on \\(\\textbf{X}\\) , along with the standard errors and \\(Z\\) -scores as in Table 3.2. We wish to establish which variable, when dropped, will increase residual sum-of-squares the least. How would you do this? Soln. 3.10 Let's follow the ideas in Ex 3.9 . Suppose \\(\\textbf{x}_k\\) is the last predictor we added, and the decrease in sum-of-residuals is \\((\\textbf{q}_k^T\\textbf{y})^2\\) . On the other hand, the \\(Z\\) -score for the coefficient \\(\\hat{\\beta}_p\\) is, e.g., see (3.28)-(3.29) in the text, \\[\\begin{equation} Z_k = \\frac{\\textbf{q}_k^T\\textbf{y}}{\\hat\\sigma}.\\nonumber \\end{equation}\\] Therefore, we choose the predictor with the smallest absolute value of \\(Z\\) -score to drop. Remark Alternatively, we can use Ex 3.1 to arrive at the same result.","title":"Ex. 3.10"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-11/","text":"Ex. 3.11 Show that the solution to the multivariate linear regression problem (3.40) is given by (3.39). What happens if the covariance matrices \\(\\boldsymbol{\\Sigma}_i\\) are different for each observation? Soln. 3.11 Like (3.38), we write (3.40) in matrix form \\[\\begin{equation} \\text{RSS}(\\textbf{B};\\boldsymbol{\\Sigma}) = \\text{tr}[(\\textbf{Y}-\\textbf{X}\\textbf{B})^T\\boldsymbol{\\Sigma}^{-1}(\\textbf{Y}-\\textbf{X}\\textbf{B})].\\nonumber \\end{equation}\\] By properties of trace operator, we have \\[\\begin{eqnarray} &&\\text{RSS}(\\textbf{B};\\boldsymbol{\\Sigma})\\nonumber\\\\ &=& \\text{tr}[(\\textbf{Y}^T\\boldsymbol{\\Sigma}^{-1}-\\textbf{B}^T\\textbf{X}^T\\boldsymbol{\\Sigma}^{-1})(\\textbf{Y}-\\textbf{X}\\textbf{B})]\\nonumber\\\\ &=&\\text{tr}(\\textbf{Y}^T\\boldsymbol{\\Sigma}^{-1}\\textbf{Y}-\\textbf{Y}^T\\boldsymbol{\\Sigma}^{-1}\\textbf{X}\\textbf{B} - \\textbf{B}^T\\textbf{X}^T\\boldsymbol{\\Sigma}^{-1}\\textbf{Y} + \\textbf{B}^T\\textbf{X}^T\\boldsymbol{\\Sigma}^{-1}\\textbf{X}\\textbf{B}).\\nonumber \\end{eqnarray}\\] Taking derivative and setting it to be zero, we get \\[\\begin{eqnarray} &&\\frac{\\partial \\text{RSS}(\\textbf{B};\\boldsymbol{\\Sigma})}{\\partial \\textbf{B}}\\nonumber\\\\ &=& \\textbf{X}^T(\\boldsymbol{\\Sigma}^{-1}+(\\boldsymbol{\\Sigma}^{-1})^{T})\\textbf{X}\\textbf{B} - \\textbf{X}^T(\\boldsymbol{\\Sigma}^{-1}+(\\boldsymbol{\\Sigma}^{-1})^{T})\\textbf{Y}\\nonumber\\\\ &=&\\textbf{0}. \\label{eq:3-11a} \\end{eqnarray}\\] Note that \\(\\boldsymbol{\\Sigma}\\) is a positive definite symmetric matrix, there exists \\(\\textbf{S}\\) such that \\(\\boldsymbol{\\Sigma}^{-1}=\\textbf{S}\\textbf{S}^T\\) . Therefore we obtain \\[\\begin{eqnarray} \\hat{\\textbf{B}} &=& (\\textbf{X}^T\\textbf{S}\\textbf{S}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{S}\\textbf{S}^T\\textbf{Y}\\nonumber\\\\ &=&(\\textbf{X}^T\\textbf{S}\\textbf{S}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{S}\\textbf{S}^T\\textbf{X}\\textbf{X}^T(\\textbf{X}\\textbf{X}^T)^{-1}\\textbf{Y}\\nonumber\\\\ &=&\\textbf{X}^T(\\textbf{X}\\textbf{X}^T)^{-1}\\textbf{Y}\\nonumber\\\\ &=&(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{X}\\textbf{X}^T(\\textbf{X}\\textbf{X}^T)^{-1}\\textbf{Y}\\nonumber\\\\ &=&(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{Y},\\nonumber \\end{eqnarray}\\] which is (3.39) in the text. When \\(\\boldsymbol{\\Sigma}_i\\) are different, the simple solution for \\(\\textbf{B}\\) above does not hold. Instead, we have to deal with equations like \\(\\eqref{eq:3-11a}\\) with different \\(\\boldsymbol{\\Sigma}_i\\) . Numerical solutions are available though, as the problem is essentially in quadratic form of \\(\\textbf{B}\\) .","title":"Ex. 3.11"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-12/","text":"Ex. 3.12 Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix \\(\\textbf{X}\\) with \\(p\\) additional rows \\(\\sqrt{\\lambda}\\textbf{I}\\) , and augment \\(\\textbf{y}\\) with \\(p\\) zeros. By introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero. This is related the idea of hints due to Abu-Mostafa (1995), where model constraints are implemented by adding artificial data examples that satisfy them. Soln. 3.12 We write \\[\\begin{equation} \\tilde{\\textbf{X}} = \\begin{pmatrix} \\textbf{X}\\\\ \\sqrt \\lambda \\textbf{I}_p \\end{pmatrix} \\text{ and } \\tilde{\\textbf{y}} = \\begin{pmatrix} \\textbf{y}\\\\ \\textbf{0}_p \\end{pmatrix},\\nonumber \\end{equation}\\] where \\(\\textbf{I}_p\\) is a \\(p\\times p\\) identity matrix, and \\(\\textbf{0}_p\\) is a \\(p\\times 1\\) vector. Thus we have \\[\\begin{equation} \\tilde{\\textbf{X}}^T\\tilde{\\textbf{X}} = \\begin{pmatrix} \\textbf{X}^T&\\sqrt \\lambda \\textbf{I}_p \\end{pmatrix} \\begin{pmatrix} \\textbf{X}\\\\ \\sqrt \\lambda \\textbf{I}_p \\end{pmatrix}=\\textbf{X}^T\\textbf{X} + \\lambda\\textbf{I}_p\\nonumber \\end{equation}\\] and \\[\\begin{equation} \\tilde{\\textbf{X}}^T\\textbf{y} = \\begin{pmatrix} \\textbf{X}^T&\\sqrt \\lambda \\textbf{I}_p \\end{pmatrix} \\begin{pmatrix} \\textbf{y}\\\\ \\textbf{0}_p \\end{pmatrix} = \\textbf{X}^T\\textbf{y}.\\nonumber \\end{equation}\\] Then we have \\[\\begin{eqnarray} \\hat\\beta_{\\text{new}} &=& (\\tilde{\\textbf{X}}^T\\tilde{\\textbf{X}})^{-1}\\tilde{\\textbf{X}}^T\\tilde{\\textbf{y}}\\nonumber\\\\ &=& (\\textbf{X}^T\\textbf{X} + \\lambda\\textbf{I}_p)^{-1}\\textbf{X}^T\\textbf{y},\\nonumber \\end{eqnarray}\\] which is the solution to ridge regression with parameter \\(\\lambda\\) .","title":"Ex. 3.12"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-13/","text":"Ex. 3.13 Derive the expression (3.62), and show that \\(\\hat\\beta^{\\text{pcr}}(p) = \\hat\\beta^{\\text{ls}}\\) . Soln. 3.13 Since \\(\\bb{z}_m=\\bb{X}v_m\\) , from (3.61) in the text we have \\[\\begin{eqnarray} \\hat{\\by}^{\\text{pcr}}_{(M)} &=& \\bar y\\bb{1} + \\bX\\sum_{m=1}^M\\hat\\theta_mv_m\\nonumber\\\\ &=&\\begin{pmatrix} \\bb{1} & \\bX \\end{pmatrix} \\begin{pmatrix} \\bar y\\\\ \\sum_{m=1}^M\\hat\\theta_mv_m \\end{pmatrix}.\\nonumber \\end{eqnarray}\\] Therefore, we can represent \\[\\begin{equation} \\hat\\beta^{\\text{pcr}}(M) = \\sum_{m=1}^M\\hat\\theta_mv_m,\\nonumber \\end{equation}\\] which is (3.62) in the text. Recall the SVD decomposition of \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T\\) . Here \\(\\bb{U}\\) and \\(\\bb{V}\\) are \\(N\\times p\\) and \\(p\\times p\\) orthogonal matrices, and \\(\\bb{D}\\) is a \\(p\\times p\\) diagonal matrix. We have \\(\\bX\\bb{V} = \\bb{U}\\bb{D}\\) since \\(\\bb{V}\\) is orthogonal, so that \\[\\begin{equation} \\bb{z}_m = \\bX v_m = d_mu_m,\\nonumber \\end{equation}\\] where \\(d_m\\in\\mathbb{R}\\) is the \\(m\\) -th diagonal element in \\(\\bb{D}\\) and \\(u_m\\) is \\(m\\) -th column in \\(\\bb{U}\\) . By definition of \\(\\hat\\theta_m=\\langle \\bb{z}_m, \\by\\rangle/\\langle \\bb{z}_m, \\bb{z}_m \\rangle\\) , we have \\(\\hat\\theta_m = \\langle u_m, \\by\\rangle/d_m\\) so that \\[\\begin{equation} \\hat\\beta^{\\text{pcr}}(p) = \\sum_{m=1}^M\\hat\\theta_mv_m =\\bb{V}\\begin{pmatrix} \\hat\\theta_1\\\\\\hat\\theta_2\\\\\\vdots\\\\\\hat\\theta_p \\end{pmatrix} = \\bb{V}\\bb{D}^{-1}\\bb{U}^T\\by.\\nonumber \\end{equation}\\] On the other hand, recall \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T\\) , by simple algebra we have \\[\\begin{eqnarray} \\hat\\beta^{ls} &=&(\\bX^T\\bX)^{-1}\\bX^T\\by\\nonumber\\\\ %&=&(\\bb{V}\\bb{D}^2\\bb{V}^T)^{-1}\\bb{U}\\bb{D}\\bb{V}^T\\by\\nonumber\\\\ &=&\\bb{V}\\bb{D}^{-1}\\bb{U}^T\\by.\\nonumber \\end{eqnarray}\\] Therefore we have \\[\\begin{equation} \\hat\\beta^{\\text{pcr}}(p)=\\hat\\beta^{ls}.\\nonumber \\end{equation}\\]","title":"Ex. 3.13"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-14/","text":"Ex. 3.14 Show that in the orthogonal case, PLS stops after \\(m=1\\) steps, because subsequent \\(\\hat\\varphi_{mj}\\) in Step 2 in Algorithm 3.3 are zero. Soln. 3.14 Observe that in Algorithm 3.3 on partial least squares (PLS) if \\(\\hat\\varphi_{mj}=0\\) for all \\(j\\) on any given step \\(m\\) then the algorithm will stop. We start with \\(m=1\\) . We have (a) \\[\\begin{equation} \\bb{z}_1 = \\sum_{j=1}^p\\hat\\varphi_{1j}\\bx^{(0)}_j \\text{ with } \\hat\\varphi_{1j} = \\langle \\bx^{(0)}_j, \\by \\rangle. \\non \\end{equation}\\] (b) For \\(\\hat\\theta_1\\) , we have \\[\\begin{eqnarray} \\hat\\theta_1 = \\langle \\bb{z}_1, \\by \\rangle/\\langle \\bb{z}_1, \\bb{z}_1 \\rangle\\non \\end{eqnarray}\\] where \\[\\begin{eqnarray} \\langle \\bb{z}_1, \\bb{z}_1 \\rangle &=& \\left\\langle \\sum_{j=1}^p\\hat\\varphi_{1j}\\bx^{(0)}_j, \\sum_{j=1}^p\\hat\\varphi_{1j}\\bx_{j}^{(0)}\\right\\rangle\\non\\\\ &=& \\sum_{i=1}^p\\sum_{j=1}^p\\hat\\varphi_{1i}\\hat\\varphi_{1j}\\langle \\bx^{(0)}_j, \\bx^{(0)}_{j'} \\rangle\\non\\\\ &=& \\sum_{i=1}^p\\sum_{j=1}^p\\hat\\varphi_{1i}\\hat\\varphi_{1j}\\delta_{ij}\\non\\\\ &=&\\sum_{i=1}^p\\hat\\varphi_{1i}^2\\non \\end{eqnarray}\\] since \\(\\bx_i^{(0)}\\) are orthogonal. On the other hand, we have \\[\\begin{equation} \\langle \\bb{z}_1, \\by \\rangle = \\sum_{i=1}^p \\hat\\varphi_{1i}\\langle \\bx^{(0)}_j, \\by\\rangle = \\sum_{i=1}^p \\hat\\varphi_{1i}^2\\non \\end{equation}\\] so we have \\(\\hat\\theta_1 =1\\) . (c) We have \\[\\begin{equation} \\hat\\by^{(1)} = \\hat\\by^{(0)} + \\bb{z}_1 = \\hat\\by^{(0)} + \\sum_{i=1}^p\\hat\\varphi_{1i}\\bx_{i}^{(0)}.\\non \\end{equation}\\] (d) For each \\(j=1,...,p\\) , \\[\\begin{eqnarray} \\bx_j^{(1)} &=& \\bx_j^{(0)} - \\frac{\\langle \\bb{z}_1, \\bx_{j}^{(0)} \\rangle}{\\langle \\bb{z}_1, \\bb{z}_1 \\rangle}\\cdot \\bb{z}_1\\non\\\\ &=&\\bx_j^{(0)} - \\frac{\\hat\\varphi_{1j}}{\\sum_{i=1}^p\\varphi_{1i}^2}\\bb{z}_1\\non\\\\ &=&\\bx_j^{(0)} - \\left(\\frac{\\hat\\varphi_{1j}}{\\sum_{i=1}^p\\hat\\varphi_{1i}^2}\\right)\\sum_{i=1}^p\\hat\\varphi_{1i}\\bx^{(0)}_i.\\non \\end{eqnarray}\\] Now we move to \\(m=2\\) . It's easy to see that for any \\(j=1,...,p\\) we have \\[\\begin{eqnarray} \\hat\\varphi_{2j} &=& \\langle \\bx^{(1)}_j, \\by \\rangle \\non\\\\ &=&\\langle \\bx_j^{(0)}, \\by \\rangle - \\left(\\frac{\\hat\\varphi_{1j}}{\\sum_{i=1}^p\\hat\\varphi_{1i}^2}\\right)\\sum_{i=1}^p\\hat\\varphi^2_{1i}\\non\\\\ &=&\\hat\\varphi_{1j} - \\hat\\varphi_{1j}=0.\\non \\end{eqnarray}\\] Therefore we see the algorithm stops in Step 2.","title":"Ex. 3.14"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-15/","text":"Ex. 3.15 Verify expression (3.64), and hence show that the partial least squares directions are a compromise between the ordinary regression coefficient and the principal component directions. Soln. 3.15 Note that \\[\\begin{equation} \\text{Corr}^2(\\by, \\bX\\alpha)\\text{Var}(\\bX\\alpha) =\\frac{\\text{Cov}^2(\\by, \\bX\\alpha)}{\\text{Var}(\\by)\\text{Var}(\\bX\\alpha)}\\text{Var}(\\bX\\alpha) = \\frac{\\text{Cov}^2(\\by, \\bX\\alpha)}{\\text{Var}(\\by)}\\non \\end{equation}\\] We are essentially solving \\[\\begin{eqnarray} \\max_\\alpha &&(\\by^T\\bX\\alpha)^2\\non\\\\ \\text{s.t.} &&\\|\\alpha\\|=1\\non\\\\ && \\alpha^T\\bb{S}\\hat\\varphi_l = 0, \\ l=1,...,m-1,\\non \\end{eqnarray}\\] where \\(\\bb{S}=\\bX^T\\bX\\) is the sample covariance matrix of the \\(\\bx_j\\) . We start with the case \\(m=1\\) , which immediately gives what we call the first canonical covariance variable (see Ex. 3.20 ) with \\[\\begin{equation} \\hat\\alpha_1 = \\bX^T\\by/\\|\\bX^T\\by\\|_2.\\non \\end{equation}\\] Note that \\(\\hat\\alpha_1 \\propto \\hat\\varphi_1\\) in Algorithm 3.3 in the text. The second canonical covariance variable, namely \\(\\hat\\alpha_2\\) , has to maximize the same objective with additional constraint \\(\\hat\\alpha_2^T\\bb{S}\\hat\\alpha_1=0\\) . It turns out \\[\\begin{equation} \\label{eq:3-15a} \\hat\\alpha_2 \\propto \\bX^T\\by - \\left(\\frac{\\by^T\\bX \\bb{S} \\bX^T\\by}{\\by^T\\bX \\bb{S}^2 \\bX^T\\by}\\right)\\bb{S}\\bX^T\\by. \\end{equation}\\] To see that, we first verify that \\[\\begin{eqnarray} \\hat\\alpha_2^T \\bb{S}\\hat\\alpha_1 &\\propto& \\by^T\\bX\\bb{S}\\bX^T\\by - \\left(\\frac{\\by^T\\bX \\bb{S} \\bX^T\\by}{\\by^T\\bX \\bb{S}^2 \\bX^T\\by}\\right)\\by^T\\bX\\bb{S}^T\\bb{S}\\bX^T\\by\\non\\\\ &=&0.\\non \\end{eqnarray}\\] Second, for \\(\\alpha_2\\) satisfying \\(\\alpha_2^T \\bb{S}\\hat\\alpha_1=0\\) , that is, \\(\\alpha_2^T \\bb{S}\\bX^T\\by=0\\) , we have the objective to maximize \\[\\begin{equation} \\alpha_2^T\\bX^T\\by = \\alpha_2^T\\left(\\bX^T\\by - \\left(\\frac{\\by^T\\bX \\bb{S} \\bX^T\\by}{\\by^T\\bX \\bb{S}^2 \\bX^T\\by}\\right)\\bb{S}\\bX^T\\by\\right).\\non \\end{equation}\\] Therefore we see \\(\\eqref{eq:3-15a}\\) holds. Note that \\(\\hat\\alpha_2\\propto \\hat\\varphi_2\\) in the Algorithm 3.3 in the text. Continuing this, we are able to derive \\(\\hat\\varphi_m\\) for \\(m\\ge 1\\) . Now we are ready to show that partial least squares (PLS) directions are a compromise between the ordinary regression coefficient (OLS) and the principal component directions (PCR). The regressors for OLS, PCR and PLS may be referred to as canonical correlation , canonical variance and canonical covariance variables respectively. A generalized criterion that encompasses all three methods is \\[\\begin{eqnarray} \\max_\\alpha &&(\\by^T\\bX\\alpha)^2\\left(\\alpha^T\\bX^T\\bX\\alpha\\right)^{\\frac{r}{1-r}-1}\\non\\\\ \\text{s.t.} &&\\|\\alpha\\|=1\\non\\\\ && \\alpha^T\\bb{S}\\hat\\alpha_l = 0, \\ l=1,...,m-1,\\non \\end{eqnarray}\\] where \\(r\\in [0, 1).\\) When \\(r =0\\) we recover OLS, and when \\(r \\ra 1\\) we get PCR. The case when \\(r=1/2\\) gives PLS. Note that this generalized regression is referred to as continuum regression . See paper Continuum Regression and Ridge Regression for more details.","title":"Ex. 3.15"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-16/","text":"Ex. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators in the orthogonal case. Soln. 3.16 In the orthogonal case, we have \\(\\bX^T\\bX=\\bI\\) . (1) For best-subset, note that the QR decomposition of \\(\\bX\\) can be written as \\(\\bX=\\bX\\bI\\) since \\(\\bX\\) is orthogonal. Also, \\(\\hat\\beta=(\\bX^T\\bX)^{-1}\\bX\\by=\\bX\\by\\) . Then, from Ex. 3.9 , we know that at each step \\(q\\) we need to choose \\(k\\) such that \\[\\begin{equation} k = \\underset{q < k \\le p}{\\operatorname{argmax}} \\bx^T_k\\by = \\underset{q < k \\le p}{\\operatorname{argmax}} \\hat\\beta_k.\\non \\end{equation}\\] Therefore we verify the formula \\(\\hat\\beta_j \\cdot \\bb{1}(|\\hat\\beta_j| \\ge |\\hat\\beta_{(M)}|)\\) . (2) For ridge regression, we know \\[\\begin{eqnarray} \\hat\\beta^{\\text{ridge}} &=& (\\bX^T\\bX + \\lambda\\bI)^{-1}\\bX^T\\by\\non\\\\ &=&\\frac{1}{1+\\lambda}\\bX^T\\by \\non\\\\ &=&\\frac{1}{1+\\lambda}\\hat\\beta,\\non \\end{eqnarray}\\] which is the desired formula. (3) For lasso, we are solving \\[\\begin{equation} \\min_{\\beta} \\frac{1}{2}\\|\\by-\\bX\\beta\\|^2_2 + \\lambda\\|\\beta\\|_1.\\non \\end{equation}\\] When \\(\\bX\\) is orthogonal, recall that \\(\\hat\\beta = \\bX^T\\by\\) , we have \\[\\begin{eqnarray} &&\\min_{\\beta} \\left(-\\by^T\\bX\\beta + \\frac{1}{2}\\beta^T\\beta\\right) + \\gamma \\|\\beta\\|_1\\non\\\\ &=&\\min_{\\beta} \\left(-\\by^T\\bX\\beta + \\frac{1}{2}\\|\\beta\\|_2^2\\right) + \\gamma \\|\\beta\\|_1\\non\\\\ &=&\\min_{\\beta} \\frac{1}{2}\\|\\beta\\|_2^2 -\\hat\\beta^T\\beta + \\gamma \\|\\beta\\|_1\\non\\\\ &=&\\min_{\\beta} \\sum_{j=1}^p\\left(\\frac{1}{2}\\beta_i^2 - \\hat\\beta_i\\beta_i + \\gamma |\\beta_i|\\right).\\non \\end{eqnarray}\\] Therefore it suffices to solve the minimization for each \\(i\\) individually. Consider two cases: a) When \\(\\hat\\beta_j \\ge 0\\) , for optimal solution \\(\\beta^\\ast\\) , we need \\(\\beta^\\ast_j\\ge 0\\) . Consider if \\(\\beta^\\ast_j < 0\\) , we could choose \\(\\beta^{\\text{new}} = -\\beta^\\ast -\\epsilon\\) for some \\(\\epsilon>0\\) , so that \\(\\beta^\\ast\\) is strictly less optimal than \\(\\beta^{\\text{new}}\\) (which contradicts the optimality of \\(\\beta^\\ast\\) .) Therefore, we need to solve \\[\\begin{equation} \\min_{\\beta_j\\ge 0}\\frac{1}{2}\\beta_j^2 + (\\gamma-\\hat\\beta_j)\\beta_j,\\non \\end{equation}\\] which is quadratic in \\(\\beta_j\\) and its solution is easily seen to be \\((\\hat\\beta_j-\\gamma)_+\\) . b) When \\(\\hat\\beta_j < 0\\) , similarly, we need \\(\\beta_j < 0\\) . Thus, we need to solve \\[\\begin{equation} \\min_{\\beta_j < 0}\\frac{1}{2}\\beta_j^2 - (\\gamma+\\hat\\beta_j)\\beta_j,\\non \\end{equation}\\] which has the solution \\((\\hat\\beta_j + \\gamma)_-\\) . In both cases, the solution can be written as \\(\\text{sign}(\\hat\\beta_j)(|\\hat\\beta_j|-\\lambda)_+\\) .","title":"Ex. 3.16"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-17/","text":"Ex. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in Chapter 1. Soln. 3.17 This is a programming exercise. Note that: The predictors need to be standardized to have zero mean and unit variance. The estimate for prediction error is from cross validation. See more details in Section 7.10 in the text. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 import pathlib import numpy as np import pandas as pd from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.feature_selection import RFE from sklearn.cross_decomposition import PLSRegression # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () data = pd . read_csv ( DATA_PATH . joinpath ( \"prostate.csv\" ), header = 0 ) data_train = data . loc [ data [ 'train' ] == 'T' ] data_test = data . loc [ data [ 'train' ] == 'F' ] X_train = data_train . loc [:, 'lcavol' : 'pgg45' ] X_test = data_test . loc [:, 'lcavol' : 'pgg45' ] y_train = pd . DataFrame ( data_train . loc [:, 'lpsa' ]) y_test = pd . DataFrame ( data_test . loc [:, 'lpsa' ]) # a utility function to print results def displayResults ( name , model ): if name == \"Best Subset\" : intercept = model . estimator_ . intercept_ [ 0 ] coef = model . estimator_ . coef_ elif name == \"PCR\" : lr_tuple = model . steps [ 1 ] lr_est = lr_tuple [ 1 ] intercept = lr_est . intercept_ [ 0 ] coef = lr_est . coef_ elif name == \"PLS\" : intercept = model . y_mean_ [ 0 ] coef = model . coef_ else : # default intercept = model . intercept_ [ 0 ] coef = model . coef_ print ( \"Intercept of final {} model is: {:.3f} \" . format ( name , intercept )) print ( \"Coefficient of final {} model is: {} \" . format ( name , coef )) # pre-processing pipeline = Pipeline ([( 'std_scaler' , StandardScaler ())]) X_train_prepared = pipeline . fit_transform ( X_train ) X_test_prepared = pipeline . transform ( X_test ) # define various models and their parameter grids models = [ { \"name\" : \"LS\" , \"params\" : [{ 'fit_intercept' : [ True ]}], \"estimator\" : LinearRegression () }, { \"name\" : \"Ridge\" , \"params\" : [ { 'alpha' : [ 0.01 , 0.1 , 1 , 2 , 3 , 4 , 5 , 7 , 10 ]} ], \"estimator\" : Ridge () }, { \"name\" : \"Lasso\" , \"params\" : [ { 'alpha' : [ 0.001 , 0.01 , 0.1 , 0.2 , 0.5 , 1 , 5 ]} ], \"estimator\" : Lasso () }, { \"name\" : \"PLS\" , \"params\" : [ { 'n_components' : np . arange ( 1 , 9 )} ], \"estimator\" : PLSRegression ( scale = False ) }, { \"name\" : \"PCR\" , \"params\" : [ { 'pca__n_components' : np . arange ( 1 , 9 )} ], \"estimator\" : Pipeline ( steps = [( 'pca' , PCA ()), ( 'linear regression' , LinearRegression ())]) }, { \"name\" : \"Best Subset\" , \"params\" : [ { 'n_features_to_select' : np . arange ( 1 , 9 )} ], \"estimator\" : RFE ( LinearRegression ()) } ] for model in models : name = model [ \"name\" ] print ( \"******** Start running model: {} **********\" . format ( name )) estimator = model [ \"estimator\" ] param_grid = model [ \"params\" ] grid_search = GridSearchCV ( estimator , param_grid , cv = 10 , scoring = 'neg_mean_squared_error' , return_train_score = True ) grid_search . fit ( X_train_prepared , y_train ) cv_res = grid_search . cv_results_ for mean_score , params in zip ( cv_res [ \"mean_test_score\" ], cv_res [ \"params\" ]): print ( \"CV results for model {} with parameter {} is {} \" . format ( name , params , np . sqrt ( - mean_score ))) final_model = grid_search . best_estimator_ final_y_pred = final_model . predict ( X_test_prepared ) final_test_error = mean_squared_error ( final_y_pred , y_test ) displayResults ( name , final_model ) print ( \"Test error of final {} model is : {:.3f} \" . format ( name , final_test_error )) print ( \"******** End running model: {} **********\" . format ( name ))","title":"Ex. 3.17"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-18/","text":"Ex. 3.18 Read about conjugate gradient algorithms (Murray et al., 1981, for example) Practical Optimization , and establish a connection between these algorithms and partial least squares. Soln. 3.18 We briefly review the conjugate gradient algorithm described in Murray et al., 1981. The conjugate-gradient method described in Section 4.8.3.1 in Practical Optimization can be applied to minimize the quadratic function \\(c^T + \\frac{1}{2}x^TGx\\) , where \\(G\\) is symmetric and positive definite, it computes the solution of the system \\[\\begin{equation} Gx = -c.\\non \\end{equation}\\] When describing the linear conjugate-gradient method, it's customary to use the notation \\(r_j\\) (for residual) for the gradient vector \\(c+Gx_j\\) . To initiate the iterations, we adopt the convention that \\(\\beta_{-1}=0, p_{-1}=0\\) . Given \\(x_0\\) and \\(r_0=c+ Gx_0\\) , each iteration includes the following steps for \\(k=0,1,...\\) : \\[\\begin{eqnarray} p_k &=&-r_k + \\beta_{k-1}p_{k-1}\\non\\\\ \\alpha_k &=& \\frac{\\|r_k\\|_2^2}{p_k^TGp_k}\\non\\\\ x_{k+1} &=& x_k + \\alpha_kp_k\\non\\\\ r_{k+1} &=& r_k + \\alpha_kGp_k\\non\\\\ \\beta_k &=& \\frac{\\|r_{k+1}\\|_2^2}{\\|r_k\\|_2^2}\\non \\end{eqnarray}\\] In theory, the algorithm will compute the exact solution within a fixed number of iterations. In particular, the method has the property that, if exact arithmetic is used, convergence will occur in \\(m\\le n\\) iterations, where \\(m\\) is the number of distinct eigenvalues of \\(G\\) . The connection between this algorithm (iteratively getting \\(\\hat\\beta^{m}\\) ) and PLS (iteratively getting \\(\\hat \\by^m\\) ) is \\(\\hat \\by^m = \\bX\\hat\\beta^m\\) .","title":"Ex. 3.18"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-19/","text":"Ex. 3.19 Show that \\(\\|\\hat\\beta^{\\text{ridge}}\\|\\) increases as its tuning parameter \\(\\lambda\\ra 0\\) . Does the same property hold for the lasso and partial least squares estimates? For the latter, consider the tuning parameter to be the successive steps in the algorithm. Soln. 3.19 Recall the SVD decomposition of \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T\\) . Here \\(\\bb{U}\\) and \\(\\bb{V}\\) are \\(N\\times p\\) and \\(p\\times p\\) orthogonal matrices, and \\(\\bb{D}\\) is a \\(p\\times p\\) diagonal matrix. So we have \\[\\begin{eqnarray} \\beta^{\\text{ridge}} &=& (\\bX^T\\bX + \\lambda\\bI)^{-1}\\bX^T\\by\\non\\\\ &=&\\left(\\bb{V}\\bb{D}^2\\bb{V}^T + \\lambda\\bI\\right)^{-1}\\bb{V}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&\\left(\\bb{V}(\\bb{D}^2 + \\lambda\\bI\\right)\\bb{V}^T)^{-1}\\bb{V}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&\\bb{V}^T(\\bb{D}^2 + \\lambda\\bI)^{-1}\\bb{D}\\bb{U}^T\\by.\\non \\end{eqnarray}\\] Therefore, \\[\\begin{eqnarray} \\|\\beta^{\\text{ridge}}\\|_2^2 &=& \\by^T\\bb{U}\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-1}(\\bb{D}^2+\\lambda\\bI)^{-1}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&(\\bb{U}^T\\by)^T[\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-2}\\bb{D}](\\bb{U}^T\\by)\\non\\\\ &=&\\sum_{j=1}^p\\frac{d_j^2(\\bb{U}^T\\by)_j^2}{(d_j^2 + \\lambda)^2}.\\non \\end{eqnarray}\\] where \\(\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-2}\\bb{D}\\) represents a diagonal matrix with elements \\(\\frac{d_j^2}{(d_j^2 + \\lambda)^2}\\) . Therefore we see that \\(\\|\\hat\\beta^{\\text{ridge}}\\|\\) increases as its tuning parameter \\(\\lambda\\ra 0\\) . For Lasso, there is no explicit solution as Ridge, however, we can start with the orthogonal case, where the formula is given in Ex. 3.16 . It's easy to see that \\(\\|\\hat\\beta^{\\text{lasso}}\\|_1\\) increases as \\(\\lambda\\ra 0\\) . For the general case, recall the dual form of Lasso defined in (3.51) and (3.52) in the text. It's easy to see that \\(t\\) in (3.51) and \\(\\lambda\\) in (3.52) have an inverse relationship, therefore, as \\(\\lambda\\ra 0\\) , \\(t\\) increases and so does the norm of optimal solutions (see Figure 3.11 for an intuitive illustration in \\(\\mathbb{R}^2\\) ).","title":"Ex. 3.19"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-20/","text":"Ex. 3.20 Consider the canonical-correlation problem (3.67). Show that the leading pair of canonical variates \\(u_1\\) and \\(v_1\\) solve the problem \\[\\begin{equation} \\max_{\\substack{u^T(\\bb{Y}^T\\bb{Y})u=1\\\\ v^T(\\bb{X}^T\\bb{X})v=1}} u^T(\\bb{Y}^T\\bb{X})v,\\non \\end{equation}\\] a generalized SVD problem. Show that the solution is given by \\(u_1=(\\bb{Y}^T\\bb{Y})^{-\\frac{1}{2}}u_1^\\ast\\) , and \\(v_1=(\\bb{X}^T\\bb{X})^{-\\frac{1}{2}}v_1^\\ast\\) , where \\(u_1^\\ast\\) and \\(v_1^\\ast\\) are the leading left and right singular vectors in \\[\\begin{equation} \\label{eq:3-20a} (\\bb{Y}^T\\bb{Y})^{-\\frac{1}{2}}(\\bb{Y}^T\\bb{X})(\\bb{X}^T\\bb{X})^{-\\frac{1}{2}}=\\bb{U}^\\ast\\bb{D}^\\ast\\bb{V}^{\\ast T}. \\end{equation}\\] Show that the entire sequence \\(u_m, v_m, m=1,...,\\min(K,p)\\) is also given by (3.87) ( \\(\\eqref{eq:3-20a}\\) above). Soln. 3.20 First, the correlation (3.67) in the text, for each \\(m\\) , is given by \\[\\begin{eqnarray} \\text{Corr}(\\bY u_m, \\bX v_m) &=& \\frac{\\text{Cov}(\\bY u_m, \\bX v_m)}{\\sqrt{\\text{Var}(\\bY u_m)\\text{Var}(\\bX v_m)}}\\non\\\\ &=&\\frac{u_m^T(\\bY^T \\bX)v_m}{\\sqrt{\\text{Var}(\\bY u_m)\\text{Var}(\\bX v_m)}}.\\non \\end{eqnarray}\\] Therefore we know \\(u_1\\) and \\(v_1\\) solve the problem \\[\\begin{equation} \\max_{\\substack{u^T(\\bb{Y}^T\\bb{Y})u=1\\\\ v^T(\\bb{X}^T\\bb{X})v=1}} u^T(\\bb{Y}^T\\bb{X})v.\\non \\end{equation}\\] To find its solution, we write out the Lagrangian function \\[\\begin{equation} L(u, v, \\lambda_1, \\lambda_2) = u^T(\\bb{Y}^T\\bb{X})v - \\frac{\\lambda_1}{2}(u^T(\\bb{Y}^T\\bb{Y})u-1)-\\frac{\\lambda_2}{2}(v^T(\\bb{X}^T\\bb{X})v-1).\\non \\end{equation}\\] Taking derivatives and setting them to zero yield \\[\\begin{eqnarray} \\frac{\\partial L(u, v, \\lambda_1, \\lambda_2)}{\\partial u} &=& \\bY^T\\bX v - \\lambda_1 \\bY^T\\bY u = 0\\non\\\\ \\frac{\\partial L(u, v, \\lambda_1, \\lambda_2)}{\\partial v} &=& \\bX^T\\bY u - \\lambda_2 \\bX^T\\bX v = 0.\\label{eq:3-20b} \\end{eqnarray}\\] Multiplying the first equation by \\(u^T\\) and the second by \\(v^T\\) , and noting the constraints (e.g., \\(u^T(\\bb{Y}^T\\bb{Y})u=1\\) ), we obtain \\[\\begin{eqnarray} u^T\\bY^T\\bX v &=& \\lambda_1\\non\\\\ v^T\\bX^T\\bY u &=& \\lambda_2.\\label{eq:3-20c} \\end{eqnarray}\\] We see that \\(\\lambda_1 = \\lambda_2 = u^T\\bY^T\\bX v\\) , and we denote \\(\\lambda := \\lambda_1 = \\lambda_2\\) . Denote \\(\\bb{M} = (\\bb{Y}^T\\bb{Y})^{-\\frac{1}{2}}(\\bb{Y}^T\\bb{X})(\\bb{X}^T\\bb{X})^{-\\frac{1}{2}}\\) , we need to find the relation between \\(\\bb{M}\\) and the pair \\(u\\) and \\(v\\) . Recall the definition of \\(u_1^\\ast\\) and \\(v_1^\\ast\\) , it's easy to verify that \\(\\eqref{eq:3-20b}\\) can be rewritten as \\[\\begin{eqnarray} Mv_1^\\ast &=& \\lambda u_1^\\ast,\\non\\\\ M^Tu_1^\\ast &=& \\lambda v_1^\\ast.\\non \\end{eqnarray}\\] Therefore we know \\(u_1=(\\bb{Y}^T\\bb{Y})^{-\\frac{1}{2}}u_1^\\ast\\) and \\(v_1=(\\bb{X}^T\\bb{X})^{-\\frac{1}{2}}v_1^\\ast\\) solve the optimization problem. For the entire sequence \\(u_m, v_m, m=1,...,\\min(K,p)\\) , we have already solved the case \\(m=1\\) . Consider \\(1 < k \\le \\min(K, p)\\) and assume \\(u_j, v_j\\) for \\(1\\le j < k\\) have been solved as above. Then the problem becomes \\[\\begin{equation} \\max_{\\substack{u^T(\\bb{Y}^T\\bb{Y})u=1\\\\ v^T(\\bb{X}^T\\bb{X})v=1 \\\\ u^Tu_j = 0\\ \\forall j < k\\\\ v^Tv_j = 0\\ \\forall j < k}} u^T(\\bb{Y}^T\\bb{X})v.\\non \\end{equation}\\] The Lagrangian becomes \\[\\begin{eqnarray} L &=& u^T(\\bb{Y}^T\\bb{X})v - \\frac{\\lambda_1}{2}(u^T(\\bb{Y}^T\\bb{Y})u-1)-\\frac{\\lambda_2}{2}(v^T(\\bb{X}^T\\bb{X})v-1)\\non\\\\ &&-\\sum_{j<k}\\alpha_ju^Tu_j - \\sum_{j<k}\\beta_jv^Tv_j.\\non \\end{eqnarray}\\] Similar to \\(\\eqref{eq:3-20b}\\) , we have \\[\\begin{eqnarray} \\frac{\\partial L}{\\partial u} &=& \\bY^T\\bX v - \\lambda_1 \\bY^T\\bY u -\\sum_{j<k}\\alpha_ju_j= 0\\non\\\\ \\frac{\\partial L}{\\partial v} &=& \\bX^T\\bY u - \\lambda_2 \\bX^T\\bX v -\\sum_{j<k}\\beta_jv_j= 0.\\non \\end{eqnarray}\\] We multiply the first equation by \\(u^T\\) and the second by \\(v^T\\) , and note the constraints here, in addition to \\(u^T(\\bb{Y}^T\\bb{Y})u=1\\) we have \\(u^Tu_j=0\\) for \\(j < k\\) , we obtain the same set of equations as \\(\\eqref{eq:3-20c}\\) . Then the rest follows the same arguments.","title":"Ex. 3.20"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-21/","text":"Ex. 3.21 Show that the solution to the reduced-rank regression problem (3.68), with \\(\\boldsymbol{\\Sigma}\\) estimated by \\(\\bb{Y}^T\\bb{Y}/N\\) , is given by (3.69). Hint : Transform \\(\\bb{Y}\\) to \\(\\bb{Y}^\\ast=\\bb{Y}\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\) , and solved in terms of the canonical vectors \\(u_m^\\ast\\) . Show that \\(\\bb{U}_m=\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\bb{U}_m^\\ast\\) , and a generalized inverse is \\(\\bb{U}_m^- = \\bb{U}_m^{\\ast T}\\boldsymbol{\\Sigma}^{\\frac{1}{2}}\\) . Soln. 3.21 The problem can be rewritten as \\[\\begin{eqnarray} \\underset{\\text{rank}(\\bb{B})=m}{\\operatorname{argmin}} \\text{trace}\\left[(\\bY-\\bX\\bb{B})\\bm{\\Sigma}^{-1}(\\bY-\\bX\\bb{B})^T\\right].\\non \\end{eqnarray}\\] By properties of trace operator and the fact that \\(\\bm{\\Sigma}\\) is definite positive and symmetric, we have \\[\\begin{eqnarray} &&\\text{trace}\\left[(\\bY-\\bX\\bb{B})\\bm{\\Sigma}^{-1}(\\bY-\\bX\\bb{B})^T\\right]\\non\\\\ &=&\\text{trace}\\left[(\\bY\\bm{\\Sigma}^{-\\frac{1}{2}}-\\bX\\bb{B}\\bm{\\Sigma}^{-\\frac{1}{2}})(\\bY\\bm{\\Sigma}^{-\\frac{1}{2}}-\\bX\\bb{B}\\bm{\\Sigma}^{-\\frac{1}{2}})^T\\right]\\non\\\\ &=&\\text{trace}\\left[(\\bY\\bm{\\Sigma}^{-\\frac{1}{2}}-\\bX\\bb{B}\\bm{\\Sigma}^{-\\frac{1}{2}})(\\bm{\\Sigma}^{-\\frac{1}{2}}\\bY^T-\\bm{\\Sigma}^{-\\frac{1}{2}}\\bB^T\\bX^T)\\right]\\non\\\\ &=&\\text{trace}\\left[\\bY\\bm{\\Sigma}^{-1}\\bY^T + \\bX\\bB\\bm{\\Sigma}^{-\\frac{1}{2}}\\bm{\\Sigma}^{-\\frac{1}{2}}\\bB^T\\bX^T - 2\\bm{\\Sigma}^{-\\frac{1}{2}}\\bY^T\\bX\\bB\\bm{\\Sigma}^{-\\frac{1}{2}} \\right]\\non\\\\ &=&\\text{trace}\\left[\\bY\\bm{\\Sigma}^{-1}\\bY^T + \\bm{\\Sigma}^{-\\frac{1}{2}}\\bB^T\\bX^T\\bX\\bB\\bm{\\Sigma}^{-\\frac{1}{2}} - 2\\bm{\\Sigma}^{-\\frac{1}{2}}\\bY^T\\bX\\bB\\bm{\\Sigma}^{-\\frac{1}{2}} \\right]\\non\\\\ &=&\\text{trace}\\left[\\bY\\bm{\\Sigma}^{-1}\\bY^T+\\bm{\\Sigma}^{-1}\\bY^T\\bX(\\bX^T\\bX)^{-1}\\bX^T\\bY\\right] + \\text{trace}\\left[\\bb{C}\\bb{C}^T\\right]\\label{eq:3-21a} \\end{eqnarray}\\] where \\[\\begin{equation} \\label{eq:3-21b} \\bb{C} = \\bm{\\Sigma}^{-\\frac{1}{2}}\\bB^T(\\bX^T\\bX)^{\\frac{1}{2}}-\\bm{\\Sigma}^{-\\frac{1}{2}}(\\bY^T\\bX)(\\bX^T\\bX)^{-\\frac{1}{2}}.\\non \\end{equation}\\] Note that the first summand in \\(\\eqref{eq:3-21a}\\) is independent of \\(\\bB\\) , therefore the original problem reduces to the standard low-rank approximation problem \\[\\begin{equation} \\label{eq:3-21c} \\underset{\\text{rank}(\\bb{B})=m}{\\operatorname{argmin}}\\|\\bm{\\Sigma}^{-\\frac{1}{2}}\\bB^T(\\bX^T\\bX)^{\\frac{1}{2}}-\\bm{\\Sigma}^{-\\frac{1}{2}}(\\bY^T\\bX)(\\bX^T\\bX)^{-\\frac{1}{2}}\\|_F^2, \\end{equation}\\] where \\(\\|M\\|_F^2 = \\text{trace}(M^TM)\\) is the squared Frobenius norm for matrix \\(M\\) (see, e.g., page 540 in the text) By standard results in low-rank approximation (e.g., Eckart\u2013Young\u2013Mirsky theorem), we know the optimal \\(\\hat\\bB(m)\\) with rank \\(m\\) that solves problem \\(\\eqref{eq:3-21c}\\) satisfies \\[\\begin{equation} \\label{eq:3-21d} \\bm{\\Sigma}^{-\\frac{1}{2}}\\hat\\bB(m)^T(\\bX^T\\bX)^{\\frac{1}{2}} = \\sum_{i=1}^m d_i\\bb{u}_i\\bb{v}_i^T \\end{equation}\\] where \\(\\bb{u}_i\\) , \\(\\bb{v}_i\\) and \\(d_i\\) are obtained from the following SVD \\[\\begin{equation} \\label{eq:3-21e} \\bm{\\Sigma}^{-\\frac{1}{2}}(\\bY^T\\bX)(\\bX^T\\bX)^{-\\frac{1}{2}} = \\bb{U}^\\ast\\bb{D}^\\ast\\bb{V}^{\\ast T}. \\end{equation}\\] Since \\(\\bb{U}^\\ast\\) has orthogonal columns, \\(\\eqref{eq:3-21e}\\) yields \\[\\begin{equation} \\bb{U}^{\\ast ^T}\\bm{\\Sigma}^{-\\frac{1}{2}}(\\bY^T\\bX)(\\bX^T\\bX)^{-\\frac{1}{2}} = \\bb{D}^\\ast\\bb{V}^{\\ast ^T}\\non \\end{equation}\\] and thus by matrix transpose \\[\\begin{equation} \\bb{V}^\\ast\\bb{D}^\\ast = (\\bX^T\\bX)^{-\\frac{1}{2}}\\bX^T\\bY\\bm{\\Sigma}^{-\\frac{1}{2}}\\bb{U}^\\ast.\\non \\end{equation}\\] Then, by \\(\\eqref{eq:3-21d}\\) , we know \\[\\begin{eqnarray} \\label{eq:3-21f} \\hat \\bB(m) &=& (\\bX^T\\bX)^{-\\frac{1}{2}}\\left(\\sum_{i=1}^m (\\bb{v}_i d_i)\\cdot \\bb{u}_i^T\\right)\\bm{\\Sigma}^{\\frac{1}{2}}\\non\\\\ &=&(\\bX^T\\bX)^{-\\frac{1}{2}}\\left((\\bX^T\\bX)^{-\\frac{1}{2}}\\bX^T\\bY\\bm{\\Sigma}^{-\\frac{1}{2}}\\bb{U}_m^\\ast \\cdot \\bb{U}_m^{\\ast ^T}\\right)\\bm{\\Sigma}^{\\frac{1}{2}}\\non\\\\ &=&(\\bX^T\\bX)^{-1}\\bX^T\\bY\\bb{U}_m\\bb{U}_m^-\\non\\\\ &=&\\hat \\bB\\bb{U}_m\\bb{U}_m^-, \\end{eqnarray}\\] where \\(\\bb{U}_m=\\bm{\\Sigma}^{-\\frac{1}{2}}\\bb{U}_m^\\ast\\) and \\(\\bb{U}_m^- = \\bb{U}_m^{\\ast T}\\bm{\\Sigma}^{\\frac{1}{2}}\\) . Note that by far we haven't used the assumption that \\(\\bm{\\Sigma}=\\bY^T\\bY/N\\) . By \\(\\eqref{eq:3-21c}\\) , it's easy to see that the solution remains unchanged if we multiply a constant to \\(\\bm{\\Sigma}\\) . So without loss of generality, we assume \\(\\bm{\\Sigma} = \\bY^T\\bY\\) , then \\(\\eqref{eq:3-21e}\\) becomes exactly the same as (3.87) in the text.","title":"Ex. 3.21"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-22/","text":"Ex. 3.22 Show that the solution in Ex. 3.21 does not change if \\(\\Sigma\\) is estimated by the more natural quantity \\((\\bb{Y}-\\bb{X}\\hat{\\bb{B}})^T(\\bb{Y}-\\bb{X}\\hat{\\bb{B}})/(N-pK)\\) . Soln. 3.22 Note that in our solution for Ex. 3.21 , we don't assume the form of \\(\\Sigma\\) so the same arguments applies here.","title":"Ex. 3.22"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-23/","text":"Ex. 3.23 Consider a regression problem with all variables and response having mean zero and standard deviation one. Suppose also that each variable has identical absolute correlation with the response: \\[\\begin{equation} \\frac{1}{N}|\\langle \\bb{x}_j, \\bb{y} \\rangle| = \\lambda, \\ j=1,...,p.\\non \\end{equation}\\] Let \\(\\hat \\beta\\) be the least-squares coefficient of \\(\\bb{y}\\) on \\(\\bX\\) , and let \\(\\bb{u}(\\alpha) = \\alpha\\bX\\hat\\beta\\) for \\(\\alpha\\in [0,1]\\) be the vector that moves a fraction \\(\\alpha\\) toward the least squares fit \\(\\bb{u}\\) . Let \\(RSS\\) be the residual sum-of-squares from the full least squares fit. (a) Show that \\[\\begin{equation} \\frac{1}{N}|\\langle \\bx_j, \\by-\\bb{u}(\\alpha)\\rangle| = (1-\\alpha)\\lambda, \\ j=1,...,p,\\non \\end{equation}\\] and hence the correlations of each \\(\\bx_j\\) with the residuals remain equal in magnitude as we progress toward \\(\\bb{u}\\) . (b) Show that these correlations are all equal to \\[\\begin{equation} \\lambda(\\alpha) = \\frac{(1-\\alpha)}{\\sqrt{(1-\\alpha)^2 + \\frac{\\alpha(2-\\alpha)}{N}\\cdot RSS}} \\cdot \\lambda,\\non \\end{equation}\\] and hence they decrease monotonically to zero. (c) Use these results to show that the LAR algorithm in Section 3.4.4 keeps the correlations tied and monotonically decreasing, as claimed in (3.55). Soln. 3.23 (a) By definition we have \\[\\begin{eqnarray} \\frac{1}{N}|\\langle \\bX, \\by-\\bb{u}(\\alpha)\\rangle| &=& \\frac{1}{N}|\\langle \\bX, \\by - \\alpha\\bX\\langle\\bX, \\bX\\rangle^{-1}\\langle\\bX, \\by\\rangle|\\non\\\\ &=&\\frac{1}{N}|\\langle\\bX, \\by\\rangle - \\alpha \\langle\\bX, \\by\\rangle|\\non\\\\ &=&(1-\\alpha) \\frac{1}{N}|\\langle\\bX, \\by\\rangle|.\\non \\end{eqnarray}\\] Since \\(\\frac{1}{N}|\\langle \\bb{x}_j, \\bb{y} \\rangle| = \\lambda\\) , we have \\[\\begin{equation} \\frac{1}{N}|\\langle \\bx_j, \\by-\\bb{u}(\\alpha)\\rangle| = (1-\\alpha)\\lambda, \\ j=1,...,p.\\non \\end{equation}\\] (b) From (a), the correlations are \\[\\begin{equation} \\frac{(1-\\alpha)\\lambda}{\\sqrt{\\frac{\\langle \\bx_j, \\bx_j\\rangle}{N}}\\sqrt{\\frac{\\langle \\by-\\bb{u}(\\alpha),\\by-\\bb{u}(\\alpha)\\rangle }{N}}}=\\frac{(1-\\alpha)\\lambda}{\\sqrt{\\frac{\\langle \\by-\\bb{u}(\\alpha),\\by-\\bb{u}(\\alpha)\\rangle }{N}}}.\\non \\end{equation}\\] We need to calculate \\(\\langle \\by-\\bb{u}(\\alpha),\\by-\\bb{u}(\\alpha)\\rangle\\) . By definition of \\(\\bb{u}(\\alpha)\\) , we have \\[\\begin{eqnarray} \\langle \\by-\\bb{u}(\\alpha),\\by-\\bb{u}(\\alpha)\\rangle &=& \\langle\\by,\\by\\rangle + \\alpha^2\\langle \\bX, \\bX\\rangle^{-1}\\langle\\bX, \\by\\rl^2-2\\alpha\\langle\\bX, \\bX\\rl^{-1} \\langle\\bX, \\by\\rl^2\\non\\\\ &=&\\langle\\by,\\by\\rangle + (\\alpha^2-2\\alpha)\\langle\\bX, \\bX\\rl^{-1} \\langle\\bX, \\by\\rl^2.\\non \\end{eqnarray}\\] On the other hand, we have \\[\\begin{equation} \\text{RSS} = \\langle \\by, \\by\\rl - \\langle\\bX,\\bX\\rl^{-1}\\langle\\bX, \\by\\rl^2.\\non \\end{equation}\\] So we have \\[\\begin{eqnarray} \\frac{1}{N}\\langle \\by-\\bb{u}(\\alpha),\\by-\\bb{u}(\\alpha)\\rangle &=& \\frac{1}{N}\\langle\\by,\\by\\rangle + \\frac{1}{N}(\\alpha^2-2\\alpha)\\left(\\langle\\by,\\by\\rangle-\\text{RSS}\\right)\\non\\\\ &=&(\\alpha-1)^2\\frac{1}{N}\\langle\\by,\\by\\rangle + \\frac{(2\\alpha-\\alpha^2)}{N}\\text{RSS}\\non\\\\ &=&(\\alpha-1)^2+\\frac{\\alpha(2-\\alpha)}{N}\\text{RSS}.\\non \\end{eqnarray}\\] The proof is now complete. (c) When \\(\\alpha=0\\) we have \\(\\lambda(0)=\\lambda\\) ; when \\(\\alpha=1\\) we have \\(\\lambda(1)=0\\) , where all correlations are tied and decrease from \\(\\lambda\\) to 0 as \\(\\alpha\\) moves from 0 to 1.","title":"Ex. 3.23"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-24/","text":"Ex. 3.24 LAR directions. Using the notation around equation (3.55) on page 74, show that the LAR direction makes an equal angle with each of the predictors in \\(\\mathcal{A}_k\\) . Soln. 3.24 By definition of LAR we have \\[\\begin{eqnarray} \\bX_{\\mathcal{A}_k}^Tu_k &=&\\bX_{\\mathcal{A}_k}^T(\\bX_{\\mathcal{A}_k}\\delta_k)\\non\\\\ &=&\\bX_{\\mathcal{A}_k}^T\\bX_{\\mathcal{A}_k}(\\bX_{\\mathcal{A}_k}^T\\bX_{\\mathcal{A}_k})^{-1}\\bX_{\\mathcal{A}_k}^Tr_k\\non\\\\ &=&\\bX_{\\mathcal{A}_k}^Tr_k.\\non \\end{eqnarray}\\] By procedures of LAR, a new predictor \\(x_{j'}\\) is added when the absolute value of \\(x_{j'}^Tr\\) equals that of \\(x_j^Tr\\) for all predictors \\(x_j\\in \\mathcal{A}_k\\) , we know the direction \\(u_k\\) makes an equal angle with all predictors in \\(\\mathcal{A}_k\\) .","title":"Ex. 3.24"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-25/","text":"Ex. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2 Least Angle Regression ).} Starting at the beginning of the \\(k\\) th step of the LAR algorithm, derive expressions to identify the next variable to enter the active set at step \\(k+1\\) , and the value of \\(\\alpha\\) at which this occurs (using the notation around equation (3.55) on page 74). Soln. 3.25 Suppose the next step of LAR algorithm update is \\[\\begin{equation} \\label{eq:3-25a} \\hat{\\textbf{f}}_k(\\alpha) = \\hat{\\textbf{f}}_k + \\alpha\\cdot \\textbf{u}_k. \\end{equation}\\] Then, the current correlation with residual, for each \\(\\textbf{x}_j\\) , becomes \\[\\begin{equation} \\label{eq:3-25b} c_j(\\alpha) = \\textbf{x}_j^T(\\textbf{y}-\\hat{\\textbf{f}}_k(\\alpha)) = \\textbf{x}_j^T\\textbf{r}_k - \\alpha\\textbf{x}_j^T\\textbf{u}_k. \\end{equation}\\] By Ex. 3.23 and Ex. 3.24 , \\(c_j(\\alpha)\\) and \\(\\textbf{x}_j^T\\textbf{u}_k\\) are both the same for \\(j\\in \\mathcal{A}_k\\) . Therefore, for \\(j\\in\\mathcal{A}_k\\) , we have \\[\\begin{equation} \\label{eq:3-25c} |c_j(\\alpha)| = \\hat C - \\alpha A, \\end{equation}\\] showing that all of the maximal absolute current correlations decline equally where \\[\\begin{eqnarray} \\hat C &=& \\max_{j}|\\textbf{x}_j^T\\textbf{r}_k|\\non\\\\ A &=& \\textbf{x}_j^T\\textbf{u}_k.\\non \\end{eqnarray}\\] For \\(j\\notin \\mathcal{A}_k\\) , equating \\(\\eqref{eq:3-25b}\\) with \\(\\eqref{eq:3-25c}\\) shows that the optimal value \\(\\hat\\alpha\\) is \\[\\begin{equation} \\hat\\alpha = \\frac{\\hat C - \\textbf{x}_j^T\\textbf{r}_k}{A-\\textbf{x}_j^T\\textbf{u}_k}.\\non \\end{equation}\\] Likewise for \\(-c_j(\\alpha)\\) , the optimal values is \\[\\begin{equation} \\hat\\alpha = \\frac{\\hat C + \\textbf{x}_j^T\\textbf{r}_k}{A + \\textbf{x}_j^T\\textbf{u}_k}.\\non \\end{equation}\\] Therefore, we conclude that the optimal value \\(\\hat\\alpha\\) can be written as \\[\\begin{equation} \\label{eq:3-25d} \\hat\\alpha = {\\min_{j\\in\\mathcal{A}_k^c}}^+\\Big\\{\\frac{\\hat C - \\textbf{x}_j^T\\textbf{r}_k}{A-\\textbf{x}_j^T\\textbf{u}_k}, \\frac{\\hat C + \\textbf{x}_j^T\\textbf{r}_k}{A + \\textbf{x}_j^T\\textbf{u}_k}\\Big\\}, \\end{equation}\\] where indicates that the minimum is taken over only positive components within each choice of \\(j\\in\\mathcal{A}_k^c\\) . Note that \\(\\eqref{eq:3-25d}\\) here is the same as the formula (2.13) given in Least Angle Regression .","title":"Ex. 3.25"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-26/","text":"Ex. 3.26 Forward stepwise regression enters the variable at each step that most reduces the residual sum-of-squares. LAR adjusts variables that have the most (absolute) correlation with the current residuals. Show that these two entry criteria are not necessarily the same. [Hint: let \\(\\bx_{j.\\mathcal{A}}\\) be the \\(j\\) th variable, linearly adjusted for all the variables currently in the model. Show that the first criterion amounts to identifying the \\(j\\) for which \\(\\text{Cor}(\\bx_{j.\\mathcal{A}}, \\bb{r})\\) is largest in magnitude]. Soln. 3.26 The hint is derived in Ex. 3.9 . Therefore, the difference between forward stepwise regression and LAR becomes clearer. The former chooses and includes the variable while the latter chooses, adjusts and then includes the same variable. The paragraph below is cited from Section 2 in Least Angle Regression . ``The LARS procedure works roughly as follows. As with classic Forward Selection, we start with all coefficients equal to zero, and find the predictor most correlated with the response, say \\(x_{j1}\\) . We take the largest step possible in the direction of this predictor until some other predictor, say \\(x_{j2}\\) , has as much correlation with the current residual. At this point LARS parts company with Forward Selection. Instead of continuing along \\(x_{j1}\\) , LARS proceeds in a direction equiangular between the two predictors until a third variable \\(x_{j3}\\) earns its way into the \u201cmost correlated\u201d set. LARS then proceeds equiangularly between \\(x_{j1}\\) , \\(x_{j2}\\) and \\(x_{j3}\\) , that is, along the \u201cleast angle direction,\u201d until a fourth variable enters, and so on.\"","title":"Ex. 3.26"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-27/","text":"Ex. 3.27 Lasso and LAR : Consider the lasso problem in Lagrange multiplier form: with \\(L(\\beta)=\\frac{1}{2}\\sum_i(y_i-\\sum_j x_{ij}\\beta_j)^2\\) , we minimize \\[\\begin{equation} \\label{eq:3-27a} L(\\beta) + \\lambda\\sum_j|\\beta_j| \\end{equation}\\] for fixed \\(\\lambda>0\\) . (a) Setting \\(\\beta_j=\\beta_j^+ - \\beta_j^-\\) with \\(\\beta^+_j, \\beta^-_j\\ge 0\\) , expression \\(\\eqref{eq:3-27a}\\) becomes \\(L(\\beta) + \\lambda\\sum_j(\\beta_j^++\\beta_j^-)\\) . Show that the Lagrange dual function is \\[\\begin{equation} L(\\beta) + \\lambda \\sum_j(\\beta^+_j + \\beta^-_j)-\\sum_{j}\\lambda^+_j\\beta^+_j-\\sum_j\\lambda^-_j\\beta^-_j\\non \\end{equation}\\] and the Karush-Kuhn-Tucker optimality conditions are \\[\\begin{eqnarray} \\nabla L(\\beta)_j + \\lambda - \\lambda_j^+ &=& 0 \\non\\\\ -\\nabla L(\\beta)_j + \\lambda - \\lambda_j^- &=& 0 \\non\\\\ \\lambda^+_j\\beta^+_j &=& 0\\non\\\\ \\lambda^-_j\\beta^-_j &=& 0,\\non \\end{eqnarray}\\] along with the non-negativity constraints on the parameters and all the Lagrange multipliers. (b) Show that \\(|\\nabla L(\\beta)_j|\\le \\lambda \\ \\forall j\\) , and that the KKT conditions imply one of the following three scenarios: \\[\\begin{eqnarray} \\lambda = 0 &\\Rightarrow& \\nabla L(\\beta)_j = 0 \\ \\forall j\\non\\\\ \\beta^+_j > 0, \\lambda > 0 &\\Rightarrow& \\lambda^+_j = 0, \\nabla L(\\beta)_j = -\\lambda < 0, \\beta_j^-=0\\non\\\\ \\beta^-_j > 0, \\lambda > 0 &\\Rightarrow& \\lambda^-_j = 0, \\nabla L(\\beta)_j = \\lambda < 0, \\beta_j^+=0.\\non \\end{eqnarray}\\] Hence show that for any ``active'' predictor having \\(\\beta_j\\neq 0\\) , we must have \\(\\nabla L(\\beta)_j = -\\lambda\\) if \\(\\beta_j > 0\\) , and \\(\\nabla L(\\beta)_j = \\lambda\\) if \\(\\beta_j < 0\\) . Assuming the predictors are standardized, relate \\(\\lambda\\) to the correlation between the \\(j\\) th predictor and the current residuals. (c) Suppose that the set of active predictors is unchanged for \\(\\lambda_0\\ge\\lambda\\ge\\lambda_1\\) . Show that there is a vector \\(\\gamma_0\\) such that \\[\\begin{equation} \\hat\\beta(\\lambda) = \\hat\\beta(\\lambda_0) - (\\lambda-\\lambda_0)\\gamma_0.\\non \\end{equation}\\] Thus the lasso solution path is linear as \\(\\lambda\\) ranges from \\(\\lambda_0\\) to \\(\\lambda_1\\) (Efron et al., 2004 Least Angle Regression ; Rosset and Zhu, 2007 Piecewise linear regularized solution paths }). Soln. 3.27 (a) Now we have new variables \\(\\beta_j^+\\ge 0\\) and \\(\\beta_j^-\\ge 0\\) , so we need to introduce new variables \\(\\lambda_j^+\\) and \\(\\lambda_j^-\\) in the Lagrange function: \\[\\begin{equation} L(\\beta) + \\lambda\\sum_j(\\beta_j^+ + \\beta_j^-) -\\sum_{j}\\lambda^+_j\\beta^+_j-\\sum_j\\lambda^-_j\\beta^-_j.\\non \\end{equation}\\] Taking derivatives of above w.r.t \\(\\beta_j^+\\) and \\(\\beta_j^-\\) separately we obtain \\[\\begin{eqnarray} \\nabla L(\\beta)_j + \\lambda - \\lambda_j^+ &=& 0 \\non\\\\ -\\nabla L(\\beta)_j + \\lambda - \\lambda_j^- &=& 0. \\non \\end{eqnarray}\\] The other two conditions are the complementary slackness conditions: \\[\\begin{eqnarray} \\lambda^+_j\\beta^+_j &=& 0\\non\\\\ \\lambda^-_j\\beta^-_j &=& 0.\\non \\end{eqnarray}\\] (b) From the first two equations in (a) we obtain \\[\\begin{equation} \\lambda_j^+ + \\lambda_j^-=2\\lambda\\ge0,\\non \\end{equation}\\] and \\[\\begin{equation} \\nabla L(\\beta)_j = \\frac{1}{2}\\left(\\lambda_j^- - \\lambda_j^+\\right).\\non \\end{equation}\\] Therefore \\[\\begin{equation} |\\nabla L(\\beta)_j| = \\frac{1}{2}|\\lambda_j^- - \\lambda_j^+|\\le \\frac{1}{2}\\left(\\lambda_j^- + \\lambda_j^+\\right)=\\lambda.\\non \\end{equation}\\] So we have \\(\\lambda = 0 \\Rightarrow \\nabla L(\\beta)_j = 0\\) . If \\(\\beta_j^+ > 0\\) and \\(\\lambda > 0\\) , from complementary slackness conditions we know \\(\\lambda_j^+ = 0\\) and thus \\(\\nabla L(\\beta)_j = -\\lambda < 0\\) . Then we have \\(\\lambda_j^- = 2\\lambda > 0\\) , by complementary slackness conditions again we have \\(\\beta_j^-=0\\) . Therefore we have \\[\\begin{equation} \\beta^+_j > 0,\\ \\lambda > 0 \\Rightarrow \\lambda^+_j = 0,\\ \\nabla L(\\beta)_j = -\\lambda < 0,\\ \\beta_j^-=0.\\non \\end{equation}\\] Similar arguments yields \\[\\begin{equation} \\beta^-_j > 0, \\ \\lambda > 0 \\Rightarrow \\lambda^-_j = 0,\\ \\nabla L(\\beta)_j = \\lambda < 0,\\ \\beta_j^+=0.\\non \\end{equation}\\] It's then easy to see \\(\\nabla L(\\beta)_j = -\\lambda\\text{sign}(\\beta_j)\\) . By definition of \\(L(\\beta)\\) , we have \\[\\begin{equation} \\nabla L(\\beta)_j = x_j^T(y-X\\beta).\\non \\end{equation}\\] Under the assumption that the predictors are standardized, we see \\(|\\lambda|\\) equals the absolute value of the correlation between the \\(j\\) th predictor and the current residuals. (c) From (b) we have \\[\\begin{equation} -\\lambda\\text{sign}(\\hat\\beta_j(\\lambda)) = x_j^T(y-X\\hat\\beta_j(\\lambda)\\non \\end{equation}\\] for all \\(j\\in S\\) where \\(S\\) is the set of active variables. Note that if \\(j\\notin S\\) , we have \\(\\hat\\beta_j(\\lambda)=0\\) . Therefore we can write \\[\\begin{equation} X^T(y-X\\hat\\beta(\\lambda)) = \\theta(\\lambda),\\non \\end{equation}\\] where \\(\\theta\\) is a vector function such that \\[\\begin{equation} \\theta(\\lambda)_j = \\begin{cases} -\\lambda\\text{sign}(\\hat\\beta_j(\\lambda)) & \\text{if } j \\in S\\\\ x_j^Ty & \\text{if } j \\notin S.\\non \\end{cases} \\end{equation}\\] Therefore we solve for \\(\\hat\\beta(\\lambda)\\) as \\[\\begin{equation} \\hat\\beta(\\lambda) = (X^TX)^{-1}X^Ty - (X^TX)^{-1}\\theta(\\lambda).\\non \\end{equation}\\] Thus we see \\[\\begin{eqnarray} \\hat\\beta(\\lambda) - \\hat\\beta(\\lambda_0) &=& (X^TX)^{-1}\\left(\\theta(\\lambda_0)-\\theta(\\lambda)\\right),\\non \\end{eqnarray}\\] where \\[\\begin{equation} \\theta(\\lambda_0)-\\theta(\\lambda) = \\begin{cases} (\\lambda-\\lambda_0)\\text{sign}(\\hat\\beta_j(\\lambda_0)) & \\text{if } j \\in S\\\\ 0 & \\text{if } j \\notin S.\\non \\end{cases} \\end{equation}\\] Thus the lasso solution path is linear as \\(\\lambda\\) ranges from \\(\\lambda_0\\) to \\(\\lambda_1\\)","title":"Ex. 3.27"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-28/","text":"Ex. 3.28 Suppose for a given \\(t\\) in (3.51), the fitted lasso coefficient for variable \\(X_j\\) is \\(\\hat\\beta_j=a\\) . Suppose we augment our set of variables with an identical copy \\(X^\\ast_j=X_j\\) . Characterize the effect of this exact collinearity by describing the set of solutions for \\(\\hat\\beta_j\\) and \\(\\hat\\beta_j^\\ast\\) , using the same value of \\(t\\) . Soln. 3.28 The original lasso problem is \\[\\begin{eqnarray} \\hat\\beta^{\\text{lasso}} = &&\\underset{\\beta}{\\operatorname{argmin}}\\sum_{i=1}^N\\left(y_i-\\beta_0-\\sum_{k=1}^px_{ik}\\beta_k\\right)^2\\non\\\\ &&\\text{s.t.} \\ \\sum_{k=1}^p|\\beta_k|\\le t.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (P_0)\\non \\end{eqnarray}\\] We know that \\(\\hat\\beta^{\\text{lasso}}_j =a\\) . When an identical copy \\(X^\\ast_j=X_j\\) is included, the problem becomes \\[\\begin{eqnarray} \\hat\\beta^{\\text{new}} = &&\\underset{\\beta}{\\operatorname{argmin}}\\sum_{i=1}^N\\left(y_i-\\beta_0-\\sum_{k=1}^px_{ik}\\beta_k - x_{ij}\\beta_j^\\ast\\right)^2\\non\\\\ &&\\text{s.t.} \\ \\sum_{k=1}^p|\\beta_k| + |\\beta_j^\\ast|\\le t.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (P_1)\\non \\end{eqnarray}\\] Denote \\(\\tilde\\beta_j = \\beta_j + \\beta_j^\\ast\\) , the problem \\(P_1\\) can be rewritten as \\[\\begin{eqnarray} \\hat\\beta^{\\text{new}} = &&\\underset{\\beta}{\\operatorname{argmin}}\\sum_{i=1}^N\\left(y_i-\\beta_0-\\sum_{k\\neq j}^px_{ik}\\beta_k - x_{ij}\\tilde\\beta_j\\right)^2\\non\\\\ &&\\text{s.t.} \\ \\sum_{k\\neq j}^p|\\beta_k| + |\\tilde\\beta_j| + (|\\beta_j|+|\\beta_j^\\ast|-|\\tilde\\beta_j|)\\le t.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (P_2)\\non \\end{eqnarray}\\] Comparing \\(P_2\\) with original lasso problem \\(P_0\\) we see that the objective is the same while the constraint of \\(P_2\\) is more strict than that of \\(P_0\\) because \\(|\\beta_j|+|\\beta_j^\\ast|-|\\tilde\\beta_j|\\ge 0\\) . On the other hand, note that by symmetry, we have \\(\\hat\\beta_j = \\hat\\beta_j^\\ast\\) . Given an optimal solution \\(\\hat\\beta^{\\text{lasso}}\\) to original problem \\(P_0\\) , we can set \\(\\beta_j=\\beta_j^\\ast=\\frac{1}{2}\\hat\\beta^{\\text{lasso}}_j=\\frac{a}{2}\\) . In that case, we obtain an optimal solution to problem \\(P_2\\) because \\(|\\beta_j|+|\\beta_j^\\ast|-|\\tilde\\beta_j| = |a/2|+|a/2|-|a|=0\\) . Therefore we see that if we include an identical copy of \\(X_j^\\ast = X_j\\) , the coefficients for other variables \\(X_k\\) for \\(k\\neq j\\) remain the same while the coefficients for \\(X_j^\\ast\\) and \\(X_j\\) are the half of the original.","title":"Ex. 3.28"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-29/","text":"Ex. 3.29 Suppose we run a ridge regression with parameter \\(\\lambda\\) on a single variable \\(X\\) , and get coefficient \\(a\\) . We now include an exact copy \\(X^\\ast = X\\) , and refit our ridge regression. Show that both coefficients are identical, and derive their value. Show in general that if \\(m\\) copies of a variable \\(X_j\\) are included in a ridge regression, their coefficients are all the same. Soln. 3.29 In this exercise, \\(X\\in \\mathbb{R}^{N\\times 1}\\) . The solution for one-dimensional ridge regression is \\[\\begin{equation} \\hat\\beta = \\frac{X^Ty}{X^TX+\\lambda}.\\non \\end{equation}\\] When a new copy \\(X^\\ast=X\\) is added, we need to solve \\[\\begin{equation} \\min_{\\beta_1, \\beta_2}\\|y-X^T\\beta_1 - X^T\\beta_2\\|_2^2 + \\lambda\\|\\beta_1\\|_2^2 + \\lambda\\|\\beta_2\\|_2^2.\\non \\end{equation}\\] First by symmetry, we know \\(\\hat\\beta_1=\\hat\\beta_2\\) . Then we are essentially solving \\[\\begin{equation} \\min_{\\beta}\\|y-2X^T\\beta\\|_2^2 + 2\\lambda\\|\\beta\\|_2^2,\\non \\end{equation}\\] which yields \\[\\begin{equation} \\hat\\beta (= \\hat\\beta_1 = \\hat\\beta_2) = \\frac{X^Ty}{2X^TX+\\lambda}.\\non \\end{equation}\\] Similar arguments lead to solution for the general case \\(m\\ge 2\\) : \\[\\begin{equation} \\hat\\beta_k = \\frac{X^Ty}{mX^TX+\\lambda}, \\ k=1,...,m.\\non \\end{equation}\\]","title":"Ex. 3.29"},{"location":"ESL-Solution/3-Linear-Methods-for-Regression/ex3-30/","text":"Ex. 3.30 Consider the elastic-net optimization problem: \\[\\begin{equation} \\min_\\beta \\|\\by-\\bX\\beta\\|^2 + \\lambda\\left[\\alpha \\|\\beta\\|_2^2 + (1-\\alpha)\\|\\beta\\|_1\\right].\\non \\end{equation}\\] Show how one can turn this into a lasso problem, using an augmented version of \\(\\bX\\) and \\(\\by\\) . Soln. 3.30 Assume \\(\\by\\in\\mathbb{R}^{N\\times 1}\\) , \\(\\bX\\in\\mathbb{R}^{N\\times (p+1)}\\) and \\(\\beta\\in\\mathbb{R}^{(p+1)\\times 1}\\) . We first augment \\(\\bX\\) by \\[\\begin{equation} \\tilde\\bX = \\begin{pmatrix} \\bX\\\\ \\gamma \\bb{I}_{p+1} \\end{pmatrix}\\in \\mathbb{R}^{(N+p+1)\\times (p+1)}\\non \\end{equation}\\] for \\(\\gamma > 0\\) . Then we augment \\(\\by\\) by \\[\\begin{equation} \\tilde \\by = \\begin{pmatrix} \\by\\\\ \\bb{0}_{p+1} \\end{pmatrix}\\in\\mathbb{R}^{(N+p+1)\\times 1}.\\non \\end{equation}\\] Then we have \\[\\begin{equation} \\|\\tilde \\by - \\tilde\\bX\\beta\\|_2^2 = \\left\\|\\begin{pmatrix} \\by-\\bX\\beta\\\\ \\gamma\\beta \\end{pmatrix}\\right\\|_2^2 = \\|\\by-\\bX\\beta\\|_2^2 + \\gamma^2\\|\\beta\\|_2^2.\\non \\end{equation}\\] So consider the lasso problem for \\((\\tilde\\by, \\tilde\\bX)\\) \\[\\begin{equation} \\min_{\\beta} \\|\\tilde \\by - \\tilde\\bX\\beta\\|_2^2 + \\delta\\|\\beta\\|_1,\\non \\end{equation}\\] which is essentially \\[\\begin{equation} \\min_{\\beta}\\|\\by-\\bX\\beta\\|_2^2 + \\gamma^2\\|\\beta\\|_2^2 + \\delta\\|\\beta\\|_1.\\non \\end{equation}\\] By choosing \\(\\gamma = \\sqrt{\\lambda\\alpha}\\) and \\(\\delta = \\lambda(1-\\alpha)\\) we get the original elastic-net problem. Remark This exercise is similar to Ex. 3.12 .","title":"Ex. 3.30"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-1/","text":"Ex. 4.1 Show how to solve the generalized eigenvalue problem \\(a^T\\textbf{B}a\\) subject to \\(a^T\\textbf{W}a = 1\\) by transforming to a standard eigenvalue problem. Soln. 4.1 We are solving a constraint optimization problem \\[\\begin{eqnarray} &&\\max_{a} a^T\\textbf{B}a\\nonumber\\\\ \\text{s.t.} && a^T\\textbf{W}a = 1.\\nonumber \\end{eqnarray}\\] The Lagrangian multiplier is \\[\\begin{equation} L(a,\\lambda) = a^T\\textbf{B}a - \\lambda(a^T\\textbf{W}a-1).\\nonumber \\end{equation}\\] Taking partial derivative w.r.t \\(a\\) and setting it to be zero we get \\[\\begin{equation} \\frac{\\partial L(a, \\lambda)}{\\partial a} = 2\\textbf{B}a + \\lambda(2\\textbf{W}a)=0,\\nonumber \\end{equation}\\] which is equivalent to \\[\\begin{equation} \\label{eq:ex41eigen} \\textbf{W}^{-1}\\textbf{B}a = \\lambda a. \\end{equation}\\] Now it's easy to see that \\(\\eqref{eq:ex41eigen}\\) is a standard eigenvalue problem.","title":"Ex. 4.1"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-2/","text":"Ex. 4.2 Suppose we have features \\(x\\in \\mathbb{R}^p\\) , a two-class response, with class sizes \\(N_1\\) , \\(N_2\\) , and the target coded as \\(-N/N_1\\) , \\(N/N_2\\) . (a) Show that the LDA rule classifies to class 2 if \\[\\begin{equation} \\label{eq:ex43LDA} x^T\\hat\\Sigma^{-1}(\\hat\\mu_2 - \\hat\\mu_1) > \\frac{1}{2}(\\hat\\mu_2 + \\hat\\mu_1)^T\\hat\\Sigma^{-1}(\\hat\\mu_2 - \\hat\\mu_1) - \\log(N_2/N_1), \\end{equation}\\] and class 1 otherwise. (b) Consider minimization of the least squares criterion \\[\\begin{equation} \\sum_{i=1}^N(y_i-\\beta_0-x_i^T\\beta)^2.\\nonumber \\end{equation}\\] Show that the solution \\(\\hat\\beta\\) satisfies \\[\\begin{equation} \\left[(N-2)\\hat\\Sigma + N\\hat\\Sigma_B\\right]\\beta = N(\\hat\\mu_2-\\hat\\mu_1)\\nonumber \\end{equation}\\] (after simplification), where \\(\\hat\\Sigma_B=\\frac{N_1N_2}{N}(\\hat\\mu_2-\\hat\\mu_1)(\\hat\\mu_2-\\hat\\mu_1)^T\\) . (c) Hence show that \\(\\hat\\Sigma_B\\beta\\) is in the direction \\((\\hat\\mu_2-\\hat\\mu_1)\\) and thus \\[\\begin{equation} \\hat\\beta \\propto \\hat\\Sigma^{-1}(\\hat\\mu_2 - \\hat\\mu_1).\\nonumber \\end{equation}\\] Therefore the least-squares regression coefficient is identical to the LDA coefficient, up to a scalar mupliple. (d) Show that this result holds for any (distinct) coding of the two classes. (e) Find the solution \\(\\hat\\beta_0\\) (up to the same scalar multiple as in (c)), and hence the predicted value \\(\\hat f(x) = \\hat\\beta_0 + x^T\\hat\\beta\\) . Consider the following rule: classify to class 2 if \\(\\hat f(x) > 0\\) and class 1 otherwise. Show this is not the same as the LDA rule unless the classes have equal numbers of observations. ( The use of multiple measurements in taxonomic problems , Pattern Recognition and Neural Networks ) Soln. 4.2 (a) We have \\(\\pi_1=N_1/N\\) and \\(\\pi_2 = N_2/N\\) . The conclusion follows directly by (4.9) in the textbook. (b) We start by introducing notations used in Chapter 3. Let \\(x_i^T = (x_{i1}, ..., x_{ip})\\in \\mathbb{R}^{1\\times p}\\) , \\(\\textbf{1}^T = (1,...,1)\\in \\mathbb{R}^{1\\times p}\\) , \\(Y^T = (y_1, ..., y_N)\\in \\mathbb{R}^{1\\times N}\\) , \\(\\beta^T = (\\beta_{1}, ..., \\beta_{p})\\in \\mathbb{R}^{1\\times p}\\) . \\[\\begin{equation} \\textbf{X} = \\begin{pmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\ 1 & x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{pmatrix} = \\begin{pmatrix} 1 & x_1^T\\\\ 1 & x_2^T\\\\ \\vdots & \\vdots\\\\ 1 & x_N^T \\end{pmatrix}\\in \\mathbb{R}^{N\\times (p+1)}\\nonumber \\end{equation}\\] and \\[\\begin{equation} \\textbf{X}^T = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\\\ x_{11} & x_{21} & \\cdots & x_{N1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{1p} & x_{2p} & \\cdots & x_{Np} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ x_1 & x_2 & \\cdots & x_N \\end{pmatrix}\\in \\mathbb{R}^{(p+1)\\times N}.\\nonumber \\end{equation}\\] So that we have \\[\\begin{equation} \\textbf{X}^T\\textbf{X} = \\begin{pmatrix} N & \\sum_{i=1}^Nx_i^T\\\\ \\sum_{i=1}^Nx_i & \\sum_{i=1}^Nx_ix_i^T \\end{pmatrix} \\in \\mathbb{R}^{(p+1)\\times (p+1)}.\\nonumber \\end{equation}\\] From knowledge in linear regression, e.g., (3.6) in the textbook, we have \\[\\begin{equation} \\label{eq:ex42normal} \\textbf{X}^T\\textbf{X}\\begin{pmatrix} \\beta_0\\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} N & \\sum_{i=1}^Nx_i^T\\\\ \\sum_{i=1}^Nx_i & \\sum_{i=1}^Nx_ix_i^T \\end{pmatrix} \\begin{pmatrix} \\beta_0\\\\ \\beta \\end{pmatrix}= \\textbf{X}^TY= \\begin{pmatrix} \\sum_{i=1}^Ny_i\\\\ \\sum_{i=1}^Ny_ix_i \\end{pmatrix}\\in \\mathbb{R}^{(p+1)\\times 1}.\\nonumber \\end{equation}\\] Therefore we have \\[\\begin{eqnarray} N\\beta_0 + \\left(\\sum_{i=1}^Nx_i^T\\right)\\beta &=& \\sum_{i=1}^Ny_i\\nonumber\\\\ \\beta_0\\sum_{i=1}^Nx_i + \\left(\\sum_{i=1}^Nx_ix_i^T\\right)\\beta&=&\\sum_{i=1}^Ny_ix_i.\\label{eq:ex43beta0} \\end{eqnarray}\\] From the first equation above, we get \\[\\begin{equation} \\label{eq:ex43beta0solution} \\beta_0 = \\frac{1}{N}\\left(\\sum_{i=1}^Ny_i-\\left(\\sum_{i=1}^Nx_i^T\\right)\\beta\\right), \\end{equation}\\] plug above into \\(\\eqref{eq:ex43beta0}\\) and solve for \\(\\beta\\) we obtain \\[\\begin{equation} \\left[\\sum_{i=1}^Nx_ix_i^T - \\frac{1}{N}\\left(\\sum_{i=1}^Nx_i\\right)\\left(\\sum_{i=1}^Nx_i^T\\right)\\right]\\beta = \\sum_{i=1}^Ny_ix_i-\\frac{1}{N}\\left(\\sum_{i=1}^Ny_i\\right)\\left(\\sum_{i=1}^Nx_i\\right).\\label{eq:ex43beta} \\end{equation}\\] We pause here and turn to \\(\\hat\\mu_1, \\hat\\mu_2\\) and \\(\\Sigma\\) . Let's we encode \\(y_i\\) for class 1 as \\(t_1\\) and \\(y_i\\) for class 2 as \\(t_2\\) . Note that for this particular problem (b), \\(t_1=-N/N_1\\) and \\(t_2=N/N_2\\) . By definition, we have \\[\\begin{eqnarray} \\hat\\mu_1 = \\frac{1}{N_1}\\sum_{i:g_i=1}x_i, && \\hat\\mu_2 = \\frac{1}{N_2}\\sum_{i:g_i=2}x_i\\nonumber\\\\ \\sum_{i=1}^Nx_i = N_1\\hat\\mu_1 + N_2\\hat\\mu_2, && \\sum_{i=1}^Nx_i^T = N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T\\nonumber\\\\ \\sum_{i=1}^Ny_i = N_1t_1 + N_2t_2, && \\sum_{i=1}^Ny_ix_i = t_1N_1\\hat\\mu_1 + t_2N_2\\hat\\mu_2 \\nonumber \\end{eqnarray}\\] and \\[\\begin{equation} \\hat\\Sigma = \\frac{1}{N-2}\\left(\\sum_{i:g_i=1}(x_i-\\hat\\mu_1)(x_i-\\hat\\mu_1)^T + \\sum_{i:g_i=2}(x_i-\\hat\\mu_2)(x_i-\\hat\\mu_2)^T\\right).\\nonumber \\end{equation}\\] From equations above we can rewrite \\[\\begin{equation} \\sum_{i=1}^Nx_ix_i^T = (N-2)\\hat\\Sigma + N_1\\hat\\mu_1\\hat\\mu_1^T + N_2\\hat\\mu_2\\hat\\mu_2^T.\\nonumber \\end{equation}\\] Now we turn back to \\(\\eqref{eq:ex43beta}\\) . For the LHS, \\[\\begin{eqnarray} &&\\sum_{i=1}^Nx_ix_i^T - \\frac{1}{N}\\left(\\sum_{i=1}^Nx_i\\right)\\left(\\sum_{i=1}^Nx_i^T\\right)\\nonumber\\\\ &=&\\sum_{i=1}^Nx_ix_i^T - \\frac{1}{N}\\Big(N_1\\hat\\mu_1 + N_2\\hat\\mu_2\\Big)\\Big(N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T\\Big)\\nonumber\\\\ &=&(N-2)\\hat\\Sigma + N_1\\hat\\mu_1\\hat\\mu_1^T + N_2\\hat\\mu_2\\hat\\mu_2^T - \\frac{1}{N}\\Big(N_1\\hat\\mu_1 + N_2\\hat\\mu_2\\Big)\\Big(N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T\\Big)\\nonumber\\\\ &=&(N-2)\\hat\\Sigma + \\frac{N_1N_2}{N}(\\hat\\mu_2-\\hat\\mu_1)(\\hat\\mu_2-\\hat\\mu_1)^T\\nonumber\\\\ &=&(N-2)\\hat\\Sigma + N\\hat\\Sigma_B, \\label{eq:ex43LHS} \\end{eqnarray}\\] where \\(\\hat\\Sigma_B = \\frac{N_1N_2}{N^2}(\\hat\\mu_2-\\hat\\mu_1)(\\hat\\mu_2-\\hat\\mu_1)^T\\) . For the RHS of \\(\\eqref{eq:ex43beta}\\) , \\[\\begin{eqnarray} &&\\sum_{i=1}^Ny_ix_i-\\frac{1}{N}\\left(\\sum_{i=1}^Ny_i\\right)\\left(\\sum_{i=1}^Nx_i\\right)\\nonumber\\\\ &=&t_1N_1\\hat\\mu_1 + t_2N_2\\hat\\mu_2-\\frac{1}{N}\\Big(N_1t_1 + N_2t_2\\Big)\\Big(N_1\\hat\\mu_1 + N_2\\hat\\mu_2\\Big)\\nonumber\\\\ &=&\\frac{N_1N_2}{N}(t_2-t_1)(\\hat\\mu_2-\\hat\\mu_1).\\label{eq:ex43rhs} \\end{eqnarray}\\] Combining \\(\\eqref{eq:ex43beta}\\) , \\(\\eqref{eq:ex43LHS}\\) and \\(\\eqref{eq:ex43rhs}\\) we get \\[\\begin{equation} \\label{eq:ex43b} \\left[(N-2)\\hat\\Sigma + N\\hat\\Sigma_B\\right]\\beta = \\frac{N_1N_2}{N}(t_2-t_1)(\\hat\\mu_2-\\hat\\mu_1). \\end{equation}\\] Note that \\(t_1=-N/N_1\\) , \\(t_2 = N/N_2\\) and \\(N=N_1+N_2\\) , so that \\(t_2-t_1=\\frac{N^2}{N_1N_2}\\) . Thus, \\(\\eqref{eq:ex43rhs}\\) reduces to \\(N(\\hat\\mu_2-\\hat\\mu_1)\\) and we finish the proof. (c) We have \\[\\begin{equation} \\hat\\Sigma_B\\beta = \\frac{N_1N_2}{N^2}(\\hat\\mu_2-\\hat\\mu_1)(\\hat\\mu_2-\\hat\\mu_1)^T\\beta\\non \\end{equation}\\] where \\((\\hat\\mu_2-\\hat\\mu_1)^T\\beta\\in \\mathbb{R}\\) is a scalar, thus \\(\\hat\\Sigma_B\\beta\\) is in the direction \\((\\hat\\mu_2-\\hat\\mu_1)\\) . By part (b) we have \\[\\begin{equation} \\hat\\beta\\propto \\hat\\Sigma^{-1}(\\hat\\mu_2 - \\hat\\mu_1).\\non \\end{equation}\\] (d) This follows directly from \\(\\eqref{eq:ex43b}\\) for \\(t_1\\neq t_2\\) . (e) Assuming the encoding of \\(-N/N_1\\) and \\(N/N_2\\) , by \\(\\eqref{eq:ex43beta0solution}\\) we have \\[\\begin{eqnarray} \\hat\\beta_0 &=& \\frac{1}{N}\\left(\\sum_{i=1}^Ny_i-\\left(\\sum_{i=1}^Nx_i^T\\right)\\beta\\right)\\non\\\\ &=&-\\frac{1}{N}\\left(\\sum_{i=1}^Nx_i^T\\right)\\hat\\beta\\non\\\\ &=&-\\frac{1}{N}(N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T)\\hat\\beta \\end{eqnarray}\\] so that \\[\\begin{equation} \\hat f(x) = \\hat\\beta_0 + x^T\\hat\\beta = \\left[x^T-\\frac{1}{N}(N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T)\\right]\\hat\\beta.\\non \\end{equation}\\] Since \\(\\hat\\beta\\propto \\hat\\Sigma^{-1}(\\hat\\mu_2-\\hat\\mu_1)\\) , there exists \\(\\lambda > 0\\) (up to a scalar constant, i.e., we can flip the classification sign if \\(\\lambda <0\\) ) such that \\(\\hat\\beta = \\lambda \\hat\\Sigma^{-1}(\\hat\\mu_2-\\hat\\mu_1)\\) . Therefore, \\(\\hat f(x) > 0\\) is equivalent to \\[\\begin{equation} \\left[x^T-\\frac{1}{N}(N_1\\hat\\mu_1^T + N_2\\hat\\mu_2^T)\\right]\\hat\\Sigma^{-1}(\\hat\\mu_2-\\hat\\mu_1) > 0,\\non \\end{equation}\\] which is equivalent to LDA rule \\(\\eqref{eq:ex43LDA}\\) when \\(N_1 = N_2\\) . When \\(N_1\\neq N_2\\) , \\(\\log (N_2/N_1) \\neq 0\\) in \\(\\eqref{eq:ex43LDA}\\) so they are not equivalent.","title":"Ex. 4.2"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-3/","text":"Ex. 4.3 Suppose we transform the original predictors \\(\\bb{X}\\) to \\(\\boldsymbol{\\hat Y}\\) via linear regression. In detail, let \\(\\boldsymbol{\\hat Y} = \\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\bb{Y} = \\bb{X}\\hat B\\) where \\(\\bb{Y}\\) is the indicator response matrix. Similarly for any input \\(x\\in\\mathbb{R}^p\\) , we get a transformed vector \\(\\hat y = \\hat B^Tx\\in \\mathbb{R}^K\\) . Show that LDA using \\(\\boldsymbol{\\hat Y}\\) is identical to LDA in the original space. Soln. 4.3 We start by introducing notations used in Chapter 3. Let \\(x_i^T = (x_{i1}, ..., x_{ip})\\in \\mathbb{R}^{1\\times p}\\) , \\(\\bb{1}^T = (1,...,1)\\in \\mathbb{R}^{1\\times p}\\) , \\(Y^T = (y_1, ..., y_N)\\in \\mathbb{R}^{1\\times N}\\) , \\(\\beta^T = (\\beta_{1}, ..., \\beta_{p})\\in \\mathbb{R}^{1\\times p}\\) . Let \\[\\begin{equation} \\bb{X} = \\begin{pmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\ 1 & x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{pmatrix} = \\begin{pmatrix} 1 & x_1^T\\\\ 1 & x_2^T\\\\ \\vdots & \\vdots\\\\ 1 & x_N^T \\end{pmatrix}\\in \\mathbb{R}^{N\\times (p+1)}\\non \\end{equation}\\] and \\[\\begin{equation} \\bb{X}^T = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\\\ x_{11} & x_{21} & \\cdots & x_{N1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{1p} & x_{2p} & \\cdots & x_{Np} \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ x_1 & x_2 & \\cdots & x_N \\end{pmatrix}\\in \\mathbb{R}^{(p+1)\\times N}.\\non \\end{equation}\\] We have \\[\\begin{equation} \\boldsymbol{\\hat Y} = \\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\bb{Y} = \\bb{X}\\hat B\\in \\mathbb{R}^{N\\times k},\\non \\end{equation}\\] and \\(\\hat y = \\hat B^Tx\\) for a single training sample \\(x\\) . We estimate the new parameters of the Gaussian distributions from transformed data, denoted by \\(\\pi_k^{\\text{new}}\\) , \\(\\hat\\mu_k^{\\text{new}}\\) and \\(\\hat\\Sigma^{\\text{new}}\\) , and link them back with \\(\\pi_k\\) , \\(\\hat\\mu_k\\) and \\(\\hat\\Sigma\\) estimated from original training data. First, \\(\\pi_k^{\\text{new}} = \\pi_k\\) for \\(k=1,...,K\\) since the training sample classification does not change. Second, by definition of \\(\\hat\\mu_k^{\\text{new}}\\) , note again the training sample classification does not change, we have \\[\\begin{eqnarray} \\hat\\mu_k^{\\text{new}} &=& \\sum_{g_i=k}\\frac{\\hat B^Tx_i}{N_k}\\non\\\\ &=&\\hat B^T\\sum_{g_i=k}\\frac{x_i}{N_k}\\non\\\\ &=&\\hat B^T \\hat\\mu_k.\\non \\end{eqnarray}\\] Third, by definition of \\(\\hat\\Sigma^{\\text{new}}\\) and result above, we have \\[\\begin{eqnarray} \\hat\\Sigma^{\\text{new}} &=& \\sum_{k=1}^K\\sum_{g_i=k}(\\hat B^Tx_i-\\hat\\mu_k^{\\text{new}})(\\hat B^Tx_i-\\hat\\mu_k^{\\text{new}})^T/(N-K)\\non\\\\ &=&\\frac{1}{N-K}\\sum_{k=1}^K\\sum_{g_i=k}\\hat B^T(x_i-\\hat\\mu_k)(x_i - \\hat \\mu_k)^T\\hat B\\non\\\\ &=&\\hat B^T\\left[\\frac{1}{N-K}\\sum_{k=1}^K\\sum_{g_i=k}(x_i-\\hat\\mu_k)(x_i - \\hat \\mu_k)^T\\right]\\hat B\\non\\\\ &=&\\hat B^T\\hat\\Sigma\\hat B.\\non \\end{eqnarray}\\] Therefore, the new linear discriminant function is \\[\\begin{eqnarray} \\delta_k^{\\text{new}}(x) &=& (\\hat B^Tx)^T(\\hat\\Sigma^{\\text{new}})^{-1}\\hat\\mu_k^{\\text{new}} - \\frac{1}{2}(\\hat\\mu_k^{\\text{new}})^T(\\hat\\Sigma^{\\text{new}})^{-1}\\hat\\mu_k^{\\text{new}} + \\log \\pi_k^{\\text{new}}\\non\\\\ &=&x^T\\hat B (\\hat B)^{-1}(\\hat\\Sigma)^{-1}(\\hat B^T)^{-1}\\hat B^T \\hat\\mu_k -\\frac{1}{2}\\hat\\mu_k^T\\hat B (\\hat B)^{-1}(\\hat\\Sigma)^{-1}(\\hat B^T)^{-1}\\hat B^T\\hat\\mu_k + \\log \\pi_k\\non\\\\ &=&x^T\\hat\\Sigma^{-1}\\hat\\mu_k-\\frac{1}{2}\\hat\\mu_k^T(\\hat\\Sigma)^{-1}\\hat\\mu_k + \\log \\pi_k, \\non \\end{eqnarray}\\] which is identical to the discriminant function used in the original space.","title":"Ex. 4.3"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-4/","text":"Ex. 4.4 Consider the multilogic model with \\(K\\) classes (4.17) in the textbook. Let \\(\\beta\\) be the \\((p+1)(K-1)-\\) vector consisting of all the coefficients. Define a suitably enlarged version of the input vector \\(x\\) to accommodate this vectorized coefficient matrix. Derive the Newton-Raphson algorithm for maximizing the multinomial. Soln. 4.4 We follow the development in Section 4.4.1 in the textbook. As usual, we enlarge each observation \\(x\\in \\mathbb{R}^p\\) by inserting a constant \\(1\\) at the head position so that \\(x^T \\leftarrow (1, x^T)\\) . Thus we have \\[\\begin{equation} P(G=k|X=x) = \\frac{\\exp(\\beta_k^Tx)}{1 + \\sum_{l=1}^{K-1}\\exp(\\beta_l^Tx)},\\ \\ k =1,...,K-1\\non \\end{equation}\\] and \\[\\begin{equation} P(G=K|X=x) = \\frac{1}{1 + \\sum_{l=1}^{K-1}\\exp(\\beta_l^Tx)}.\\non \\end{equation}\\] Each \\(\\beta_k\\in \\mathbb{R}^{p+1}\\) for \\(k=1,...,K-1\\) . Denote \\(\\beta = \\{\\beta_1^T, ..., \\beta_{K-1}^T\\}\\) so that \\(\\beta\\) is the \\((p+1)(K-1)-\\) vector consisting of all the coefficients. Let \\[\\begin{equation} \\label{eq:4.4pkxi} p_k(x;\\beta) = P(G=k|X=x).\\non \\end{equation}\\] The log-likelihood for \\(N\\) observations is \\[\\begin{equation} l(\\beta) = \\sum_{i=1}^N\\log p_{g_i}(x_i;\\beta).\\non \\end{equation}\\] We code the class from \\(1\\) to \\(K\\) so that \\(l(\\beta)\\) can be rewritten as \\[\\begin{eqnarray} l(\\beta) &=& \\sum_{i=1}^N\\left(\\sum_{k=1}^{K-1}\\bb{1}(y_i=k)\\log p_k(x_i;\\beta) - \\bb{1}(y_i=K)\\log \\left(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}\\right)\\right)\\non\\\\ &=&\\sum_{i=1}^N\\left(\\sum_{k=1}^{K-1}\\bb{1}(y_i=k)\\beta_k^Tx_i-\\log\\big(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}\\big)\\right)\\label{eq:multiclasslrlog} \\end{eqnarray}\\] To maximize the log-likelihood, we set its derivatives to zero. These score equations, for \\(k=1,...,K-1\\) , are \\[\\begin{eqnarray} \\frac{\\partial l(\\beta)}{\\partial \\beta_k} &=& \\sum_{i=1}^N \\left(\\bb{1}(y_i=k)x_i - \\frac{x_ie^{\\beta_k^Tx_i}}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\right)\\non\\\\ &=&\\sum_{i=1}^N x_i\\left(\\bb{1}(y_i=k) - \\frac{e^{\\beta_k^Tx_i}}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\right)\\in \\mathbb{R}^{p+1}\\label{eq:multiclasslr1st} \\end{eqnarray}\\] We write \\[\\begin{equation} \\label{eq:multiclasslr1stmat} \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\begin{pmatrix} \\frac{\\partial l(\\beta)}{\\partial \\beta_1}\\\\ \\vdots\\\\ \\frac{\\partial l(\\beta)}{\\partial \\beta_{K-1}} \\end{pmatrix}\\in \\mathbb{R}^{(p+1)(K-1)}. \\end{equation}\\] We next look for the second order derivatives of \\(\\beta\\) . By \\(\\eqref{eq:multiclasslr1st}\\) , for \\(k\\neq j \\in \\{1,...,K-1\\}\\) , we get \\[\\begin{eqnarray} \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k\\partial \\beta_j^T} &=& \\sum_{i=1}^Nx_i \\left(-\\frac{e^{\\beta_j^Tx_i}x_i^Te^{\\beta_k^Tx_i}}{(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})^2}\\right)\\non\\\\ &=& - \\sum_{i=1}^Nx_ix_i^T\\frac{e^{\\beta_k^Tx_i}}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\cdot \\frac{e^{\\beta_j^Tx_i}}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\non\\\\ &=& - \\sum_{i=1}^Nx_ix_i^Tp_k(x_i;\\beta)p_j(x_i;\\beta).\\label{eq:multiclasslr2nd} \\end{eqnarray}\\] For \\(k\\in \\{1,..., K-1\\}\\) , we have \\[\\begin{eqnarray} \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k\\partial \\beta_k^T} &=&\\sum_{i=1}^Nx_i \\left(-\\frac{e^{\\beta_k^T}x_i^T}{(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})^2}\\right)\\non\\\\ &=&-\\sum_{i=1}^Nx_ix_i^T\\frac{e^{\\beta_k^T}}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\cdot\\frac{1}{1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i}}\\non\\\\ &=&-\\sum_{i=1}^Nx_ix_i^Tp_k(x_i;\\beta)p^c_k(x_i;\\beta),\\label{eq:multiclasslr2nd2} \\end{eqnarray}\\] where \\(p^c_k(x_i;\\beta) = 1 - p_k(x_i;\\beta)\\) . By \\(\\eqref{eq:multiclasslr2nd}\\) and \\(\\eqref{eq:multiclasslr2nd2}\\) , we write \\[\\begin{equation} \\frac{\\partial^2l(\\beta)}{\\partial\\beta\\partial\\beta^T} =\\begin{pmatrix} \\frac{\\partial^2l(\\beta)}{\\partial\\beta_1\\partial\\beta_1^T} & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_1\\partial\\beta_2^T} & \\cdots & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_1\\partial\\beta_{K-1}^T}\\non\\\\ \\frac{\\partial^2l(\\beta)}{\\partial\\beta_2\\partial\\beta_2^T} & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_2\\partial\\beta_2^T} & \\cdots & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_2\\partial\\beta_{K-1}^T}\\non\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2l(\\beta)}{\\partial\\beta_{K-1}\\partial\\beta_1^T} & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_{K-1}\\partial\\beta_2^T} & \\cdots & \\frac{\\partial^2l(\\beta)}{\\partial\\beta_{K-1}\\partial\\beta_{K-1}^T} \\end{pmatrix}\\in \\mathbb{R}^{(K-1)(p+1)\\times (K-1)(p+1)} \\end{equation}\\] Starting with \\(\\beta^{\\text{old}}\\) , a single Newton update is \\[\\begin{equation} \\beta^{\\text{new}} = \\beta^{\\text{old}} - \\left(\\frac{\\partial^2l(\\beta)}{\\partial\\beta\\partial\\beta^T}\\right)^{-1}\\frac{\\partial l(\\beta)}{\\partial \\beta}\\non \\end{equation}\\] where the derivative are evaluated at \\(\\beta^{\\text{old}}\\) . Next we write score and Hessian in matrix notation. Let's define, for \\(k=1,...,K-1\\) , \\[\\begin{equation} \\bb{y}_k = \\begin{pmatrix} \\bb{1}(y_1=k)\\\\ \\bb{1}(y_2=k)\\\\ \\vdots\\\\ \\bb{1}(y_N=k) \\end{pmatrix}, \\ \\ \\bb{X} = \\begin{pmatrix} x_1^T\\\\ x_2^T\\\\ \\vdots\\\\ x_{N}^T \\end{pmatrix}, \\ \\ \\bb{p}_k= \\begin{pmatrix} p_k(x_1;\\beta)\\\\ p_k(x_2;\\beta)\\\\ \\vdots\\\\ p_k(x_N;\\beta) \\end{pmatrix} \\end{equation}\\] Then \\(\\eqref{eq:multiclasslr1st}\\) is written in matrix notation as \\[\\begin{equation} \\frac{\\partial l(\\beta)}{\\partial \\beta_k} = \\bb{X}^T(\\bb{y}_k - \\bb{p}_k).\\non \\end{equation}\\] Further define stacked vectors \\[\\begin{equation} \\bb{y} = \\begin{pmatrix} \\bb{y}_1\\\\ \\bb{y}_2\\\\ \\vdots\\\\ \\bb{y}_{K-1} \\end{pmatrix}\\ \\ \\text{and} \\ \\ \\bb{p} = \\begin{pmatrix} \\bb{p}_1\\\\ \\bb{p}_2\\\\ \\vdots\\\\ \\bb{p}_{K-1} \\end{pmatrix}.\\non \\end{equation}\\] We are able to rewrite \\(\\eqref{eq:multiclasslr1st}\\) as \\[\\begin{equation} \\label{eq:4.4mat1st} \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\begin{pmatrix} \\bb{X}^T & 0 & \\cdots & 0\\\\ 0 & \\bb{X}^T & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0 & \\cdots & \\bb{X}^T \\end{pmatrix} \\begin{pmatrix} \\bb{y}_1 - \\bb{p}_1\\\\ \\bb{y}_2 - \\bb{p}_2\\\\ \\vdots\\\\ \\bb{y}_{K-1} - \\bb{p}_{K-1} \\end{pmatrix} = \\boldsymbol{\\hat X} (\\bb{y} - \\bb{p}) \\end{equation}\\] where \\(\\boldsymbol{\\hat X}\\) is clearly the matrix above with \\(\\bb{X}^T\\) on the diagonal positions. For \\(k=1,...,K-1\\) , let \\[\\begin{equation} \\bb{P}_k = \\begin{pmatrix} p_k(x_1;\\beta)p_k^c(x_1;\\beta) & 0 & \\cdots & 0\\\\ 0 & p_k(x_2;\\beta)p_k^c(x_2\\beta) & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0 & \\cdots & p_k(x_N; \\beta)p_k^c(x_N;\\beta) \\end{pmatrix}.\\non \\end{equation}\\] By \\(\\eqref{eq:multiclasslr2nd2}\\) , we have for \\(k=1,...,K-1\\) that \\[\\begin{equation} \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k\\partial\\beta^T_k} = - \\bb{X}^T\\bb{P}_k\\bb{X}.\\non \\end{equation}\\] For \\(k=1,...,K-1\\) , let \\[\\begin{equation} \\bb{R}_k = \\begin{pmatrix} p_k(x_1;\\beta) & 0 & \\cdots & 0\\\\ 0 & p_k(x_2;\\beta) & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0 & \\cdots & p_k(x_N; \\beta) \\end{pmatrix}.\\non \\end{equation}\\] By \\(\\eqref{eq:multiclasslr2nd}\\) , we have for \\(k\\neq j\\in \\{1,...,K-1\\}\\) that \\[\\begin{equation} \\frac{\\partial^2 l(\\beta)}{\\partial \\beta_k\\partial\\beta^T_j} = - \\bb{X}^T\\bb{R}_k\\bb{R}_j\\bb{X}.\\non \\end{equation}\\] Therefore we have \\[\\begin{equation} \\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} = -\\begin{pmatrix} \\bb{X}^T\\bb{P}_1\\bb{X}& \\bb{X}^T\\bb{R}_1\\bb{R}_2\\bb{X}&\\cdots&\\bb{X}^T\\bb{R}_{1}\\bb{R}_{K-1}\\bb{X}\\\\ \\bb{X}^T\\bb{R}_2\\bb{R}_2\\bb{X}& \\bb{X}^T\\bb{P}_2\\bb{X}&\\cdots&\\bb{X}^T\\bb{R}_{2}\\bb{R}_{K-1}\\bb{X}\\\\ \\vdots & \\vdots &\\ddots &\\vdots\\\\ \\bb{X}^T\\bb{R}_{K-1}\\bb{R}_1\\bb{X}& \\bb{X}^T\\bb{R}_{K-1}\\bb{R}_2\\bb{X}&\\cdots&\\bb{X}^T\\bb{P}_{K-1}\\bb{X} \\end{pmatrix}.\\non \\end{equation}\\] Let \\[\\begin{equation} \\bb{W} = \\begin{pmatrix} \\bb{P}_1& \\bb{R}_1\\bb{R}_2&\\cdots&\\bb{R}_{1}\\bb{R}_{K-1}\\\\ \\bb{R}_2\\bb{R}_2& \\bb{P}_2&\\cdots&\\bb{R}_{2}\\bb{R}_{K-1}\\\\ \\vdots & \\vdots &\\ddots &\\vdots\\\\ \\bb{R}_{K-1}\\bb{R}_1& \\bb{R}_{K-1}\\bb{R}_2&\\cdots&\\bb{P}_{K-1} \\end{pmatrix}.\\non \\end{equation}\\] Recall \\(\\boldsymbol{\\hat X}\\) defined in \\(\\eqref{eq:4.4mat1st}\\) , we rewrite the Hessian equation above as \\[\\begin{equation} \\label{eq:ex4.4hessianmatrix} \\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} = - \\boldsymbol{\\hat X}^T\\bb{W}\\boldsymbol{\\hat X}.\\non \\end{equation}\\] The Newton step is thus \\[\\begin{eqnarray} \\beta^{\\text{new}} &=& \\beta^{\\text{old}} + (\\boldsymbol{\\hat X}^T\\bb{W}\\boldsymbol{\\hat X})^{-1}\\bb{X}^T(\\bb{y}-\\bb{p})\\non\\\\ &=&(\\boldsymbol{\\hat X}^T\\bb{W}\\boldsymbol{\\hat X})^{-1}\\boldsymbol{\\hat X}^T\\bb{W}(\\boldsymbol{\\hat X}\\beta^{\\text{old}} + \\bb{W}^{-1}(\\bb{y}-\\bb{p}))\\non\\\\ &=&(\\boldsymbol{\\hat X}^T\\bb{W}\\boldsymbol{\\hat X})^{-1}\\boldsymbol{\\hat X}^T\\bb{W}\\bb{z}.\\non \\end{eqnarray}\\] In the second and third line we have re-expressed the Newton step as a weighted least squares step, with the response \\[\\begin{equation} \\bb{z} = \\boldsymbol{\\hat X}\\beta^{\\text{old}} + \\bb{W}^{-1}(\\bb{y}-\\bb{p}).\\non \\end{equation}\\]","title":"Ex. 4.4"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-5/","text":"Ex. 4.5 Consider a two-class logistic regression problem with \\(x\\in\\mathbb{R}\\) . Characterize the maximum-likelihood estimates of the slope and intercept parameter if the sample \\(x_i\\) for two classes are separated by a point \\(x_0\\in\\mathbb{R}\\) . Generalize this result to (a) \\(x\\in \\mathbb{R}^p\\) (see Figure 4.16 in the textbook), and (b) more than two classes. Soln. 4.5 Naturally we label \\(y_i=1\\) for those \\(x_i > x_0\\) and \\(y_i=0\\) for those \\(x_i < x_0\\) . The log-likelihood for \\(N\\) observations is \\[\\begin{eqnarray} l(\\beta) &=& \\sum_{i=1}^N\\big[y_i\\beta^Tx_i -\\log(1+e^{\\beta^Tx_i})\\big]\\non\\\\ &=&\\sum_{i=1}^N\\big[y_i(\\beta_0 + \\beta_1x_i) - \\log(1+e^{\\beta_0 + \\beta_1x_i})\\big]\\non\\\\ &=&\\sum_{i=1}^N\\big[y_i(\\beta_0 + \\beta_1x_0 + \\beta_1(x_i-x_0) ) - \\log(1+e^{\\beta_0 + \\beta_1x_0 + \\beta_1(x_i-x_0)})\\big].\\non \\end{eqnarray}\\] By choosing \\(\\beta_0 = -\\beta_1 x_0\\) , the equation above is simplified as \\[\\begin{equation} l(\\beta) = \\sum_{i:x_i < x_0}[- \\log(1+e^{\\beta_1(x_i-x_0)})] + \\sum_{i:x_i > x_0}[\\beta_1(x_i-x_0)- \\log(1+e^{\\beta_1(x_i-x_0)})].\\non \\end{equation}\\] It's easy to verify that the first term above vanishes while the second term above goes to infinity when \\(\\beta_1\\ra\\infty\\) , thus the maximum of the log-likelihood will never be achieved. (a) When \\(x\\in\\mathbb{R}^p\\) with \\(p > 1\\) , there exists two subsets of \\(\\{x_1,...,x_N\\}\\) , \\(S_1\\) and \\(S_2\\) , such that \\(S_1\\cup S_2=\\{x_1,...,x_N\\}\\) , \\(S_1\\cap S_2 = \\emptyset\\) . Further more, there exists a hyperplane \\(\\hat\\beta^T x = 0\\) in \\(\\mathbb{R}^{p+1}\\) and \\[\\begin{equation} \\beta^Tx > 0 \\text{ for } x\\in S_1 \\text{ and } \\beta^Tx < 0 \\text{ for } x\\in S_2.\\non \\end{equation}\\] We label \\(y_i=1\\) for \\(x_i\\in S_1\\) and \\(y_i=0\\) for \\(x_i\\in S_2\\) . The log-likelihood becomes \\[\\begin{eqnarray} l(\\beta) &=& \\sum_{i=1}^N[y_i\\beta^Tx_i - \\log(1+e^{\\beta^Tx_i})]\\non\\\\ &=&\\sum_{i\\in S_1}\\beta^Tx_i-\\log(1+e^{\\beta^Tx_i}) + \\sum_{i\\in S_2}\\left[-\\log(1+e^{\\beta^Tx_i})\\right].\\non \\end{eqnarray}\\] Like the case when \\(p=1\\) , if we keep updating \\(\\beta\\) by \\(\\beta^{\\text{new}}\\leftarrow \\alpha\\beta^{\\text{old}}\\) with \\(\\alpha\\ra+\\infty\\) , the log-likelihood \\(l(\\beta)\\ra+\\infty\\) . (b) For the case where there are \\(K>2\\) classes in \\(\\mathbb{R}^p\\) with \\(p>1\\) , the same arguments follow. In particular, the log-likelihood becomes \\[\\begin{eqnarray} l(\\beta) &=& \\sum_{i=1}^N \\left[\\sum_{k=1}^{K-1}\\bb{1}(y_i=k)\\beta_k^Tx_i-\\log(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})\\right]\\non\\\\ &=&\\sum_{k=1}^{K-1}\\sum_{i\\in S_k} \\left[\\beta_k^Tx_i - \\log(1 + \\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})\\right]\\non\\\\ && + \\sum_{i\\in S_K}\\left[-\\log(1+\\sum_{l=1}^{K-1}e^{\\beta_l^Tx_i})\\right]\\non \\end{eqnarray}\\] There exists \\(\\beta_k\\) for \\(k=1,...,K-1\\) such that \\(\\beta^T_kx > 0\\) for \\(x\\in S_k\\) , therefore, by similar arguments of increasing \\(\\beta_k\\) in (a), the log-likelihood \\(l(\\beta)\\ra+\\infty\\) .","title":"Ex. 4.5"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-6/","text":"Ex. 4.6 Suppose we have \\(N\\) points \\(x_i\\) in \\(\\mathbb{R}^p\\) in general position, with class labels \\(g_i\\in\\{-1,1\\}\\) . Prove that the perceptron learning algorithm converges to a separating hyperplane in a finite number of steps: (a) Denote a hyperplane by \\(f(x) = \\beta_1^Tx + \\beta_0 = 0\\) or in a more compact notation \\(\\beta^Tx^\\ast=0\\) , where \\(x^\\ast = (x,1)\\) and \\(\\beta=(\\beta_1, \\beta_0)\\) . Let \\(z_i = x_i^\\ast/\\|x_i^\\ast\\|\\) . Show that separability implies the existence of a \\(\\beta_{\\text{sep}}\\) such that \\(y_i\\beta^T_{\\text{sep}}z_i\\ge 1\\ \\ \\forall i\\) . (b) Given a current \\(\\beta_{\\text{old}}\\) , the perceptron algorithm identifies a point \\(z_i\\) that is misclassified, and produces the update \\(\\beta_{\\text{new}}\\leftarrow \\beta_{\\text{old}} + y_iz_i\\) . Show that \\(\\|\\beta_{\\text{new}}-\\beta_{\\text{sep}}\\|^2\\le \\|\\beta_{\\text{old}}-\\beta_{\\text{sep}}\\|^2 - 1\\) , and hence that the algorithm converges to a separating hyperplane in no more than \\(\\|\\beta_{\\text{start}}-\\beta_{\\text{sep}}\\|^2\\) steps \uff08 Pattern Recognition and Neural Networks \uff09. Soln. 4.6 (a) By definition of separability, there exists \\(\\beta\\) such that \\[\\begin{eqnarray} \\beta^Tx_i > &0& \\text{for}\\ \\ y_i = 1\\non\\\\ \\beta^Tx_i < &0& \\text{for}\\ \\ y_i = -1.\\non \\end{eqnarray}\\] Thus we have \\(y_i\\beta^Tx_i > 0\\) for all \\(x_i\\) , thus for \\(y_i\\beta^Tz_i > 0\\) for all \\(z_i\\) . Define \\[\\begin{equation} m := \\min_{i} \\|y_i\\beta^Tz_i\\|.\\non \\end{equation}\\] Thus, \\(y_i(\\frac{1}{m}\\beta^T)z_i\\ge 1\\) . So there exists a \\(\\beta_{\\text{sep}} := \\frac{1}{m}\\beta\\) such that \\(y_i\\beta^T_{\\text{sep}}z_i\\ge 1 \\ \\forall i\\) . (b) We have \\[\\begin{eqnarray} \\|\\beta_{\\text{new}} - \\beta_{\\text{sep}}\\|^2 &=& \\|\\beta_{\\text{old}} - \\beta_{\\text{sep}} + y_iz_i\\|^2\\non\\\\ &=&\\|\\beta_{\\text{old}} - \\beta_{\\text{sep}}\\|^2 + \\|y_iz_i\\|^2 + 2y_i(\\beta_{\\text{old}} - \\beta_{\\text{sep}})^Tz_i\\non\\\\ &=&\\|\\beta_{\\text{old}} - \\beta_{\\text{sep}}\\|^2 + 1 + 2y_i\\beta_{\\text{old}}^Tz_i - 2y_i\\beta_{\\text{sep}}^Tz_i\\non\\\\ &\\le&\\|\\beta_{\\text{old}} - \\beta_{\\text{sep}}\\|^2 + 1 + 2\\cdot 0 - 2 \\cdot 1 \\non\\\\ &=&\\|\\beta_{\\text{old}} - \\beta_{\\text{sep}}\\|^2-1.\\non \\end{eqnarray}\\]","title":"Ex. 4.6"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-7/","text":"Ex. 4.7 Consider the criterion \\[\\begin{equation} D^\\ast(\\beta,\\beta_0) = -\\sum_{i=1}^Ny_i(x_i^T\\beta + \\beta_0),\\non \\end{equation}\\] a generalization of (4.41) in the textbook where we sum over all the observations. Consider minimizing \\(D^\\ast\\) subject to \\(\\|\\beta\\|=1\\) . Describe this criterion in words. Does it solve the optimal separating hyperplane problem? Soln. 4.7 When \\(\\|\\beta\\| = 1\\) , \\(\\beta^Tx_i + \\beta_0\\) is the signed distance of \\(x_i\\) to the hyperplane \\(\\beta^Tx + \\beta_0 = 0\\) . This does not solve the optimal separating hyperplane problem. Optimal separating hyperplane is actually solving a max-min problem such that each point satisfies the distance requirement, however minimizing \\(D^\\ast\\) does not have such pointwise constraint.","title":"Ex. 4.7"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-8/","text":"Ex. 4.8 Consider the multivariate Gaussian model \\(X|G=k \\sim N(\\mu_k, \\bm{\\Sigma})\\) , with the additional restriction that rank \\(\\{\\mu_k\\}_1^K = L < \\max(K-1,p).\\) Derive the constrained MLEs for the \\(\\mu_k\\) and \\(\\bm{\\Sigma}\\) . Show that the Bayes classification rule is equivalent to classifying in the reduced subspace computed by LDA Discriminant adaptive nearest neighbor classification . Soln. 4.8 We are going to maximize \\[\\begin{equation}\\label{eq:4.8loglikelihood} l(\\mu,\\Sigma) = -\\frac{1}{2}\\sum_{l=1}^K\\sum_{g_i=l}(x_i-\\mu_l)^T\\bm{\\Sigma}^{-1}(x_i-\\mu_l) - N\\log|\\bm{\\Sigma}| \\end{equation}\\] subject to \\(\\text{rank}\\{\\mu_k\\}_{k=1}^K = L < \\max(K-1,p)\\) . Let \\(B\\) be the between-class covariance matrix and for fixed \\(\\bm{\\Sigma}\\) , let \\(V\\) denote the matrix of leading \\(K\\) eigenvectors of \\(\\bm{\\Sigma}^{-1}B\\) . (a) \\(\\Sigma\\) Known, \\(\\mu_j\\) Unknown. In Section 12.5.2 of Multivariate Analysis , a solution to \\(\\eqref{eq:4.8loglikelihood}\\) is given assuming that \\(\\Sigma\\) is known. This has the form of the usual LDA solution, except with \\(W\\) replaced by \\(\\bm{\\Sigma}\\) . We can write the estimated means as \\[\\begin{equation} \\label{eq:4.8hatmuj} \\hat\\mu_j = \\bm{\\Sigma} V V^T(\\bar x_j-\\bar x) + \\bar x \\end{equation}\\] and thus the estimated (rank \\(K\\) ) between-matrix as \\[\\begin{equation} \\hat B_{(K)} = \\bm{\\Sigma} V V^TBVV^T\\bm{\\Sigma}.\\non \\end{equation}\\] (b) \\(\\mu_j\\) Known, \\(\\bm{\\Sigma}\\) Unknown. Although the case \\(\\mu_j\\) known, \\(\\bm{\\Sigma}\\) unknown is not explicitly stated in Multivariate Analysis , we deduce (and easily check) from their equation (4.2.7) on p. 104 that \\[\\begin{equation}\\label{eq:4.8hatsigam} \\hat{\\bm{\\Sigma}} = W + \\sum_{k=1}^K\\frac{N_k}{N}(\\bar x_k-\\mu_k)(\\bar x_k-\\mu_k)^T. \\end{equation}\\] These are each obtained by solving the partial source equations for \\(\\mu_j\\) or \\(\\bm{\\Sigma}\\) , assuming that the other is known. The full maximum likelihood solution requires their simultaneous solution and suggests iteration. However, the solution is easier. We plug the estimated means \\(\\eqref{eq:4.8hatmuj}\\) (using \\(W\\) for \\(\\bm{\\Sigma}\\) ) into equation \\(\\eqref{eq:4.8hatsigam}\\) , which gives \\[\\begin{eqnarray} \\hat{\\bm{\\Sigma}} &=& W + \\sum_{k=1}^K\\frac{N_k}{N}(\\bar x_k-\\hat\\mu_k)(\\bar x_k-\\hat\\mu_k)^T\\non\\\\ &=&W + B - \\hat B_{(L)}\\non\\\\ &=&W + B - WVV^TBVV^TW\\non\\\\ &=&W + WV_{\\perp}V_{\\perp}^TBV_{\\perp}V_{\\perp}^TW\\non \\end{eqnarray}\\] where \\(V_{\\perp}^TWV=0\\) and \\(V_{\\perp}^T\\) spans the complementary \\((p-L)-\\) dimensional subspace of \\(\\mathbb{R}^p\\) . To complete the proof, we show that the same \\(V\\) is optimal using the new metric \\(\\hat{\\bm{\\Sigma}}\\) . First note that (1) \\(V^T\\hat{\\bm{\\Sigma}} V =V^TWV + 0 = I_L\\) and (2) \\(BV = WVD_L=\\hat{\\bm{\\Sigma}} VD_L\\) , where the first equality is the definition of \\(V\\) , and \\(D_K=\\text{diag}(\\gamma_1,...,\\gamma_L)\\) . We have established that \\(V\\) is also an eigenmatrix of \\(B\\) with respect to \\(\\hat{\\bm{\\Sigma}}\\) ; we have still to show that it has remained optimal. Note that \\[\\begin{eqnarray} V^T_{\\perp}\\hat{\\bm{\\Sigma}} V_{\\perp} &=& I_{p-L} + V_{\\perp}^TB V_{\\perp}\\non\\\\ &=&I_{p-L}+D_{p-L}\\non \\end{eqnarray}\\] where \\(D_{p-L}\\) are the eigenvalues of \\(B\\) corresponding to \\(V_{\\perp}\\) and \\(W\\) . So, the metric \\(\\hat{\\bm{\\Sigma}}\\) , the columns of \\(V\\) are orthonormal eigenvectors of \\(B\\) , \\(V_{\\perp}\\) is orthogonal and orthogonal to \\(V\\) . Thus the columns of \\(V_{\\perp}\\) remain eigenvectors of \\(B\\) with respect to \\(\\hat{\\bm{\\Sigma}}\\) , with eigenvalues \\((I_{p-L}+D_{p-L})^{-1}D_{p-L}\\le D_{p-L}\\) , and thus the order does not change. This shows that the constrained maximum likelihood estimated means coincide with the rank \\(L\\) LDA means. Using the fact that \\[\\begin{equation} \\hat{\\bm{\\Sigma}}^{-1} = VV^T + V_{\\perp}(D^{-1}_{p-L}(I_{p-L}+D_{p-L}))V_{\\perp}^T,\\non \\end{equation}\\] it is not difficult to show that \\[\\begin{equation} (x-\\hat\\mu_j)^T\\hat{\\bm{\\Sigma}}^{-1}(x-\\hat\\mu_j) - (x-\\hat\\mu_l)^T\\hat{\\bm{\\Sigma}}^{-1}(x-\\hat\\mu_l) \\non \\end{equation}\\] coincides with relative Euclidean distances in the reduced LDA space, and hence classification based on the fitted constrained Gaussian model and LDA coincide.","title":"Ex. 4.8"},{"location":"ESL-Solution/4-Linear-Methods-for-Classification/ex4-9/","text":"Ex. 4.9 Write a computer program to perform a quadratic discriminant analysis by fitting a separate Gaussian model per class. Try it out on the vowel data, and compute the misclassification error for the test data. The data can be found in the book website www-stat.stanford.edu/ElemStatLearn. Soln. 4.9 The misclassification error for the test data is 52.8% in our simulation. See code below. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis import pathlib import numpy as np import pandas as pd # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # train data train = pd . read_csv ( DATA_PATH . joinpath ( \"vowel_train.csv\" ), header = 0 ) y_train = pd . DataFrame ( train [ 'y' ]) y_train = y_train . to_numpy () y_train = np . reshape ( y_train , ( 1 , len ( y_train ))) . ravel () x_train = pd . DataFrame ( train [[ 'x.1' , 'x.2' , 'x.3' , 'x.4' , 'x.5' , 'x.6' , 'x.7' , 'x.8' , 'x.9' , 'x.10' ]]) x_train = x_train . to_numpy () # test data test = pd . read_csv ( DATA_PATH . joinpath ( \"vowel_test.csv\" ), header = 0 ) y_test = pd . DataFrame ( test [ 'y' ]) y_test = y_test . to_numpy () y_test = np . reshape ( y_test , ( 1 , len ( y_test ))) . ravel () x_test = pd . DataFrame ( test [[ 'x.1' , 'x.2' , 'x.3' , 'x.4' , 'x.5' , 'x.6' , 'x.7' , 'x.8' , 'x.9' , 'x.10' ]]) x_test = x_test . to_numpy () # a utility function to calculate error rate # of predictions def getErrorRate ( a , b ): if a . size != b . size : raise ValueError ( 'Expect input arrays have equal size, a has {} , b has {} ' . format ( a . size , b . size )) if a . size == 0 : raise ValueError ( 'Expect non-empty input arrays' ) return np . sum ( a != b ) / a . size # run clf = QuadraticDiscriminantAnalysis () clf . fit ( x_train , y_train ) y_predict = clf . predict ( x_test ) errorRate = getErrorRate ( y_predict , y_test ) print ( errorRate )","title":"Ex. 4.9"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-01/","text":"Ex. 5.1 Show that the truncated power basis functions in (5.3) in the textbook represent a basis for a cubic spline with the two knots as indicated. Soln. 5.1 Let \\[\\begin{equation} f(x) = \\sum_{i=1}^6\\beta_ih_i(x),\\non \\end{equation}\\] where \\(\\beta_i, i=1,...,6\\) are constants and \\[\\begin{eqnarray} &&h_1(x) = 1, \\ h_3(x) = x^2, \\ h_5(x) = (x-\\xi_1)_{+}^3,\\non\\\\ &&h_2(x) = x, \\ h_4(x) = x^3, \\ h_6(x) = (x-\\xi_2)_{+}^3,\\non \\end{eqnarray}\\] and \\(\\xi_1,\\xi_2\\) are constants. We need to show that \\(f(x)\\) is continuous and has continuous first and second derivatives at \\(\\xi_1\\) and \\(\\xi_2\\) (by choosing appropriate \\(\\beta_i\\) ). Let's rewrite \\[\\begin{equation} f(x) = \\begin{cases} \\beta_1 + \\beta_2x + \\beta_3x^2 + \\beta_4x^3, & \\text{if } x \\le \\xi_1\\\\ \\beta_1 + \\beta_2x + \\beta_3x^2 + \\beta_4x^3 + \\beta_5(x-\\xi_1)^3, & \\text{if } \\xi_1 < x < \\xi_2\\\\ \\beta_1 + \\beta_2x + \\beta_3x^2 + \\beta_4x^3 + \\beta_5(x-\\xi_1)^3 + \\beta_6(x-\\xi_2)^3, & \\text{if } x \\ge \\xi_2.\\non \\end{cases} \\end{equation}\\] For \\(\\xi_1\\) , it's easy to see that \\[\\begin{equation} f'_{-}(\\xi_1) = \\lim_{h\\ra 0^{-}}\\frac{f(x+h)-f(x)}{h} = \\beta_2 + 2\\beta_3\\xi_1 + 3\\beta_4\\xi_1^2\\non \\end{equation}\\] and \\[\\begin{equation} f'_{+}(\\xi_1) = \\lim_{h\\ra 0^{+}}\\frac{f(x+h)-f(x)}{h} = \\beta_2 + 2\\beta_3\\xi_1 + 3\\beta_4\\xi_1^2 + 0\\non \\end{equation}\\] so that the left-hand derivative at \\(\\xi_1\\) equals its right-hand derivative and thus \\(f'\\) is continuous at \\(\\xi_1\\) . Similarly, we have \\[\\begin{equation} f''_{-}(\\xi_1) = \\lim_{h\\ra 0^{-}}\\frac{f'(x+h)-f'(x)}{h} = 2\\beta_3 + 6\\beta_4\\xi_1\\non \\end{equation}\\] and \\[\\begin{equation} f''_{+}(\\xi_1) = \\lim_{h\\ra 0^{+}}\\frac{f'(x+h)-f'(x)}{h} = 2\\beta_3 + 6\\beta_4\\xi_1 + 0,\\non \\end{equation}\\] and thus \\(f''\\) is continuous at \\(\\xi_1\\) . Similar arguments apply to \\(\\xi_2\\) . Therefore, it is easy to see that \\(f(x)\\) is continuous and has continuous first and second derivatives for \\(x\\in\\mathbb{R}\\) .","title":"Ex. 5.1"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-02/","text":"Ex. 5.2 Suppose that \\(B_{i,M}(x)\\) is an order- \\(M\\) \\(B\\) -spline defined in the Appendix on page 186 through the sequence (5.77)-(5.78). (a) Show by induction that \\(B_{i,M}(x) = 0\\) for \\(x\\not \\in [\\tau_i, \\tau_{i+M}]\\) . This shows, for example, that the support of cubic \\(B\\) -splines is at most 5 knots. (b) Show by induction that \\(B_{i,M}(x) > 0\\) for \\(x\\in (\\tau_i, \\tau_{i+M})\\) . The \\(B\\) -splines are positive in the interior of their support. (c) Show by induction that \\(\\sum_{i=1}^{K+M}B_{i,M}(x) = 1 \\ \\forall x\\in [\\xi_0, \\xi_{K+1}]\\) . (d) Show that \\(B_{i,M}\\) is a piecewise polynomial of order \\(M\\) (degree M-1) on \\([\\xi_0, \\xi_{K+1}]\\) , with breaks only at the knots \\(\\xi_1,...,\\xi_K\\) . (e) Show that an order- \\(M\\) \\(B\\) -spline basis function is the density function of a convolution of \\(M\\) uniform random variables. Warning In (e), the claim does not seem to be true unless \\(\\tau_{i+1}-\\tau_{i}=1\\) . See details below. Soln. 5.2 (a) For \\(B_{i,1}(x)\\) , by (5.77) we know that \\(B_{i,1}=0\\) for \\(x\\not \\in [\\tau_1, \\tau_2]\\) . Assume that \\(B_{i, m-1}(x)=0\\) for \\(x\\not\\in [\\tau_i, \\tau_{i+m-1}]\\) . If \\(x\\not \\in [\\tau_i, \\tau_{i+m}]\\) , then \\(x\\not \\in [\\tau_i, \\tau_{i+m-1}]\\) , thus \\(B_{i, m-1}(x) =0\\) by our assumption. Similarly \\(x\\not\\in [\\tau_{i+1}, \\tau_{i+m}]\\) , thus \\(B_{i+1, m-1}(x) = 0\\) by our assumption again. Therefore, by (5.78), for \\(x\\not \\in [\\tau_i, \\tau_{i+m}]\\) , we have \\(B_{i,m}(x) = 0\\) . By mathematical induction, the proof is complete. For cubic \\(B\\) -splines, \\(M=4\\) , their support is at most \\(\\tau_1, \\tau_2, ..., \\tau_5\\) , i.e., at most 5 knots. (b) For \\(B_{i,1}(x)\\) , by (5.77), \\(B_{i,1}(x) = 1 > 0\\) for \\(x\\in (\\tau_1, \\tau_2)\\) . Assume that \\(B_{i,m-1}(x) > 0\\) for \\(x\\in (\\tau_{i}, \\tau_{i+m-1})\\) . For any \\(x\\in (\\tau_i, \\tau_{i+m})\\) , either \\(x\\in (\\tau_i, \\tau_{i+m-1})\\) or \\(x\\in (\\tau_{i+1}, \\tau_{i+m})\\) . In either case, by (5.78) and our assumption, we have \\(B_{i, m}(x) > 0\\) . Therefore, the proof is complete. (c) When \\(M=1\\) , by (5.77) we have \\(\\sum_{i=1}^{K+1}B_{i,M}(x) = 1\\) for \\(x\\in [\\xi_0,\\xi_{K+1}]\\) . Suppose that the same holds for \\(M=m-1\\) , i.e., \\(\\sum_{i=1}^{K+m-1}B_{i, m-1}(x)\\) for \\(x\\in [\\xi_0, \\xi_{K+1}]\\) . Let's move to the case when \\(M=m\\) now. By (a), we have \\(B_{1, m-1}(x) =0\\) for \\(x\\not\\in [\\tau_1, \\tau_{m}]\\) , thus also for \\(x\\not\\in [\\xi_0,\\xi_{K+1}]\\) . Similarly, \\(B_{K+m, m-1}(x)=0\\) and \\(B_{K+m+1, m-1}(x)=0\\) for \\(x\\not\\in [\\xi_0,\\xi_{K+1}]\\) . By (5.78), we have \\(x \\in [\\xi_0,\\xi_{K+1}]\\) \\[\\begin{eqnarray} \\sum_{i=1}^{K+m}B_{i,m}(x) &=& \\sum_{i=1}^{K+m}\\left[\\frac{x-\\tau_i}{\\tau_{i+m-1}-\\tau_i}B_{i,m-1}(x) + \\frac{\\tau_{i+m}-x}{\\tau_{i+m}-\\tau_{i+1}}B_{i+1, m-1}(x)\\right]\\non\\\\ &=&\\sum_{i=1}^{K+m}\\frac{x-\\tau_i}{\\tau_{i+m-1}-\\tau_i}B_{i, m-1}(x) + \\sum_{i=1}^{K+m}\\frac{\\tau_{i+m}-x}{\\tau_{i+m}-\\tau_{i+1}}B_{i+1, m-1}(x)\\non\\\\ &=&\\sum_{i=2}^{K+m-1}\\frac{x-\\tau_i}{\\tau_{i+m-1}-\\tau_i}B_{i, m-1}(x) + \\sum_{i=2}^{K+m-1}\\frac{\\tau_{i+m-1}-x}{\\tau_{i+m-1}-\\tau_{i}}B_{i, m-1}(x)\\non\\\\ &=&\\sum_{i=2}^{K+m-1} \\left(\\frac{x-\\tau_i}{\\tau_{i+m-1}-\\tau_i} + \\frac{\\tau_{i+m-1}-x}{\\tau_{i+m-1}-\\tau_{i}}\\right)B_{i, m-1}(x)\\non\\\\ &=&\\sum_{i=1}^{K+m-1}B_{i, m-1}(x)\\non\\\\ &=&1.\\non \\end{eqnarray}\\] Thus the proof is complete. (d) Again we use induction. When \\(M=1\\) , by (5.77), \\(B_{i,1}\\) is a piecewise polynomial of order 1 on \\([\\xi_0, \\xi_{K+1}]\\) with breaks only at the knots \\(\\xi_1,...,\\xi_K\\) . Suppose the same holds true for \\(M=m-1\\) , by (5.78), \\(B_{i,m}\\) increases 1 order because \\(x\\) multipliy by \\(B_{i,m-1}(x)\\) in both summands. It's easy to see the break points remain the same. (e) Note that the claim does not seem to be true unless \\(\\tau_{i+1}-\\tau_{i}=1\\) . For example, consider \\(B_{i,1}(x) = \\bb{1}([\\tau_i, \\tau_{i+1}))\\) . We have \\[\\begin{equation} \\int B_{i,1}(x)dx = \\tau_{i+1} - \\tau_i.\\non \\end{equation}\\] So unless \\(\\tau_{i+1}-\\tau_i=1\\) , \\(B_{i,1}\\) is not a density function. With this condition, we see that (5.78) in the text reduces to \\[\\begin{equation} B_{i,m}(x) = \\frac{x-\\tau_i}{m-1}B_{i,m-1}(x) + \\frac{\\tau_{i+m}-x}{m-1}B_{i+1,m-1}(x),\\non \\end{equation}\\] for each \\(i=1,...,K+2M-m\\) . Now it's easy to verify that \\(B_{i,m}\\) is a convolution of \\(B_{i,m-1}\\) and the density function, denoted as \\(f\\) , of \\(U(0,1)\\) where \\(U\\) is uniformly distributed on \\([0,1]\\) , thus the claim is true. For example, when \\(m=2\\) , by the definition above, we have \\[\\begin{equation} B_{i,2}(x) = \\begin{cases} x - \\tau_i & \\text{ if } x\\in [\\tau_i, \\tau_{i+1})\\non\\\\ \\tau_{i+2} - x & \\text{ if } x\\in [\\tau_{i+1}, \\tau_{i+2})\\non\\\\ 0 & \\text{otherwise}. \\end{cases} \\end{equation}\\] On the other hand, it's easy to verify that \\[\\begin{equation} \\int f(x-y)B_{i,1}(y)dy = \\begin{cases} x - \\tau_i & \\text{ if } x\\in [\\tau_i, \\tau_{i+1})\\non\\\\ \\tau_{i+2} - x & \\text{ if } x\\in [\\tau_{i+1}, \\tau_{i+2})\\non\\\\ 0 & \\text{otherwise}. \\end{cases} \\end{equation}\\] So we know that the claim holds for \\(m=2\\) .","title":"Ex. 5.2"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-03/","text":"Ex. 5.3 Write a program to reproduce Figure 5.3 on page 145. Soln. 5.3 Figure 1 is a reproduced version. We sample 50 points, \\(x_i\\) , uniformly from \\([0,1]\\) and let i.i.d. error terms \\(\\epsilon_i\\sim N(0, 0.01)\\) . The linear and cubic polynomial fits have two and four degrees of freedom, respectively, while the cubic spline and natural cubic spline each have six degrees of freedom. The cubic spline has two knots at 0.33 and 0.66, while the natural spline has boundary knots at 0.1 and 0.9, and four interior knots uniformly spaced between them. Figure 1: Pointwise Variance Curves for Four Different Models Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 import numpy as np from patsy import dmatrix import statsmodels.api as sm import plotly.graph_objects as go from numpy.linalg import inv # generate data np . random . seed ( 42 ) x = np . random . uniform ( 0 , 1 , 50 ) x = np . sort ( x ) err = np . random . normal ( 0 , 1 , 50 ) y = np . add ( x , err ) # linear regression def GLCov ( x , sigma ): x_m = np . array ([ np . ones ( len ( x )), x ]) . transpose () x_m_t = x_m . transpose () x_c = np . matmul ( x_m_t , x_m ) x_c_inv = inv ( x_c ) x_c_inv = x_c_inv * ( sigma * sigma ) return x_c_inv cov = GLCov ( x , 1 ) pt_var = cov [ 0 ][ 0 ] + cov [ 1 ][ 1 ] * x * x + 2 * cov [ 0 ][ 1 ] * x # global cubic from numpy.linalg import inv from numpy.linalg import multi_dot def GlobalCubicCov ( x , sigma ): x_m = np . array ([ np . ones ( len ( x )), x , x * x , x * x * x ]) . transpose () x_m_t = x_m . transpose () x_c = np . matmul ( x_m_t , x_m ) x_c_inv = inv ( x_c ) x_c_inv = x_c_inv * ( sigma * sigma ) return x_c_inv x_square = x * x x_cubic = x * x * x m = GlobalCubicCov ( x , 1 ) x_m = np . array ([ np . ones ( len ( x )), x , x * x , x * x * x ]) . transpose () res = multi_dot ([ x_m , m , x_m . transpose ()]) pt_var_cubic = res . diagonal () # Fit a cubic spline with two knots at 0.33 and 0.66 x_cubic = dmatrix ( 'bs(x, knots=(0.33, 0.66))' , { 'x' : x }) fit_cubic = sm . GLM ( y , x_cubic ) . fit () # Fit a natural spline with lower and upper bounds x_natural = dmatrix ( 'cr(x, df=6, lower_bound=0.1, upper_bound=0.9)' , { 'x' : x }) fit_natural = sm . GLM ( y , x_natural ) . fit () # Create spline lines for 50 evenly spaced values of age # line_cubic = fit_cubic.predict(dmatrix('bs(xp, knots=(0.33, 0.66))', {'xp': x})) # line_natural = fit_natural.predict(dmatrix('cr(xp, df=6)', {'xp': x})) # natural cubic spline H = np . asarray ( x_natural ) sigma = 1 m_Sigma = sigma * sigma * ( inv ( np . matmul ( H . transpose (), H ))) m_cubic_natural = multi_dot ([ H , m_Sigma , H . transpose ()]) res_cubic_natural = m_cubic_natural . diagonal () # cubic spline H = np . asarray ( x_cubic ) sigma = 1 m_Sigma = sigma * sigma * ( inv ( np . matmul ( H . transpose (), H ))) m_cubic = multi_dot ([ H , m_Sigma , H . transpose ()]) res_cubic = m_cubic . diagonal () # Create traces fig = go . Figure () fig . add_trace ( go . Scatter ( x = x , y = pt_var , mode = 'lines+markers' , name = 'Global Linear' )) fig . add_trace ( go . Scatter ( x = x , y = pt_var_cubic , mode = 'lines+markers' , name = 'Global Cubic Spline' )) fig . add_trace ( go . Scatter ( x = x , y = res_cubic , mode = 'lines+markers' , name = 'Cubic Spline - 2 knots' )) fig . add_trace ( go . Scatter ( x = x , y = res_cubic_natural , mode = 'lines+markers' , name = 'Natural Cubic Spline - 6 knots' )) fig . update_layout ( xaxis_title = \"x\" , yaxis_title = \"Pointwise Variance\" , ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) fig . show ()","title":"Ex. 5.3"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-04/","text":"Ex. 5.4 Consider the truncated power series representation for cubic splines with \\(K\\) interior knots. Let \\[\\begin{equation} f(X) = \\sum_{j=0}^3\\beta_jX^j + \\sum_{k=1}^K\\theta_k(X-\\xi_k)_{+}^3.\\non \\end{equation}\\] Prove that the natural boundary conditions for natural cubic splines (Section 5.2.1) imply the following linear constraints on the coefficients: \\[\\begin{eqnarray} \\beta_2 = 0, \\ \\ \\sum_{k=1}^K\\theta_k = 0\\non\\\\ \\beta_3 = 0, \\ \\ \\sum_{k=1}^K\\xi_k\\theta_k=0.\\non \\end{eqnarray}\\] Hence derive the basis (5.4) and (5.5). Soln. 5.4 When \\(X<\\xi_1\\) , \\(f(X) = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3\\) , since \\(f(X)\\) is linear when \\(X < \\xi_1\\) , we have \\(\\beta_2=0\\) and \\(\\beta_3=0\\) . When \\(X > \\xi_K\\) , we have \\[\\begin{eqnarray} f(X) &=& \\sum_{j=0}^3\\beta_jX^j + \\sum_{k=1}^K\\theta_k(X-\\xi_k)^3\\non\\\\ &=&\\beta_0 + \\beta_1X + 3\\left(\\sum_{k=1}^K\\theta_k\\xi_k^2\\right) X - 3\\left(\\sum_{k=1}^K\\theta_k\\xi_k\\right)X^2 + \\left(\\sum_{k=1}^K\\theta_k\\right)X^3.\\non \\end{eqnarray}\\] Since \\(f(X)\\) is linear when \\(X > \\xi_K\\) , we have \\[\\begin{equation} \\label{eq:5-4a} \\sum_{k=1}^K\\theta_k = 0 \\text{ and } \\sum_{k=1}^K\\theta_k\\xi_k = 0. \\end{equation}\\] To derive (5.4) and (5.5) in the text, we need to rewrite \\(f(X)\\) as a series summation, in the form of \\(\\sum_k\\alpha_k N_k(X)\\) , defined in (5.4) and (5.5). It's easy to see that for \\(N_1(X)\\) and \\(N_2(X)\\) , we have \\(\\alpha_1 = \\beta_1\\) and \\(\\alpha_2 = \\beta_2\\) . By comparing the coefficient of \\((X-\\xi_k)_+^3\\) for each \\(k \\le K-2\\) , we see that \\(\\alpha_k = (\\xi_K-\\xi_k)\\theta_k\\) . It remains to verify that \\[\\begin{equation} \\sum_{k=1}^{K-2}\\alpha_kN_{k+2}(X) = \\sum_{k=1}^K\\theta_k(X-\\xi_k)_{+}^3.\\non \\end{equation}\\] We have \\[\\begin{eqnarray} &&\\sum_{k=1}^{K-2}\\alpha_kN_{k+2}(X)\\non\\\\ &=& \\sum_{k=1}^{K-2}(\\xi_K-\\xi_k)\\theta_k\\left[\\frac{(X-\\xi_k)_+^3-(X-\\xi_K)_+^3}{\\xi_K-\\xi_k} - \\frac{(X-\\xi_{K-1})_+^3-(X-\\xi_K)_+^3}{\\xi_K-\\xi_{K-1}}\\right]\\non\\\\ &=&\\sum_{k=1}^{K-2}\\theta_k(x-\\xi_k)_+^3 -\\left(\\sum_{k=1}^{K-2}\\theta_k\\right)(x-\\xi_K)_+^3 \\non\\\\ && - \\frac{1}{\\xi_K-\\xi_{K-1}}\\left[\\xi_K\\left(\\sum_{k=1}^{K-2}\\theta_k\\right) - \\sum_{k=1}^{K-2}\\xi_k\\theta_k\\right][(X-\\xi_{K-1})_+^3-(X-\\xi_K)_+^3].\\non \\end{eqnarray}\\] Note that by \\(\\eqref{eq:5-4a}\\) , we have \\(\\sum_{k=1}^{K-2}\\theta_k=-\\theta_{K-1}-\\theta_K\\) and \\(\\sum_{k=1}^{K-2}\\xi_k\\theta_k=-\\theta_{K-1}\\xi_{K-1} - \\theta_K\\xi_K\\) , the summation above becomes \\[\\begin{eqnarray} && \\sum_{k=1}^{K-2}\\theta_k(x-\\xi_k)_+^3 + (\\theta_{K-1}+\\theta_K)(x-\\xi_K)_+^3 \\non\\\\ && - \\frac{(-\\xi_K\\theta_{K-1}-\\xi_K\\theta_K + \\xi_{K-1}\\theta_{K-1} + \\xi_K\\theta_K)}{\\xi_K-\\xi_{K-1}}[(X-\\xi_{K-1})_+^3-(X-\\xi_K)_+^3]\\non\\\\ &=& \\sum_{k=1}^{K-2}\\theta_k(x-\\xi_k)_+^3 + (\\theta_{K-1}+\\theta_K)(x-\\xi_K)_+^3\\non\\\\ && + \\theta_{K-1}[(X-\\xi_{K-1})_+^3-(X-\\xi_K)_+^3]\\non\\\\ &=& \\sum_{k=1}^K\\theta_k(x-\\xi_k)_+^3.\\non \\end{eqnarray}\\] Now the proof is complete.","title":"Ex. 5.4"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-05/","text":"Ex. 5.5 Write a program to classify the phoneme data using a quadratic discriminant analysis (Section 4.3). Since there are many correlated features, you should filter them using a smooth basis of natural cubic splines (Section 5.2.3). Decide beforehand on a series of five different choices for the number and position of the knots, and use tenfold cross-validation to make the final selection. The phoneme data are available from the book website . Soln. 5.5 We first try to reproduce lower panel of Figure 5.5 in the text. First we fit the raw data using a logistic regression as mentioned in the text. This will give us jagged gray curve. Second, to use natural cubic spline to filter input \\(p=256\\) frequencies, we need to get the basis matrix \\(\\bb{H}\\in\\mathbb{R}^{p\\times 12}\\) of natural cubic spline on integer set \\(\\{1,2, ..., 256\\}\\) with degree of freedom 11. Then we replace original \\(\\bX\\in\\mathbb{R}^{N\\times p}\\) with \\(\\bX^\\ast = \\bX\\bb{H}\\) , and fit a logistic regression using \\(\\bX\\) . We obtain the fitted coefficients \\(\\hat\\beta\\) and the red curve in the figure is simply recovered from \\(\\bb{H}\\hat\\beta\\) . The reproduced version is Figure 1. The implementation details can be found below. Figure 1: Reproduce of Figure 5.5 What this exercise asks is, instead of using logistic regression, we need to use quadratic discriminant analysis (QDA) together with a 10-fold cross validation. We also need to choose five different internal knots sets, which can be done by choosing five different degree of freedoms and by defaults the knots are assumed to be uniformly placed over our frequency set. At the end, we choose the model with the best error rate. Table 1 displays the numerical results from our experiments. We see that when we have the lowest mean error rate with \\(\\text{df}=11\\) . DF Mean Error Rate 5 0.263 11 0.186 50 0.189 100 0.220 200 0.357 Table 1: Mean Error Rates for Different Choices of Knots Code: reproduce Figure 5.5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 import pathlib import numpy as np import pandas as pd from patsy import dmatrix import plotly.graph_objects as go from sklearn.linear_model import LogisticRegression # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # phoneme data data = pd . read_csv ( DATA_PATH . joinpath ( \"phoneme.csv\" ), header = 0 ) values = [ 'aa' , 'ao' ] data = data . loc [ data [ 'g' ] . isin ( values )] X = pd . DataFrame ( data . iloc [:, 1 : 257 ]) y = pd . DataFrame ( data [ 'g' ]) frequencies = np . arange ( 257 )[ 1 :] H = dmatrix ( 'cr(x, df=11)' , { 'x' : frequencies }, return_type = \"dataframe\" ) X_ast = np . dot ( X , H ) clf = LogisticRegression ( random_state = 0 ) . fit ( X_ast , y ) beta = clf . coef_ red_curve = np . dot ( H , beta . transpose ()) red_curve = np . array ( red_curve ) . ravel () X = np . array ( X ) clf = LogisticRegression ( random_state = 0 ) . fit ( X , y ) beta = clf . coef_ raw = np . array ( beta ) . ravel () fig = go . Figure () fig . add_trace ( go . Scatter ( x = frequencies , y = red_curve , mode = 'lines' , name = 'Regularized' , line_color = '#cc3300' )) fig . add_trace ( go . Scatter ( x = frequencies , y = raw , mode = 'lines' , name = 'Raw' , line_color = '#666699' )) # Add figure title fig . update_layout ( title_text = 'Phoneme Classification: Raw and Restricted Logistic Regression' ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) # Set x-axis title fig . update_xaxes ( title_text = 'Frequency' ) # Set y-axes titles fig . update_yaxes ( title_text = 'Logistic Regression Coefficients' ) fig . show () Code 5.5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import pathlib import numpy as np import pandas as pd from patsy import dmatrix from sklearn.model_selection import cross_val_score from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # phoneme data data = pd . read_csv ( DATA_PATH . joinpath ( \"phoneme.csv\" ), header = 0 ) values = [ 'aa' , 'ao' ] data = data . loc [ data [ 'g' ] . isin ( values )] X = data . iloc [:, 1 : 257 ] y = pd . DataFrame ( data [ 'g' ]) frequencies = np . arange ( 257 )[ 1 :] # choose 5 different degree of freedoms # internal knots are uniformly distributed by default dfs = np . array ([ 5 , 11 , 50 , 100 , 200 ]) for df in dfs : H = dmatrix ( 'cr(x, df= {} )' . format ( df ), { 'x' : frequencies }, return_type = \"dataframe\" ) X_ast = np . dot ( X , H ) clf = QuadraticDiscriminantAnalysis () acc_scores = cross_val_score ( clf , X_ast , y , cv = 10 , scoring = 'accuracy' ) # 10 fold CV mean_err_rate = ( 1 - acc_scores ) . mean () print ( 'When df is {} , mis-classification error rate is {} ' . format ( df , mean_err_rate ))","title":"Ex. 5.5"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-06/","text":"Ex. 5.6 Suppose you wish to fit a periodic function, with a known period \\(T\\) . Describe how you could modify the truncated power series basis to achieve this goal. Soln. 5.6 If the period \\(T\\) is known, without loss of generality, the problem reduces to use truncated power series on domain \\([0, T]\\) . For \\(x\\in \\mathbb{R}\\) , it's easy to map it to \\(x^\\ast\\in [0,T]\\) such that \\(f(x) = f(x^\\ast)\\) . Alternatively, we could consider Fourier basis for periodic function.","title":"Ex. 5.6"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-07/","text":"Ex. 5.7 Derivation of smoothing splines ( Nonparametric Regression and Generalized Linear Models ). Suppose that \\(N\\ge 2\\) , and that \\(g\\) is the natural cubic spline interpolant to the pairs \\({x_i, z_i}_1^N\\) , with \\(a<x_1<\\dots<x_N<b\\) . This is a natural spline with a knot at every \\(x_i\\) ; being an \\(N\\) -dimensional space of functions, we can determine the coefficients such that it interpolates the sequence \\(z_i\\) exactly. Let \\(\\tilde g\\) be any other differentiable function on \\([a,b]\\) that interpolates the \\(N\\) pairs. (a) Let \\(h(x) = \\tilde g(x) - g(x)\\) . Use integration by parts and the fact that \\(g\\) is a natural cubic spline to show that \\[\\begin{eqnarray} \\int_a^bg''(x)h''(x)dx &=& -\\sum_{j=1}^{N-1}g'''(x_j^+)\\{h(x_{j+1}-h(x_j))\\}\\non\\\\ &=&0.\\non \\end{eqnarray}\\] (b) Hence show that \\[\\begin{equation} \\int_a^b\\tilde g''(t)^2dt\\ge \\int_a^bg''(x)^2dt,\\non \\end{equation}\\] and the equality can only hold if \\(h\\) is identically zero in \\([a,b]\\) . (c) Consider the penalized least squares problem \\[\\begin{equation} \\min_f \\left[\\sum_{i=1}^N(y_i-f(x_i))^2 + \\lambda\\int_a^bf''(t)^2dt\\right].\\non \\end{equation}\\] Use (b) to argue that the minimizer must be a cubic spline with knots at each of the \\(x_i\\) . Soln. 5.7 (a) Using integration by parts, we obtain \\[\\begin{eqnarray} \\int_a^bg''(x)h''(x)dx &=& [h'(b)g''(b)-h'(a)g''(a)] - \\int_a^bg'''(x)h'(x)dx\\non\\\\ &=&0-\\int_a^bg'''(x)h'(x)dx\\non\\\\ &=&-\\int_{x_1}^{x_N}g'''(x)h'(x)dx,\\non \\end{eqnarray}\\] where the second equation and the last equation follow from the fact that \\(g\\) is a natural cubic spline thus linear on \\([a, x_1]\\) and \\([x_N, b]\\) . Again, since \\(g\\) is a natural cubic spline, so that \\(g'''\\) is a constant on each interval \\((x_j, x_{j+1})\\) for \\(j=1,...,N-1\\) . By definition of integration we obtain the item above is equal to \\[\\begin{equation} -\\sum_{j=1}^{N-1}g'''(x_j^+)\\{h(x_{j+1})-h(x_j)\\}.\\non \\end{equation}\\] Since both \\(\\tilde g\\) and \\(g\\) interpolate the \\(N\\) pairs \\(\\{x_i, z_i\\}_1^N\\) , we know \\(h(x_j)=0\\) for \\(j=1,...,N\\) . Thus the proof is complete. (b) From (a), we have \\[\\begin{equation} \\int_a^bg''(t)h''(t)dt = \\int_a^bg''(t)(\\tilde g''(t)-g''(t))dt = 0\\non \\end{equation}\\] so that \\[\\begin{equation} \\label{eq:57tmp} \\int_a^bg''(t)^2dt = \\int_a^b\\tilde g''(t)g''(t)dt.\\non \\end{equation}\\] Then, we start with \\(\\int_a^b h''(t)^2dt\\ge 0\\) . \\[\\begin{eqnarray} \\int_a^b h''(t)^2dt &=& \\int_a^b (\\tilde g''(t)-g''(t))^2dt\\non\\\\ &=&\\int_a^b [\\tilde g''(t)^2 + g''(t)^2 - 2\\tilde g''(t)g''(t)]dt\\non\\\\ &=&\\int_a^b [\\tilde g''(t)^2 + g''(t)^2 - 2g''(t)^2]dt\\non\\\\ &=&\\int_a^b [\\tilde g''(t)^2 - g''(t)^2]dt\\non\\\\ &\\ge&0. \\non \\end{eqnarray}\\] Thus we have \\[\\begin{equation} \\int_a^b\\tilde g''(t)^2dt \\ge \\int_a^bg''(t)^2dt,\\non \\end{equation}\\] and the equality can only hole if \\(h''\\) is zero in \\([a,b]\\) . Thus \\(h\\) is linear in \\([a,b]\\) . However by definition \\(h(x_j)=0\\) for all \\(j=1, ..., N\\) , we conclude that \\(h\\) must be identically zero in \\([a,b]\\) . (c) Consider a minimizer \\(f_0\\) . We can always construct a natural cubic spline \\(f\\) such that \\(f_0\\) and \\(f\\) have the same values at each of \\(\\{x_i, i=1,...,N\\}\\) , therefor we have \\(\\sum_{i=1}^N(y_i-f_0(x_i))^2 = \\sum_{i=1}^N(y_i-f(x_i))^2\\) . From (b) we know that \\[\\begin{equation} \\int_a^b f_0''(t)^2dt \\ge \\int_a^b f''(t)^2dt.\\non \\end{equation}\\] Since \\(f_0\\) is a minimize, the above inequality is actually a equality, therefore by (b) again, \\(f_0=f\\) in \\([a, b]\\) . The proof is complete.","title":"Ex. 5.7"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-08/","text":"Ex. 5.8 In the appendix to this chapter we show how the smoothing spline computations could be more efficiently carried out using a \\((N + 4)\\) dimensional basis of B-splines. Describe a slightly simpler scheme using a \\((N +2)\\) dimensional B-spline basis defined on the \\(N-2\\) interior knots. Soln. 5.8 I believe the relevant text is in the last section of Appendix in Chapter 5. The use of B-splines reduces the complexity from \\(O(N^3)\\) to \\(O(N)\\) via Cholesky decomposition of a 4-banded matrix \\((\\textbf{B}^T\\textbf{B}+\\lambda \\bm{\\Omega})\\) . It seems to me, using \\((N+2)\\) dimensional B-splines yields \\(\\textbf{B}\\in \\mathbb{R}^{N\\times (N+2)}\\) and \\(\\bm{\\Omega}\\in\\mathbb{R}^{(N+2)\\times (N+2)}\\) , which is slightly simpler than \\(N\\times (N+4)\\) and \\((N+4)\\times (N+4)\\) matrices, however it's unclear to me what the essential differences are.","title":"Ex. 5.8"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-09/","text":"Ex. 5.9 Derive the Reinsch form \\(\\bb{S}_\\lambda = (\\bb{I} + \\lambda \\bb{K})^{-1}\\) for the smoothing spline. Soln. 5.9 We have \\[\\begin{eqnarray} \\bb{S} &=& \\bb{N}(\\bb{N}^T\\bb{N} + \\lambda\\bm{\\Omega}_N)^{-1}\\bb{N}^T\\non\\\\ &=&\\bb{N}(\\bb{N}^T(\\bb{I} + \\lambda (\\bb{N}^T)^{-1}\\bm{\\Omega}_N\\bb{N}^{-1})\\bb{N})^{-1}\\bb{N}^T\\non\\\\ &=&(\\bb{I} + \\lambda \\bb{K})^{-1}\\non \\end{eqnarray}\\] where \\(\\bb{K} = (\\bb{N}^T)^{-1}\\bm{\\Omega}_N\\bb{N}^{-1}\\) .","title":"Ex. 5.9"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-10/","text":"Ex. 5.10 Derive an expression for \\(\\text{Var}(\\hat f_\\lambda(x_0))\\) and \\(\\text{Bias}(\\hat f_\\lambda(x_0))\\) . Using the example (5.22), create a version of Figure 5.9 where the mean and several (pointwise) quantiles of \\(\\hat f_\\lambda(x)\\) are shown. Soln. 5.10 Since \\[\\begin{equation} \\hat{\\mathbf{f}} = \\bb{N}(\\bb{N}^T\\bb{N} + \\lambda \\bm{\\Omega}_N)^{-1}\\bb{N}^T\\by\\non \\end{equation}\\] we have \\[\\begin{equation} \\hat f_\\lambda(x_0) = \\bb{N}(x_0)^T(\\bb{N}^T\\bb{N} + \\lambda \\bm{\\Omega}_N)^{-1}\\bb{N}^T\\by\\non \\end{equation}\\] where \\(\\bb{N}(x_0) = (N_1(x_0), ..., N_N(x_0))^T\\) . Therefore we have \\[\\begin{eqnarray} \\text{Var}(\\hat f_\\lambda(x_0)) &=& \\bb{N}(x_0)^T(\\bb{N}^T\\bb{N} + \\lambda \\bm{\\Omega}_N)^{-1}\\bb{N}^T\\text{Var}(\\by)(\\bb{N}^T\\bb{N} + \\lambda \\bm{\\Omega}_N)^{-1}\\bb{N}(x_0)\\non \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\text{Bias}(\\hat f_\\lambda(x_0)) &=& f_\\lambda(x_0) - E[\\hat f_\\lambda(x_0)]\\non\\\\ &=&f_\\lambda(x_0) - \\bb{N}(x_0)(\\bb{N}^T\\bb{N} + \\lambda \\bm{\\Omega}_N)^{-1}\\bb{N}^T\\by.\\non \\end{eqnarray}\\] Next we reproduce Figure 5.9 in the text. Since smoothing splines has an explicit, finite-dimensional, unique minimizer which is a natural cubic spline with knots at the unique values of the \\(x_i\\) , we could use codes for natural cubic splines (I haven't found a convenient package for smoothing splines in Python yet.) Figure 1: df=5 Figure 2: df=9 Figure 3: df=15 Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import numpy as np from numpy.linalg import multi_dot import statsmodels.api as sm from patsy import dmatrix import plotly.graph_objects as go from numpy.linalg import inv np . random . seed ( 42 ) n = 100 sigma = 1 def f ( x ): return np . sin ( 12 * ( x + 0.2 )) / ( x + 0.2 ) X = np . random . uniform ( 0 , 1 , n ) X = np . sort ( X ) epsilon = np . random . normal ( 0 , sigma , n ) y_true = f ( X ) y_realized = y_true + epsilon df = 15 x_nc = dmatrix ( 'cr(x, df= {} )' . format ( df ), { 'x' : X }, return_type = \"dataframe\" ) fit_natural = sm . GLM ( y_realized , x_nc ) . fit () line_nc = fit_natural . predict ( dmatrix ( 'cr(xp, df= {} )' . format ( df ), { 'xp' : X })) H = np . asarray ( x_nc ) sigma = 1 m_Sigma = sigma * sigma * ( inv ( np . matmul ( H . transpose (), H ))) m_nc = multi_dot ([ H , m_Sigma , H . transpose ()]) pt_var_nc = m_nc . diagonal () pt_std_nc = np . sqrt ( pt_var_nc ) upper = line_nc + 2 * pt_std_nc lower = line_nc - 2 * pt_std_nc # Create traces fig = go . Figure () fig . add_trace ( go . Scatter ( x = X , y = y_realized , mode = 'markers' , name = 'Raw' )) fig . add_trace ( go . Scatter ( x = X , y = y_true , mode = 'lines' , name = 'True' , line_color = '#993399' )) fig . add_trace ( go . Scatter ( x = X , y = line_nc , mode = 'lines' , name = 'Fitted' , line_color = '#009933' )) fig . add_trace ( go . Scatter ( x = X , y = lower , mode = 'lines' , name = 'Lower Limit' , line_color = '#ffff99' )) fig . add_trace ( go . Scatter ( x = X , y = upper , mode = 'lines' , name = 'Upper Limit' , fill = 'tonexty' , line_color = '#ffff99' )) fig . update_layout ( xaxis_title = \"X\" , yaxis_title = \"y\" , ) fig . show ()","title":"Ex. 5.10"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-11/","text":"Ex. 5.11 Prove that for a smoothing spline the null space of \\(\\bb{K}\\) is spanned by functions linear in \\(X\\) . Soln. 5.11 First recall the definition of \\(\\bm{\\Omega}_N\\) is \\[\\begin{equation} \\{\\bm{\\Omega}_N\\}_{jk} = \\int N''_j(t)N''_k(t)dt.\\non \\end{equation}\\] Since \\(N_1 =1\\) and \\(N_2 = x\\) , both have vanished second order derivative, thus we have \\[\\begin{equation} \\bm{\\Omega}_N = \\begin{pmatrix} 0& 0&\\cdots&0\\\\ 0& 0&\\cdots&0\\\\ \\vdots & \\vdots &\\ddots &\\vdots\\\\ 0& 0&\\cdots& \\int N''_N(t)N''Nk(t)dt \\end{pmatrix}.\\non \\end{equation}\\] Thus, for any \\(x^T=(c_1, c_2x, 0,...,0)\\) where \\(c_1\\) and \\(c_2\\) are constants, it's easy to show that \\[\\begin{equation} \\bm{\\Omega}_N\\bb{N}^{-1}x = \\bb{0}\\non \\end{equation}\\] so that \\[\\begin{equation} \\bb{K}x=\\bb{0}.\\non \\end{equation}\\]","title":"Ex. 5.11"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-12/","text":"Ex. 5.12 Characterize the solution to the following problem, \\[\\begin{equation} \\label{eq:5-12w} \\min_f\\text{RSS}(f,\\lambda) = \\sum_{i=1}^N\\omega_i\\{y_i-f(x_i)\\}^2 + \\lambda\\int \\{f''(t)^2\\}dt, \\end{equation}\\] where the \\(\\omega_i\\ge 0\\) are observation weights. Characterize the solution to the smoothing spline problem (5.9) when the training data have ties in \\(X\\) . Soln. 5.12 Following the same arguments in Ex. 5.7 , the solution to \\(\\eqref{eq:5-12w}\\) is a cubic spline with knots at the unique values of \\(\\{x_i, i=1,...,N\\}\\) and fitted spline can be represented as \\[\\begin{equation} \\hat f(x) = \\sum_{j=1}^NN_j(x_j)\\hat \\theta_j\\non \\end{equation}\\] where \\[\\begin{equation} \\hat \\theta = (\\bb{N}^T\\bb{W}\\bb{N} + \\lambda\\bm{\\Omega}_N)^{-1}\\bb{N}^T\\bb{W}\\bb{y}\\non \\end{equation}\\] and \\(\\bb{W} = \\text{diag}(\\omega_1, ..., \\omega_n)\\) . Suppose we group all the training data into \\(n\\) groups. Each group \\(i\\) contains \\(n_i\\ge 1\\) training data which have the same \\(x_i\\) and let \\(\\bar y_i\\) be their \\(y_i\\) 's average. So the first summand in (5.9) can be rewritten as \\[\\begin{eqnarray} &&\\sum_{i=1}^n\\sum_{j\\in n_i}(y_j-f(x_i))^2\\non\\\\ &=&\\sum_{i=1}^n\\sum_{j\\in n_i}(y_j-\\bar y_i + \\bar y_i - f(x_i))^2\\non\\\\ &=&\\sum_{i=1}^n\\sum_{j\\in n_i}[(y_j-\\bar y_i)^2 + (\\bar y_i - f(x_i))^2 + 2(y_j-\\bar y_i)(\\bar y_i - f(x_i))]\\non \\end{eqnarray}\\] Note that \\[\\begin{equation} \\sum_{i=1}^n\\sum_{j\\in n_i} (\\bar y_i - f(x_i))^2 = \\sum_{i=1}^n n_i(\\bar y_i - f(x_i))^2,\\non \\end{equation}\\] and \\[\\begin{equation} \\sum_{i=1}^n\\sum_{j\\in n_i}(y_j-\\bar y_i)(\\bar y_i - f(x_i)) = 0\\non \\end{equation}\\] and \\(\\sum_{i=1}^n\\sum_{j\\in n_i}(y_j-\\bar y_i)^2\\) is independent of \\(f\\) . So minimizing (5.9) in text is equivalent to minimizing \\[\\begin{equation} \\sum_{i=1}^nn_i(\\bar y_i - f(x_i))^2 + \\lambda\\int{f''(t)}^2dt,\\non \\end{equation}\\] which is treated by the first half of this problem.","title":"Ex. 5.12"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-13/","text":"Ex. 5.13 You have fitted a smoothing spline \\(\\hat f_\\lambda\\) to a sample of \\(N\\) pairs \\((x_i, y_i)\\) . Suppose you augment your original sample with the pair \\(x_0, \\hat f_\\lambda(x_0)\\) , and refit; describe the result. Use this to derive the \\(N\\) -fold cross-validation formula (5.26). Soln. 5.13 Let \\(\\hat f^{(-i)}_\\lambda(x_i)\\) denote the predicted value for the \\(i-\\) th case when \\(\\{x_i, y_i\\}\\) is left out of the data doing the fitting. We claim that \\[\\begin{equation} \\label{eq:5-13leave} \\hat f^{(-i)}_\\lambda(x_i) = \\frac{1}{1-S_\\lambda(i,i)}\\sum_{j\\ne i} S_\\lambda(i,j)y_j. \\end{equation}\\] Starting from \\(\\eqref{eq:5-13leave}\\) , we multiply \\((1-S_\\lambda(i,i))\\) on both sides and move one term from left side to right side, we have \\[\\begin{equation} \\hat f^{(-i)}_\\lambda(x_i) = \\sum_{j\\ne i} S_\\lambda(i,j)y_j + S_\\lambda(i,i)\\hat f^{(-i)}_\\lambda(x_i).\\non \\end{equation}\\] Recall that \\[\\begin{equation} \\hat f_\\lambda(x_i) = \\sum_{j=1}^nS_\\lambda(i,j)y_j\\non, \\end{equation}\\] we have \\[\\begin{equation} \\hat f^{(-i)}_\\lambda(x_i) = \\hat f_\\lambda(x_i) + S_\\lambda(i,i)\\hat f^{(-i)}_\\lambda(x_i) - S_\\lambda(i,i)y_i,\\non \\end{equation}\\] thus \\[\\begin{equation} y_i - \\hat f^{(-i)}_\\lambda(x_i) = \\frac{y_i-\\hat f_\\lambda(x_i)}{1-S_\\lambda(i,i)}.\\non \\end{equation}\\] It remains to prove \\(\\eqref{eq:5-13leave}\\) . Intuitively, any reasonable smoother is constant preserving, which means \\(S_\\lambda \\bb{1} = \\bb{1}\\) . Therefore, the rows of \\(S_\\lambda\\) sum to one. Thus if we want to use the same smoother with the \\(i\\) -th row and column deleted, we must re-normalize the rows to sum to one, that gives \\(\\eqref{eq:5-13leave}\\) . For a rigorous proof, please see Ex. 7.3 (a).","title":"Ex. 5.13"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-14/","text":"Ex. 5.14 Derive the constraints on the \\(\\alpha_j\\) in the thin-plate spline expansion (5.39) to guarantee that the penalty \\(J(f)\\) is finite. How else could one ensure that the penalty was finite? Soln. 5.14 Consider \\[\\begin{equation} f(x,y) = \\beta_0 + \\beta^T(x,y) + \\sum_{j=1}^N\\alpha_jh_j(x,y)\\non \\end{equation}\\] where \\[\\begin{eqnarray} h_j(x,y) &=& [(x-x_j)^2 + (y-y_j)^2]\\log\\left(\\sqrt{(x-x_j)^2 + (y-y_j)^2}\\right)\\non\\\\ &=&\\frac{1}{2}[(x-x_j)^2 + (y-y_j)^2]\\log((x-x_j)^2 + (y-y_j)^2).\\non \\end{eqnarray}\\] Without loss of generality, we will drop the constant and linear term \\(\\beta_0 + \\beta^T(x,y)\\) in \\(f\\) since they vanish after taking second derivatives below. The penalty function \\(J\\) is \\[\\begin{equation} J(f) = \\int\\int_{\\mathbb{R}^2}\\left[\\left(\\frac{\\partial^2f(x,y)}{\\partial x^2}\\right)^2+2\\left(\\frac{\\partial^2f(x,y)}{\\partial x\\partial y}\\right)^2 + \\left(\\frac{\\partial^2f(x,y)}{\\partial y^2}\\right)^2\\right]dxdy.\\non \\end{equation}\\] Next we compute each integrand above. First, denote \\[\\begin{eqnarray} r_{jx} &=& x-x_j\\non\\\\ r_{jy} &=& y-y_j\\non\\\\ r^2_j &=& r^2_{jx} + r^2_{jy}=(x-x_j)^2 + (y-y_j)^2.\\non \\end{eqnarray}\\] Then we have \\[\\begin{eqnarray} \\frac{\\partial f(x,y)}{\\partial x} &=& \\sum_{j=1}^N\\alpha_j[r_{jx}(\\log(r_j^2) + 1)]\\non\\\\ \\frac{\\partial^2 f(x,y)}{\\partial^2 x} &=& \\sum_{j=1}^N\\alpha_j\\left[\\log(r_j^2) + 2\\frac{r_{jx}^2}{r_j^2} + 1\\right]\\non\\\\ \\frac{\\partial^2 f(x,y)}{\\partial x\\partial y} &=& \\sum_{j=1}^N\\alpha_j\\frac{2r_{jx}r_{jy}}{r_j^2}\\non\\\\ \\frac{\\partial^2 f(x,y)}{\\partial^2 y} &=& \\sum_{j=1}^N\\alpha_j\\left[\\log(r_j^2) + 2\\frac{r_{jy}^2}{r_j^2} + 1\\right].\\non \\end{eqnarray}\\] To get penalty \\(J[f]\\) , we calculate the first integrand as \\[\\begin{eqnarray} &&\\left(\\frac{\\partial^2 f(x,y)}{\\partial^2 x}\\right)^2\\non\\\\ &=&\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)^2 + \\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)^2 + \\left(\\sum_{j=1}^N\\alpha_j\\right)^2\\non\\\\ &&+2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)\\non\\\\ &&+2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)\\left(\\sum_{j=1}^N\\alpha_j\\right)\\non\\\\ &&+2\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)\\left(\\sum_{j=1}^N\\alpha_j\\right).\\non \\end{eqnarray}\\] At this point we see that \\(\\sum_{j=1}^N\\alpha_j = 0\\) , otherwise the integral would be infinite. So that above integrand is simplified to \\[\\begin{eqnarray} \\label{eq:5-14a} &&\\left(\\frac{\\partial^2 f(x,y)}{\\partial^2 x}\\right)^2\\non\\\\ &=&\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)^2 + \\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)^2+2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right).\\non \\end{eqnarray}\\] Similarly we have \\[\\begin{eqnarray} \\label{eq:5-14b} \\left(\\frac{\\partial^2 f(x,y)}{\\partial x\\partial y}\\right)^2=\\left(\\sum_{j=1}^N\\alpha_j\\frac{2r_{jx}r_{jy}}{r_j^2}\\right)^2.\\non \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\label{eq:5-14c} &&\\left(\\frac{\\partial^2 f(x,y)}{\\partial^2 y}\\right)^2\\non\\\\ &=&\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)^2 + \\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jy}^2}{r_j^2}\\right)^2+2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jy}^2}{r_j^2}\\right).\\non \\end{eqnarray}\\] Sum them up we get \\[\\begin{eqnarray} &&2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)^2 + \\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)^2+\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jy}^2}{r_j^2}\\right)^2\\non\\\\ && + \\left(\\sum_{j=1}^N\\alpha_j\\frac{2r_{jx}r_{jy}}{r_j^2}\\right)^2\\non\\\\ && + 2\\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2+r_{jy}^2}{r_j^2}\\right).\\non \\end{eqnarray}\\] Note that \\(\\frac{r_{jx}^2+r_{jy}^2}{r_j^2}=1\\) and \\(\\sum_{j=1}^N\\alpha_j=0\\) , the last summand above vanishes and we are left with \\[\\begin{equation} \\left(\\sum_{j=1}^N\\alpha_j\\log(r_j^2)\\right)^2 + \\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jx}^2}{r_j^2}\\right)^2+\\left(\\sum_{j=1}^N2\\alpha_j\\frac{r_{jy}^2}{r_j^2}\\right)^2+ \\left(\\sum_{j=1}^N\\alpha_j\\frac{2r_{jx}r_{jy}}{r_j^2}\\right)^2.\\non \\end{equation}\\] Remark It has been shown that \\(\\sum_{j=1}^N\\alpha_jx_j=0\\) (see, e.g., Thin-Plate Splines ) , however I don't see how to arrive that from here.","title":"Ex. 5.14"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-15/","text":"Ex. 5.15 This exercise derives some of the results quoted in Section 5.8.1. Suppose \\(K(x,y)\\) satisfying the conditions (5.45) and let \\(f(x)\\in \\mathcal{H}_K\\) . Show that (a) \\(\\langle K(\\cdot, x_i), f\\rangle_{\\mathcal{H}_K} = f(x_i)\\) . (b) \\(\\langle K(\\cdot, x_i), K(\\cdot, x_j)\\rangle_{\\mathcal{H}_K} = K(x_i, x_j)\\) . (c) If \\(g(x) = \\sum_{i=1}^N\\alpha_iK(x,x_i)\\) , then \\[\\begin{equation} J(g) = \\sum_{i=1}^N\\sum_{i=1}^NK(x_i, x_j)\\alpha_i\\alpha_j.\\non \\end{equation}\\] Suppose that \\(\\tilde g(x) = g(x) + \\rho(x)\\) , with \\(\\rho(x) \\in \\mathcal{H}_K\\) , and orthogonal in \\(\\mathcal{H}_K\\) to each of \\(K(x, x_i)\\) , i=1,...,N. Show that (d) \\[\\begin{equation} \\sum_{i=1}^NL(y_i, \\tilde g(x_i)) + \\lambda J(\\tilde g) \\ge \\sum_{i=1}^NL(y_i, g(x_i)) + \\lambda J(g)\\non \\end{equation}\\] with equality iff \\(\\rho(x) = 0\\) . Soln. 5.15 (a) Note that by (5.47) in text, the inner product \\(\\mathcal{H}_K\\) is \\[\\begin{equation} \\left\\langle \\sum_{j\\in J}a_j\\phi_i(x), \\sum_{j\\in J}b_j\\phi_j(x) \\right\\rangle_{\\mathcal{H}_K} = \\sum_{j\\in J}\\frac{a_jb_j}{\\lambda_j}.\\non \\end{equation}\\] Therefore, by definition of \\(K\\) we have \\[\\begin{eqnarray} \\langle K(\\cdot, y), f\\rangle_{\\mathcal{H}_K} &=& \\left\\langle \\sum_{i=1}^\\infty (\\gamma_i\\phi_i(x))\\phi_i(y), \\sum_{i=1}^\\infty c_i\\phi_i(x)\\right\\rangle \\non\\\\ &=&\\sum_{i=1}^\\infty \\frac{c_i\\lambda_i\\phi_i(y)}{\\lambda_i}\\non\\\\ &=&f(y).\\non \\end{eqnarray}\\] (b) It follows from (a) by letting \\(f(\\cdot) = K(\\cdot, x_j)\\) . (c) From (b) we have \\[\\begin{eqnarray} J(g) &=& \\left \\langle \\sum_{i=1}^N\\alpha_iK(x, x_i), \\sum_{i=1}^N\\alpha_iK(x, x_i) \\right\\rangle\\non\\\\ &=& \\sum_{i=1}^N\\sum_{i=1}^NK(x_i, x_j)\\alpha_i\\alpha_j.\\non \\end{eqnarray}\\] (d) Since \\(\\rho\\) is orthogonal to each \\(K(x, x_i)\\) for \\(i=1,...,N\\) , we have \\[\\begin{equation} \\lambda J(\\tilde g) = \\lambda J(g) + \\lambda\\|\\rho\\|^2_{\\mathcal{H}_K} \\ge \\lambda J(g).\\non \\end{equation}\\] Moreover, from (a), we have \\[\\begin{eqnarray} \\tilde g(x_i) &=& \\langle K(\\cdot, x_i), \\tilde g \\rangle_{\\mathcal{H}_K}\\non\\\\ &=& \\langle K(\\cdot, x_i), g + \\rho \\rangle_{\\mathcal{H}_K}\\non\\\\ &=& \\langle K(\\cdot, x_i), g \\rangle_{\\mathcal{H}_K},\\non \\end{eqnarray}\\] so that \\[\\begin{equation} L(y_i, \\tilde g(x_i)) = L(y_i, g(x_i)),\\non \\end{equation}\\] that is, the loss only depends on the data space. The proof is now complete. Remark This is the (simple version of) Representer Theorem .","title":"Ex. 5.15"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-16/","text":"Ex. 5.16 Consider the ridge regression problem (5.53), and assume \\(M\\ge N\\) . Assume you have a kernel \\(K\\) that computes the inner product \\(K(x,y) = \\sum_{m=1}^Mh_m(x)h_m(y)\\) . (a) Derive (5.62) on page 171 in the text. How would you compute the matrices \\(\\bb{V}\\) and \\(\\bb{D}_\\gamma\\) , given \\(K\\) ? Hence show that (5.63) is equivalent to (5.53). (b) Show that \\[\\begin{eqnarray} \\hat{\\mathbf{f}} &=& \\bb{H}\\hat\\beta\\non\\\\ &=&\\bb{K}(\\bb{K} + \\lambda\\bb{I})^{-1}\\bb{y},\\non \\end{eqnarray}\\] where \\(\\bb{H}\\) is the \\(N\\times M\\) matrix of evaluations \\(h_m(x_i)\\) , and \\(\\bb{K} = \\bb{H}\\bb{H}^T\\) the \\(N\\times N\\) matrix of inner-product \\(h(x_i)^Th(x_j)\\) . (c) Show that \\[\\begin{eqnarray} \\hat f(x) &=& h(x)^T\\hat{\\bm{\\beta}}\\non\\\\ &=& \\sum_{i=1}^NK(x, x_i)\\hat{\\bm{\\alpha}_i}\\non \\end{eqnarray}\\] and \\(\\hat{\\bm{\\alpha}} = (\\bb{K} + \\lambda \\bb{I})^{-1}\\bb{y}\\) . (d) How would you modify your solution if \\(M < N\\) ? Soln. 5.16 (a) By definition of the kernel \\(K\\) , we have \\[\\begin{equation} K(x, y) = \\sum_{m=1}^Mh_m(x)h_m(y) = \\sum_{i=1}^\\infty \\gamma_i\\phi_i(x)\\phi_i(y).\\non \\end{equation}\\] Multiply each summand above by \\(\\phi_k(x)\\) and calculate \\(\\langle K(x,y), \\phi_k(x) \\rl\\) , \\[\\begin{equation} \\label{eq:5-16a} \\sum_{m=1}^M\\langle h_m(x), \\phi_k(x)\\rl h_m(y) = \\sum_{i=1}^\\infty \\langle \\phi_i(x), \\phi_k(x)\\rl \\phi_i(y). \\end{equation}\\] Since \\(\\{\\phi_i, i=1,...,\\infty\\}\\) are orthogonal, we have \\[\\begin{equation} \\langle \\phi_i(x), \\phi_k(x) \\rl = \\begin{cases} 1 & \\text{ if } i=k,\\\\ 0 & \\text{ otherwise.} \\end{cases}\\non \\end{equation}\\] Thus, \\(\\eqref{eq:5-16a}\\) becomes \\[\\begin{equation} \\sum_{m=1}^M\\langle h_m(x), \\phi_k(x)\\rl h_m(y) = \\gamma_k\\phi_k(y).\\non \\end{equation}\\] Let \\(g_{km} = \\langle h_m(x), \\phi_k(x) \\rl\\) and calculate \\(\\langle K(x,y), \\phi_l(y) \\rl\\) , we get \\[\\begin{eqnarray} \\sum_{m=1}^M g_{km}h_m(y) &=& \\gamma_k\\phi_k(y),\\non\\\\ \\sum_{m=1}^M g_{km} \\langle h_m(y), \\phi_l(y) \\rl &=& \\gamma_k \\langle \\phi_k(y), \\phi_l(y) \\rl ,\\non\\\\ \\sum_{m=1}^M g_{km}g_{lm} &=& \\gamma_k\\delta_{k,l}\\non \\end{eqnarray}\\] where \\(\\delta_{k,l} = 1\\) if \\(k=l\\) and 0 otherwise. Let \\(\\bb{G}_M = \\{g_{nm}\\} \\in \\mathbb{R}^{M\\times N}\\) , we have \\[\\begin{equation} \\bb{G}_M\\bb{G}_M^T = \\text{diag}\\{\\gamma_1,\\gamma_2,...,\\gamma_M\\} = \\bb{D}_\\gamma.\\non \\end{equation}\\] Let \\(\\bb{V}^T = \\bb{D}_\\gamma^{-\\frac{1}{2}}\\bb{G}_M\\) , we have \\[\\begin{equation} \\bb{V}\\bb{V}^T\\bb{G}_M^T = \\bb{G}_M^T\\bb{D}_\\gamma^{-1}\\bb{G}_M = \\bb{I}_N.\\non \\end{equation}\\] Let \\(h(x) = (h_1(x), h_2(x), ..., h_M(x))^T\\) and \\(\\phi(x) = (\\phi_1(x), \\phi_2(x), ..., \\phi_M(x))^T\\) , then the three equations above can be rewritten as \\[\\begin{eqnarray} \\bb{G}_Mh(x) &=& \\bb{D}_\\gamma\\phi(x)\\non\\\\ \\bb{V}\\bb{D}^{-\\frac{1}{2}}_\\gamma\\bb{G}_Mh(x) &=& \\bb{V}\\bb{D}^{-\\frac{1}{2}}_\\gamma\\bb{D}_\\gamma\\phi(x)\\non\\\\ h(x) &=&\\bb{V}\\bb{D}^{\\frac{1}{2}}_\\gamma.\\non \\end{eqnarray}\\] To show that (5.63) is equivalent to (5.53) in the text, we start with (5.63). Let \\(\\beta = (\\beta_1, \\beta_2,...,\\beta_m)^T\\) and \\(c=\\bb{D}_\\gamma^{\\frac{1}{2}}\\bb{V}^T\\beta\\) , \\[\\begin{eqnarray} && \\min_{\\{\\beta_m\\}_1^M}\\sum_{i=1}^N\\left(y_i-\\sum_{m=1}^M\\beta_mh_m(x_i)\\right)^2 + \\lambda \\sum_{m=1}^M\\beta_m^2\\non\\\\ &=&\\min_{\\beta}\\sum_{i=1}^N(y_i-\\beta^Th(x_i))^2 + \\lambda\\beta^T\\beta\\non\\\\ &=&\\min_{\\beta} \\sum_{i=1}^N(y_i-\\beta^T\\bb{V}\\bb{D}^{\\frac{1}{2}}_\\gamma\\phi(x_i))^2 + \\lambda\\beta^T\\beta\\non\\\\ &=&\\min_{c}\\sum_{i=1}^N(y_i-c^T\\phi(x_i))^2 + \\lambda (\\bb{V}\\bb{D}^{\\frac{1}{2}}_\\gamma c)^T \\bb{V}\\bb{D}^{\\frac{1}{2}}_\\gamma c\\non\\\\ &=&\\min_{c}\\sum_{i=1}^N(y_i-c^T\\phi(x_i))^2 + \\lambda c^Tc\\bb{D}_\\gamma^{-1}\\non\\\\ &=&\\min_{\\{c_j\\}_1^\\infty}\\sum_{i=1}^N\\left(y_i-\\sum_{j=1}^\\infty c_j\\phi_j(x_i)\\right)^2 + \\lambda \\sum_{j=1}^\\infty \\frac{c_j^2}{\\gamma_j},\\non \\end{eqnarray}\\] which is (5.53) in the text. (b) Recall that in (a) we have \\[\\begin{equation} \\min_{\\beta}\\sum_{i=1}^N(y_i-\\beta^Th(x_i))^2 + \\lambda\\beta^T\\beta.\\non \\end{equation}\\] Taking derivative w.r.t \\(\\beta\\) and setting it to be zero yields \\[\\begin{equation} -\\bb{H}^T(\\by-\\bb{H}\\hat\\beta) + \\lambda \\hat\\beta = 0.\\non \\end{equation}\\] Thus we have \\[\\begin{equation} \\hat\\beta = (\\bb{H}^T\\bb{H} + \\lambda \\bb{I})^{-1}\\bb{H}^T\\by\\non \\end{equation}\\] and \\[\\begin{equation} \\hat{\\mathbf{f}} = \\bb{H}(\\bb{H}^T\\bb{H} + \\lambda \\bb{I})^{-1}\\bb{H}^T\\by.\\non \\end{equation}\\] By Woodbury matrix identity, we have \\[\\begin{eqnarray} (\\bb{H}^T\\bb{H} + \\lambda \\bb{I})^{-1} &=& \\frac{1}{\\lambda}\\bb{I} - \\frac{1}{\\lambda}\\bb{I}\\bb{H}^T\\left(\\bb{I} + \\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\right)^{-1}\\bb{H}\\cdot\\frac{1}{\\lambda}\\bb{I}\\non. \\end{eqnarray}\\] Therefore, we have \\[\\begin{eqnarray} \\hat{\\mathbf{f}} &=& \\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\by - \\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\left(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T\\right)^{-1}\\bb{H}\\bb{H}^T\\by\\non\\\\ &=&\\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\left[\\bb{I}-(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)^{-1}\\bb{H}\\bb{H}^T\\right]\\by\\non\\\\ &=&\\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\left[(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)^{-1}(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)-(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)^{-1}\\bb{H}\\bb{H}^T\\right]\\by\\non\\\\ &=&\\frac{1}{\\lambda}\\bb{H}\\bb{H}^T\\left[(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)^{-1}\\lambda\\bb{I}\\right]\\by\\non\\\\ &=&\\bb{H}\\bb{H}^T(\\lambda\\bb{I}+\\bb{H}\\bb{H}^T)^{-1}\\by\\non\\\\ &=&\\bb{K}(\\bb{K}+\\lambda\\bb{I})^{-1}\\by.\\non \\end{eqnarray}\\] (c) This is directly derived from (b). (d) The solution remains the same as \\(\\bb{K}+\\lambda\\bb{I}\\) is invertible as long as \\(\\lambda \\neq 0\\) . When \\(\\lambda = 0\\) however, we have \\[\\begin{equation} \\hat{\\mathbf{f}} = \\bb{H}\\hat\\beta = \\bb{H}(\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\by = \\by.\\non \\end{equation}\\]","title":"Ex. 5.16"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-17/","text":"Ex. 5.17 Show how to convert the discrete eigen-decomposition of \\(\\bb{K}\\) in Section 5.8.2 to estimates of the eigenfunctions of \\(K\\) . Soln. 5.17 As the text suggests, given a kernel matrix \\(\\bb{K}\\) , we first calculate its eigen-decomposition \\[\\begin{equation} \\bb{K} = \\bm{\\Phi}\\bb{D}_\\lambda\\bm{\\Phi}^T.\\non \\end{equation}\\] Then \\(i-\\) th row of \\(\\bm{\\Phi}\\) is the estimated vector of basis functions \\(\\phi(x_i)\\) , evaluated at point \\(x_i\\) .","title":"Ex. 5.17"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-18/","text":"Ex. 5.18 The wavelet function \\(\\psi(x)\\) of the symmlet- \\(p\\) wavelet basis has vanishing moments up to order \\(p\\) . Show that this implies that polynomials of order \\(p\\) are represented exactly in \\(V_0\\) , defined on page 176. Soln. 5.18 The expansion of a function \\(f(x)\\) in symmlet- \\(p\\) wavelet basis has the form \\[\\begin{equation} f(x) = \\sum_{m,n}f_{mn}\\psi_{mn}(x)\\non \\end{equation}\\] where \\[\\begin{equation} f_{mn} := \\int f(x)\\psi_{mn}(x)dx.\\non \\end{equation}\\] The constraint on the moments \\[\\begin{equation} \\int \\psi(x)x^ldx = 0, \\ \\ l=0,...,p-1\\non \\end{equation}\\] has important consequences. First it implies \\[\\begin{eqnarray} &&\\int \\psi_{0m}(x)x^ldx = \\int \\psi(x-m)x^ldx = \\int \\psi(y)(y+m)^ldy\\non\\\\ &=&\\sum_{k=0}^l\\frac{l!}{k!(l-k)!}\\int \\psi(y)y^kdy=0, \\ \\ l=0,...,p-1,\\non \\end{eqnarray}\\] which means that the first \\(p-1\\) moments of the unit translates of the mother wavelet function vanish. Second, changing scales gives \\[\\begin{eqnarray} &&\\int \\psi_{10}(x)x^l = \\frac{1}{\\sqrt{2}}\\int \\psi(x/2)x^ldx\\non\\\\ &=&2^{l+1/2}\\int \\psi(y)y^ldy=0,\\ \\ l=0,...,p-1.\\non \\end{eqnarray}\\] It's easy to proceed inductively to show for all \\(m\\) and \\(n\\) that \\[\\begin{equation} \\int \\psi_{nm}(x)x^ldx = 0,\\ \\ l=0,...,p-1.\\non \\end{equation}\\] This means that every symmlet- \\(p\\) wavelet basis function is orthogonal to all polynomials of degree less than \\(p\\) . Therefore we see the polynomials of degrees less than \\(p\\) can be represented exactly by the finite linear combination of the scaling functions \\(\\phi(x-k)\\) , which spans the reference space \\(V_0\\) .","title":"Ex. 5.18"},{"location":"ESL-Solution/5-Basis%20Expansions%20and%20Regularization/ex5-19/","text":"Ex. 5.19 Show that the Haar wavelet transform of a signal of length \\(N=2^J\\) can be computed in \\(O(N)\\) computations. Soln. 5.19 We refer to Section 2.1 in Wavelet Methods in Statistics with R for readers interested in the pyramidal technique for Haar wavelet transform. Here we give a high level idea on why the time complexity is \\(O(N)\\) . Given a discrete sequence of data \\(y=(y_1,y_2,...,y_N)\\) , without loss of generality, we assume that \\(N=2^J\\) for some integer \\(J\\ge 0\\) , the pyramidal technique consists of \\(J\\) steps, in each step, we calculate \\[\\begin{eqnarray} d_{j, k} = \\frac{1}{\\sqrt {2}} ( c_{j+1, 2k} - c_{j+1, 2k-1} )\\non\\\\ c_{j, k} = \\frac{1}{\\sqrt {2}} ( c_{j+1, 2k} + c_{j+1, 2k-1} )\\non \\end{eqnarray}\\] for \\(j=J-1,...,0\\) and \\(k=1,...,2^{j}\\) . Let \\(c_{J, k} = y_k\\) for initialization. At \\(j\\) -th step, the time complexity is \\(2 * 2^j=2^{j+1}\\) , therefore the total time complexity is \\[\\begin{equation} \\sum_{j=0}^{J-1}2^{j+1} = 2(2^J-1) = 2(N-1).\\non \\end{equation}\\] Thus the complexity is \\(O(N)\\) .","title":"Ex. 5.19"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-01/","text":"Ex. 6.1 Show that the Nadaraya-Watson kernel smooth with fixed metric bandwidth \\(\\lambda\\) and a Gaussian kernel is differentiable. What can be said for the Epanechnikov kernel? What can be said for the Epanechnikov kernel with adaptive nearest-neighbor bandwidth \\(\\lambda(x_0)\\) ? Soln. 6.1 By definition of the Nadaraya-Watson kernel-weighted average, we have \\[\\begin{equation} \\hat f(x_0) = \\frac{\\sum_{i=1}^NK_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^NK_\\lambda(x_0, x_i)}.\\non \\end{equation}\\] With Gaussian kernel \\[\\begin{equation} K_\\lambda(x_0, x) = \\frac{1}{\\sqrt {2\\pi}\\lambda}\\exp \\left( -\\frac{(x-x_0)^2}{2\\lambda^2} \\right), \\non \\end{equation}\\] we know \\(K_\\lambda(x_0, x)\\neq 0\\) for all \\(x_0\\) and \\(x\\) in \\(\\mathbb{R}\\) , and is differentiable in \\(x_0\\) , so that the Nadaraya-Watson kernel-weighted average is also differentiable in \\(x_0\\) . With Epanechnikov kernel \\[\\begin{equation} K_\\lambda(x_0, x) = D\\left(\\frac{|x-x_0|}{\\lambda}\\right),\\non \\end{equation}\\] with \\[\\begin{equation} D(t) = \\begin{cases} \\frac{3}{4}(1-t^2), & \\text{ if } |t|\\le 1 \\\\ 0, & \\text{ otherwise.} \\end{cases}\\non \\end{equation}\\] Note that \\(D(t)\\) is continuous but not differentiable at \\(t=1\\) , thus the kernel-weighted average holds the same property. When the bandwidth is adaptive nearest-neighbor \\(\\lambda(x_0)\\) , \\(\\hat f(x_0)\\) is still not differential by the same arguments when \\(\\frac{|x-x_0|}{\\lambda(x_0)}\\) approaches 1 from different directions.","title":"Ex. 6.1"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-02/","text":"Ex. 6.2 Show that \\(\\sum_{i=1}^N(x_i-x_0)l_i(x_0) = 0\\) for local linear regression. Define \\(b_j(x_0) = \\sum_{i=1}^N(x_i-x_0)^jl_i(x_0)\\) . Show that \\(b_0(x_0) = 1\\) for local polynomial regression of any degree (including local constants). Show that \\(b_j(x_0)=0\\) for all \\(j\\in \\{1,2,...,k\\}\\) for local polynomial regression of degree \\(k\\) . What are the implications of this on the bias? Soln. 6.2 Define the vector-valued function \\(b(x)^T = (1,x,x^2,...,x^k)\\) for \\(k\\ge 0\\) . Let \\(\\bb{B}\\) be the \\(N\\times (k+1)\\) regression matrix with \\(i\\) th row \\(b(x_i)^T\\) , and \\(\\bb{W}(x_0)\\) the \\(N\\times N\\) diagonal matrix with \\(i\\) th diagonal element \\(K_\\lambda(x_0, x_i)\\) . Then we have \\[\\begin{equation} \\label{eq:6-3bwb} b(x_0)^T = b(x_0)^T(\\bb{B}^T\\bb{W}(x_0)\\bb{B})^{-1}\\bb{B}^T\\bb{W}(x_0)\\bb{B}. \\end{equation}\\] Note the definition of \\(l_i(x_0)\\) in (6.9) in text, from \\(\\eqref{eq:6-3bwb}\\) , we have \\[\\begin{eqnarray} 1 &=& b(x_0)^T(\\bb{B}^T\\bb{W}(x_0)\\bb{B})^{-1}\\bb{B}^T\\bb{W}(x_0)\\bb{1} = \\sum_{i=1}^Nl_i(x_0)\\non\\\\ x_0 &=& b(x_0)^T(\\bb{B}^T\\bb{W}(x_0)\\bb{B})^{-1}\\bb{B}^T\\bb{W}(x_0)\\bb{B}_2 = \\sum_{i=1}^Nl_i(x_0)x_i\\non\\\\ \\cdots\\non\\\\ x_0^k &=&b(x_0)^T(\\bb{B}^T\\bb{W}(x_0)\\bb{B})^{-1}\\bb{B}^T\\bb{W}(x_0)\\bb{B}_{k+1} = \\sum_{i=1}^Nl_i(x_0)x_i^k\\non \\end{eqnarray}\\] where \\(\\bb{B}_i\\) is the \\(i\\) th column of \\(\\bb{B}\\) (note that \\(\\bb{B}_1=\\bb{1}\\) ). Therefore we have \\(b_0(x_0) = \\sum_{i=1}^Nl_i(x_0) = 1\\) and \\[\\begin{equation} b_1(x_0) = \\sum_{i=1}^N(x_i-x_0)l_i(x_0) = \\sum_{i=1}^Nl_i(x_0)x_i - x_0\\sum_{i=1}^Nl_i(x_0) = x_0 - x_0\\cdot 1 = 0.\\non \\end{equation}\\] For \\(j\\ge 2\\) , we have \\[\\begin{eqnarray} b_j(x_0) &=& \\sum_{i=1}^N(x_i-x_0)^jl_i(x_0)\\non\\\\ &=&\\sum_{i=1}^N\\left(\\sum_{b=0}^jC_j^b(-1)^bx_i^{j-b}x_0^b\\right)l_i(x_0)\\non\\\\ &=&\\sum_{b=0}^jC_j^b(-1)^bx_0^b\\left(\\sum_{i=1}^Nl_i(x_0)x_i^{j-b}\\right)\\non\\\\ &=&\\sum_{b=0}^jC_j^b(-1)^bx_0^bx_0^{j-b}\\non\\\\ &=&\\sum_{b=0}^jC_j^b(-1)^bx_0^j\\non\\\\ &=&(1-1)^jx_0^j\\non\\\\ &=& 0.\\non \\end{eqnarray}\\] By Taylor expansion we have \\[\\begin{eqnarray} E[\\hat f(x_0)] - f(x_0) &=& \\sum_{i=1}^Nl_i(x_0)f(x_i) - f(x_0)\\non\\\\ &=& f(x_0)\\sum_{i=1}^Nl_i(x_0) - f(x_0) + f'(x_0)\\sum_{i=1}^N(x_i-x_0)l_i(x_0)\\non\\\\ && + \\frac{f''(x_0)}{2}\\sum_{i=1}^N(x_i-x_0)^2l_i(x_0)\\non\\\\ && + ...\\non\\\\ && + (-1)^k\\frac{f^{(k)}}{k!}\\sum_{i=1}^N(x_i-x_0)^kl_i(x_0)\\non\\\\ && + R\\non\\\\ &=&R,\\non \\end{eqnarray}\\] where the remainder term \\(R\\) involves \\((k+1)\\) th and higher-order derivatives of \\(f\\) , on which the bias only depends.","title":"Ex. 6.2"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-03/","text":"Ex. 6.3 Show that \\(\\|l(x)\\|\\) (Section 6.1.2) increases with the degree of the local polynomial. Soln. 6.3 Let's first introduce notations. Define the vector-valued function \\(b(x)^T = (1,x,x^2,...,x^d)\\) for \\(d\\ge 1\\) . Let \\(\\bb{B}\\) be the \\(N\\times (d+1)\\) regression matrix with \\(i\\) th row \\(b(x_i)^T\\) , \\[\\begin{equation} \\label{eq:6-3B} \\bb{B} = \\begin{pmatrix} 1 & x_{1} & \\cdots & x^d_1 \\\\ 1 & x_{2} & \\cdots & x_{2}^d \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N} & \\cdots & x_{N}^d \\end{pmatrix} = \\begin{pmatrix} b(x_1)^T\\\\ b(x_2)^T\\\\ \\vdots\\\\ b(x_N)^T \\end{pmatrix}\\in \\mathbb{R}^{N\\times (d+1)}\\non \\end{equation}\\] and \\[\\begin{equation} \\bb{B}^T = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\\\ x_{1} & x_{2} & \\cdots & x_{N} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{1}^d & x_{2}^d & \\cdots & x_{N}^d \\end{pmatrix} = \\begin{pmatrix} b(x_1) & b(x_2) & \\cdots & b(x_N) \\end{pmatrix}\\in \\mathbb{R}^{(d+1)\\times N}.\\non \\end{equation}\\] Let \\(\\bb{W}(x)\\) the \\(N\\times N\\) diagonal matrix with \\(i\\) th diagonal element \\(K_\\lambda(x, x_i)\\) , that is, \\[\\begin{equation} \\bW(x) = \\begin{pmatrix} K_\\lambda(x,x_1) & 0 & \\cdots & 0\\\\ 0 & K_\\lambda(x,x_2) & \\cdots & 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 0 & 0 & \\cdots & K_\\lambda(x,x_N) \\end{pmatrix}\\in \\mathbb{R}^{N\\times N}.\\non \\end{equation}\\] Note that \\(\\bW(x) = \\bW^T(x)\\) . By definition of \\(l(x)\\) (see, e.g., (6.9) in the text), we have \\[\\begin{eqnarray} l(x_0)^T = b(x_0)^T(\\bB^T\\bW(x_0)\\bB)^{-1}\\bB^T\\bW(x_0).\\non \\end{eqnarray}\\] Denote \\(b=b(x_0)\\) and \\(\\bW = \\bW(x_0)\\) to simplify the notations from now on, we have \\[\\begin{eqnarray} \\|l(x_0)\\|^2 &=& l(x_0)^Tl(x_0)\\non\\\\ &=& b^T(\\bB^T\\bW\\bB)^{-1}\\bB^T\\bW\\bW^T\\bB(\\bB^T\\bW\\bB)^{-1} b.\\label{eq:6-3a} \\end{eqnarray}\\] We need to show \\(\\|l(x_0)\\|^2\\) is increasing in \\(d\\) . The expression involves with the weighted kernel matrix \\(\\bW\\) , however, it turns out \\(\\|l(x_0)\\|^2\\) does not depend on \\(\\bW\\) . Note that we could plug \\(\\bb{I} = \\bB\\bB^T(\\bB\\bB^T)^{-1} = (\\bB\\bB^T)^{-1}\\bB\\bB^T\\) between \\(\\bW\\) and \\(\\bW^T\\) in \\(\\eqref{eq:6-3a}\\) , we obtain \\[\\begin{eqnarray} &&\\|l(x_0)\\|^2\\non\\\\ &=&b^T(\\bB^T\\bW\\bB)^{-1}\\bB^T\\bW\\bB\\bB^T(\\bB\\bB^T)^{-1}(\\bB\\bB^T)^{-1}\\bB\\bB^T\\bW^T\\bB(\\bB^T\\bW\\bB)^{-1}b\\non\\\\ &=&b^T\\bB^T(\\bB\\bB^T)^{-1}(\\bB\\bB^T)^{-1}\\bB b,\\non \\end{eqnarray}\\] therefore we see \\(\\|l(x_0)\\|^2\\) is independent of \\(\\bW\\) . So we can take \\(\\bW = \\bb{I}\\) in \\(\\eqref{eq:6-3a}\\) , which gives \\[\\begin{equation} \\label{eq:6-3c} \\|l(x_0)\\|^2 = b^T(\\bB^T\\bB)^{-1}b. \\end{equation}\\] Now consider the case for \\(d+1\\) , we denote \\[\\begin{equation} \\hat\\bB = \\begin{pmatrix} 1 & x_{1} & \\cdots & x^d_1 & x_1^{d+1} \\\\ 1 & x_{2} & \\cdots & x_{2}^d & x_2^{d+1}\\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 1 & x_{N} & \\cdots & x_{N}^d & x_N^{d+1} \\end{pmatrix} = \\begin{pmatrix} \\bB & c \\end{pmatrix} \\in \\mathbb{R}^{N\\times (d+2)}\\non \\end{equation}\\] where \\[\\begin{equation} c = \\begin{pmatrix} x_1^{d+1}\\\\ x_2^{d+1}\\\\ \\vdots\\\\ x_N^{d+1} \\end{pmatrix}\\in \\mathbb{R}^{N\\times 1}.\\non \\end{equation}\\] Similarly, denote \\(\\hat b^T = (b^T, x_0^{d+1}) = (1,x_0,x_0^2,...,x_0^d, x_0^{d+1})\\in \\mathbb{R}^{1\\times (d+2)}\\) . In that case, \\(\\eqref{eq:6-3c}\\) becomes \\[\\begin{equation} \\label{eq:6-3d} \\|\\hat l(x_0)\\|^2 = \\hat b^T(\\hat \\bB^T \\hat \\bB)^{-1}\\hat b. \\end{equation}\\] Further, we have \\[\\begin{eqnarray} \\hat\\bB^T\\hat\\bB = \\begin{pmatrix} \\bB^T \\\\ c^T \\end{pmatrix} \\begin{pmatrix} \\bB & c \\end{pmatrix} = \\begin{pmatrix} \\bB^T\\bB & \\bB^Tc\\\\ c^T\\bB & c^Tc \\end{pmatrix} \\in \\mathbb{R}^{(d+2)\\times (d+2)}.\\non \\end{eqnarray}\\] Note that \\(c^Tc\\in \\mathbb{R}^1\\) is a scalar. Recall the formula for block matrix inverse, (e.g., Schur complement ), we have \\[\\begin{eqnarray} &&(\\hat \\bB^T \\hat \\bB)^{-1}\\non\\\\ &=& \\begin{pmatrix} (\\bB^T\\bB)^{-1} + \\frac{1}{k}(\\bB^T\\bB)^{-1}\\bB^Tcc^T\\bB(\\bB^T\\bB)^{-1} & -\\frac{1}{k}(\\bB^T\\bB)^{-1}\\bB^Tc\\\\ -\\frac{1}{k}c^T\\bB(\\bB^T\\bB)^{-1} & \\frac{1}{k} \\end{pmatrix} \\non \\end{eqnarray}\\] where \\[\\begin{equation} \\label{eq:6-3k} k = c^Tc - c^T\\bB(\\bB^T\\bB)^{-1}\\bB^Tc. \\end{equation}\\] Denote \\(\\beta = (\\bB^T\\bB)^{-1}\\bB^Tc\\in\\mathbb{R}^{(d+2)\\times 1}\\) , plug \\((\\hat \\bB^T \\hat \\bB)^{-1}\\) into \\(\\eqref{eq:6-3d}\\) , we get \\[\\begin{eqnarray} &&\\|\\hat l(x_0)\\|^2 \\non\\\\ &=& \\begin{pmatrix} b^T & x_0^{d+1} \\end{pmatrix} \\begin{pmatrix} (\\bB^T\\bB)^{-1} + \\frac{1}{k}\\beta\\beta^T & -\\frac{1}{k}\\beta\\\\ -\\frac{1}{k}\\beta^T & \\frac{1}{k} \\end{pmatrix} \\begin{pmatrix} b\\\\ x_0^{d+1} \\end{pmatrix}\\non\\\\ &=&b^T(\\bB^T\\bB)^{-1}b + \\frac{1}{k}\\left[b^T\\beta\\beta^Tb - x_0^{d+1}\\beta^Tb - x_0^{d+1}b^T\\beta+(x_0^{d+1})^2\\right]\\non\\\\ &=&b^T(\\bB^T\\bB)^{-1}b + \\frac{1}{k}\\left(x_0^{d+1}-b^T\\beta\\right)^2\\ \\ \\text{ (note } b^T\\beta \\in \\mathbb{R} )\\non\\\\ &=&\\|l(x_0)\\|^2+ \\frac{1}{k}\\left(x_0^{d+1}-b^T\\beta\\right)^2.\\non \\end{eqnarray}\\] Therefore, it suffices to show that \\(k > 0\\) for \\(k\\) defined in \\(\\eqref{eq:6-3k}\\) . To do that, we only need to show \\[\\begin{equation} \\label{eq:6-3e} \\bB(\\bB^T\\bB)^{-1}\\bB^T \\preceq \\bb{I}_N. \\end{equation}\\] Consider the QR decomposition of \\(\\bB\\) \\[\\begin{equation} \\bB = \\bb{Q}\\bb{R}\\non \\end{equation}\\] where \\(\\bb{Q}\\) is an \\(N\\times (d+1)\\) orthogonal matrix, \\(\\bb{Q}^T\\bb{Q} = \\bb{I}_N\\) , and \\(\\bb{R}\\in\\mathbb{R}^{(d+1)\\times (d+1)}\\) is an upper triangular matrix. Then \\[\\begin{equation} \\bB(\\bB^T\\bB)^{-1}\\bB^T = \\bb{Q}\\bb{R}(\\bb{R}^T\\bb{R})^{-1}\\bb{R}^T\\bb{Q}^T = \\bb{Q}\\bb{Q}^T.\\non \\end{equation}\\] Let \\((\\bb{Q} \\ \\bb{Q}_1)\\) be an \\(N\\times N\\) orthogonal matrix, we have \\[\\begin{equation} \\bb{I}_N = \\begin{pmatrix} \\bb{Q} & \\bb{Q}_1 \\end{pmatrix} \\begin{pmatrix} \\bb{Q}^T\\\\ \\bb{Q}_1^T \\end{pmatrix} = \\bb{Q}\\bb{Q}^T + \\bb{Q}_1\\bb{Q}_1^T.\\non \\end{equation}\\] The result \\(\\eqref{eq:6-3e}\\) follows by noting \\(\\bb{Q}_1\\bb{Q}_1^T\\) is positive semi-definite. The proof is now complete.","title":"Ex. 6.3"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-04/","text":"Ex. 6.4 Suppose that the \\(p\\) predictors \\(X\\) arise from sampling relatively smooth analog curves at \\(p\\) uniformly spaced abscissa values. Denote by \\(\\text{Cov}(X|Y) = \\bm{\\Sigma}\\) the conditional covariance matrix of the predictors, and assume this does not change much with \\(Y\\) . Discuss the nature of Mahalanobis choice \\(\\bb{A} = \\bm{\\Sigma}^{-1}\\) for the metric in (6.14). How does this compare with \\(\\bb{A} = \\bb{I}\\) ? How might you construct a kernel \\(\\bb{A}\\) that (a) downweights high-frequency components in the distance metric; (b) ignores them completely? Soln. 6.4 If \\(\\bb{A} = \\bb{I}\\) , then Mahalanobis distance \\(d = \\sqrt{(x-x_0)^T\\bm{\\Sigma}^{-1}(x-x_0)}\\) reduces to Euclidean distance. We first standardize each variable to unit standard deviation \\[\\begin{equation} x'_i = \\frac{x_i- E[x_i]}{\\sqrt{\\text{Var}(x_i)}}.\\non \\end{equation}\\] Then we have \\[\\begin{eqnarray} \\text{Cov}(x'_i, x'_j) &=& E[x'_ix'_j]\\non\\\\ &=&\\frac{E[(x_i-E[x_i])(x_j-E[x_j])]}{\\sqrt{\\text{Var}(x_i)}\\sqrt{\\text{Var}(x_j)}}\\non\\\\ &=&\\frac{\\Sigma_{ij}}{\\sqrt{\\text{Var}(x_i)}\\sqrt{\\text{Var}(x_j)}}\\non\\\\ &=&\\rho(x_i, x_j).\\non \\end{eqnarray}\\] We see that after standardizing variables the new covariance matrix is exactly the correlation matrix. (a) In order to downweights high-frequency components, we can decrease \\(\\rho(x_i, x_j)\\) ; (b) In order to ignore them completely, we can set \\(\\rho(x_i, x_j)\\) to be zero.","title":"Ex. 6.4"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-05/","text":"Ex. 6.5 Show that fitting a locally constant multinomial logit model of the form (6.19) amounts to smoothing the binary response indicators for each class separately using a Nadaraya-Watson kernel smoother with kernel weights \\(K_\\lambda(x_0, x_i)\\) . Soln. 6.5 If we smooth binary response indicators for each class separately using a Nadaraya-Watson kernel smoother, recall (6.2) in the text, for any class \\(j\\in \\{1,...,K\\}\\) , \\(y_i=1\\) if and only if \\(i\\in G_j\\) where \\(G_j\\) is the set of indices that belong to class \\(j\\) . Then we have \\[\\begin{equation} \\text{P}(G=j|X=x_0) = \\frac{\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)}{\\sum_{i=1}^NK_\\lambda(x_0, x_i)}\\propto \\sum_{i\\in G_j} K_\\lambda(x_0, x_i).\\non \\end{equation}\\] That means for \\(x_0\\) , we classify it to class \\(j\\) that maximizes \\(\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)\\) . On the other hand, consider the local multinomial logit model (6.19) in the text. From (6.20) in the text, \\[\\begin{equation} \\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{e^{\\hat\\beta_{j0}(x_0)}}{1+\\sum_{k=1}^{J-1}e^{\\hat\\beta_{k0}(x_0)}}\\non \\end{equation}\\] we know that we classify \\(x_0\\) to class \\(j\\) that maximizes \\(\\hat\\beta_{j0}\\) for each class \\(j\\) . It thus suffices to show that \\(\\hat\\beta_{j0}\\) is a non-decreasing function of \\(\\sum_{i\\in G_j} K_\\lambda(x_0, x_i)\\) . Let \\(\\beta\\) denote the parameter set \\(\\{\\beta_{k0}, \\beta_k, k=1,...,K-1\\}\\) . We code the class from \\(1\\) to \\(K\\) so that the log-likelihood \\(l(\\beta, x_0)\\) can be rewritten as (see, e.g., Ex. 4.4 ) \\[\\begin{eqnarray} \\label{eq:6-5a} l(\\beta, x_0) &=&\\sum_{i=1}^NK_\\lambda(x_0, x_i)\\Bigg\\{\\sum_{k=1}^{K-1}\\bb{1}(y_i=k)\\left[\\beta_{k0}(x_0) + \\beta_{k}(x_0)^T(x_i-x_0)\\right]\\non\\\\ &&\\ \\ -\\log\\Big(1+\\sum_{l=1}^{K-1}e^{\\beta_{l0}(x_0) + \\beta_{l}^T(x_0)(x_i-x_0)}\\Big)\\Bigg\\}.\\non \\end{eqnarray}\\] To maximize the log-likelihood, we need to set its derivatives to zero and then can solve the equations to find \\(\\hat\\beta_{k0}\\) . These equations, for \\(j=1,...,K-1\\) , are \\[\\begin{eqnarray} \\label{eq:6-5b} \\frac{\\partial l(\\beta, x_0)}{\\partial \\beta_{j0}} &=& \\sum_{i=1}^N K_\\lambda(x_0, x_i) \\left(\\bb{1}(y_i=j) - \\frac{e^{\\beta_{j0} + \\beta_{j}^T(x_i-x_0)}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^T(x_i-x_0)}}\\right)\\non\\\\ &=& \\sum_{i\\in G_j} K_\\lambda(x_0, x_i) -e^{\\beta_{j0}}\\cdot\\sum_{i=1}^NK_\\lambda(x_0, x_i)\\frac{e^{\\beta_j^T(x_i-x_0)}}{1+\\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^T(x_i-x_0)}}\\non\\\\ &=&0.\\non \\end{eqnarray}\\] Therefore we know that \\[\\begin{equation} \\exp(\\hat\\beta_{j0}) \\propto \\sum_{i\\in G_j} K_\\lambda(x_0, x_i).\\non \\end{equation}\\] The proof is now complete.","title":"Ex. 6.5"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-06/","text":"Ex. 6.6 Suppose that all you have is software for fitting local regression, but you can specify exactly which monomials are included in the fit. How could you use this software to fit a varying-coefficient model in some of the variables? Soln. 6.6 This exercise relates to Section 6.4.2 Structured Regression Functions in the text. Consider a varying-coefficient model formulated as follows. We divide \\(p\\) predictors in \\(X\\) into a set \\((X_1, X_2,...,X_q)\\) with \\(q < p\\) , and the remainder of the variables are denoted by a single vector \\(Z\\) . Assume the linear model \\[\\begin{equation} \\label{eq:6-6a} f(X) = \\alpha(Z) + \\beta_1(Z)X_1 + \\cdots + \\beta_1(Z)X_q.\\non \\end{equation}\\] Note that the coefficients \\(\\beta\\) can vary with \\(Z\\) . Assume that all but \\(j\\) -th term \\(\\beta_j(Z)\\) is known, we can estimate \\(\\beta_j(Z)\\) by a local regression \\[\\begin{equation} \\min_{\\beta_j(z_0)}\\sum_{i=1}^NK_\\lambda(z_0, z_i)(y_i-\\alpha(z_0) - x_{i1}\\beta_1(z_0) - \\cdots - x_{qi}\\beta_q(z_0))^2.\\non \\end{equation}\\] This is done for each \\(\\beta_j\\) in turn, repeatedly, until convergence. At each step, only a one-dimensional local regression is needed.","title":"Ex. 6.6"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-07/","text":"Ex. 6.7 Derive an expression for the leave-one-out cross-validated residual sum-of-squares for local polynomial regression. Soln. 6.7 Note that local regression smoothers are linear estimators, and we can write \\[\\begin{equation} \\hat{\\mathbf{f}} = \\bb{S}_\\lambda \\bb{y} \\non \\end{equation}\\] where \\(\\{\\bb{S}_\\lambda\\}_{ij} = l_i(x_j)\\) for \\(l_i(x)\\) defined by (6.8) in the text. Then by Ex. 7.3 we know \\[\\begin{equation} y_i - \\hat f^{-i}(x_i) = \\frac{y_i - \\hat f(x_i)}{1-\\{\\bb{S}_\\lambda\\}_{ii}}.\\non \\end{equation}\\]","title":"Ex. 6.7"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-08/","text":"Ex. 6.8 Suppose that for continuous response \\(Y\\) and predictor \\(X\\) , we model the joint density of \\(X, Y\\) using a multivariate Gaussian kernel estimator. Note that the kernel in this case would be the product kernel \\(\\phi_\\lambda(X)\\phi_\\lambda(Y)\\) . Show that the conditional mean \\(E[Y|X]\\) derived from this estimate is a Nadaraya-Watson estimator. Extend this result to classification by providing a suitable kernel for the estimation of the joint distribution of a continuous \\(X\\) and discrete \\(Y\\) . Soln. 6.8 By definition we get \\[\\begin{equation} E[Y|X] = \\int y f(y|x)dy = \\frac{\\int y f(x,y)dy}{f(x)}.\\non \\end{equation}\\] The estimates give (see (6.23) in the text) \\[\\begin{eqnarray} \\hat f(x,y) &=& \\frac{1}{N}\\sum_{i=1}^N\\phi_\\lambda(x-x_i)\\phi_\\lambda(y-y_i)\\non\\\\ \\hat f(x) &=& \\frac{1}{N}\\sum_{i=1}^N\\phi_\\lambda(x-x_i).\\non \\end{eqnarray}\\] Thus we have \\[\\begin{eqnarray} E[Y|X] &=& \\frac{\\int y \\frac{1}{N}\\sum_{i=1}^N\\phi_\\lambda(x-x_i)\\phi_\\lambda(y-y_i) dy}{\\frac{1}{N}\\sum_{i=1}^N\\phi_\\lambda(x-x_i)}\\non\\\\ &=&\\frac{\\sum_{i=1}^N\\phi_\\lambda(x-x_i)\\int y\\phi_\\lambda(y-y_i)dy}{\\sum_{i=1}^N\\phi_\\lambda(x-x_i)}\\non\\\\ &=&\\frac{\\sum_{i=1}^N\\phi_\\lambda(x-x_i)y_i}{\\sum_{i=1}^N\\phi_\\lambda(x-x_i)},\\label{eq:6-8a} \\end{eqnarray}\\] where the last equations follows from \\[\\begin{equation} \\int y\\phi_\\lambda(y-y_i)dy = y_i.\\non \\end{equation}\\] Now consider the case when \\(Y\\) is discrete. Assume that \\(Y\\) takes values in the set \\(J\\subset Z=\\{\\cdots, -1, 0, 1, \\cdots\\}\\) . If we choose a naive frequency estimate for \\(Y\\) , then we have \\[\\begin{eqnarray} \\hat f(x,i) &=& \\hat f(i) \\cdot \\hat f(x|i)\\non\\\\ &=&\\frac{N_i}{N}\\cdot\\frac{1}{N_i}\\sum_{l\\in C(i)}\\phi_\\lambda(x-x_l)\\non\\\\ &=&\\frac{1}{N}\\sum_{l\\in C(i)}\\phi_\\lambda(x-x_l)\\non\\\\ \\hat f(x) &=& \\frac{1}{N}\\sum_{i=1}^N\\phi_\\lambda(x-x_i),\\non \\end{eqnarray}\\] where \\(C(i)\\) is the set of \\(X\\) 's such that the corresponding \\(Y\\) 's are in category \\(i\\) and \\(N_i\\) is the size of \\(C(i)\\) . Then it's easy to verify that \\(\\eqref{eq:6-8a}\\) holds. Such estimate can be viewed as the combined kernel for the continuous component with the frequency for the discrete. We can further improve it by a smoothing over the estimate with respect to the discrete component \\(Y\\) using a discrete window weight function (see, e.g., Nonparametric estimation of joint discrete-continuous probability densities with applications ).","title":"Ex. 6.8"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-09/","text":"Ex. 6.9 Explore the differences between the naive Bayes model (6.27) and a generalized additive logistic regression model, in terms of (a) model assumptions and (b) estimation. If all the variables \\(X_k\\) are discrete, what can you say about the corresponding GAM? Soln. 6.9 Naive Bayes model is a generative classifier , which models the joint distribution \\((X, Y)\\) and then predicting the posterior probability \\(P(y|x)\\) with assumptions on \\(P(X)\\) . Logistic regression is a discriminative classifier , which directly models the posterior probability \\(P(y|x)\\) . The naive Bayes model assumes that given a class \\(G=j\\) , the features \\(X_k\\) are independent, while GAM only assumes the additivity of the model. The difference in the assumption leads to the difference in parameter estimation, though the two models have almost the same form. The additional assumption for naive Bayes dramatically simplify the estimation: The individual class-conditional marginal densities can be estimated separately using 1-dimensional kernel density estimates. When all the variables \\(X_k\\) are discrete, then appropriate histogram estimates can be used. On the other hand, parameters are usually estimated by Newton-Raphson methods or backfitting algorithms (e.g., Algorithm 9.1 - 9.2 in the text) for GAM. When all the variables \\(X_k\\) are discrete (e.g., survey response data), it reduces to the so-called linear multinomial choice model and parameters can be fit by maximizing the conditional log-likelihood ( Discrete choice analysis: theory and application to travel demand ).","title":"Ex. 6.9"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-10/","text":"Ex. 6.10 Suppose we have \\(N\\) samples generated from the model \\(y_i=f(x_i)+\\epsilon_i\\) , with \\(\\epsilon_i\\) independent and identically distributed with mean zero and variance \\(\\sigma^2\\) , the \\(x_i\\) assumed fixed (non random). We estimate \\(f\\) using a linear smoother (local regression, smoothing spline, etc.) with smoothing parameter \\(\\lambda\\) . Thus the vector of fitted value is given by \\(\\boldsymbol{\\hat f} = \\bm{S}_\\lambda \\bb{y}\\) . Consider the in-sample prediction error \\[\\begin{equation} \\text{PE}(\\lambda) = E \\frac{1}{N} \\sum_{i=1}^N(y_i^\\ast-\\hat f_\\lambda(x_i))^2\\non \\end{equation}\\] for predicting new responses at the \\(N\\) input values. Show that the average squared residual on the training data, ASR( \\(\\lambda\\) ), is a biased estimate (optimistic) for PE( \\(\\lambda\\) ), while \\[\\begin{equation} C_\\lambda = \\text{ASR}(\\lambda) + \\frac{2\\sigma^2}{N}\\text{trace}(\\bb{S}_\\lambda)\\non \\end{equation}\\] is unbiased. Soln. 6.10 The proof follows directly from Ex. 7.4 and Ex. 7.5 for general linear smoother. Specifically, by Ex. 7.4 , we know \\[\\begin{equation} \\text{PE}(\\lambda) = \\text{ASR}(\\lambda) + \\frac{2}{N}\\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i)\\non \\end{equation}\\] and from Ex. 7.5 we have \\[\\begin{equation} \\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) = \\text{trace}(\\bb{S}_\\lambda)\\sigma^2.\\non \\end{equation}\\] Then the proof is straightforward.","title":"Ex. 6.10"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-11/","text":"Ex. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood is maximized at \\(+\\infty\\) , and describe how. Soln. 6.11 Recall that \\[\\begin{equation} f(x) = \\sum_{m=1}^M\\alpha_m\\phi(x;\\mu_m,\\bm{\\Sigma}_m)\\non \\end{equation}\\] with \\(\\sum_{m=1}^M\\alpha_m=1\\) and \\[\\begin{equation} \\phi(x;\\mu_m,\\bm{\\Sigma}_m) = (2\\pi)^{-\\frac{d}{2}}\\text{det}(\\bm{\\Sigma})^{-\\frac{1}{2}}e^{-\\frac{1}{2}(x-\\mu_m)^T\\bm{\\Sigma}_m^{-1}(x-\\mu_m)}.\\non \\end{equation}\\] Suppose that \\(M > 1\\) . Without loss of generality, assume that \\(\\bm{\\Sigma}_m = \\sigma_m\\bb{I}\\) . Given \\(N \\ge 1\\) observations, the density becomes \\[\\begin{equation} \\prod_{i=1}^Nf(x_i) \\propto \\prod_{i=1}^N\\left(\\sum_{m=1}^M\\alpha_m \\sigma_m^{-\\frac{d}{2}}e^{-\\frac{(x_i-\\mu_m)^T(x_i-\\mu_m)}{2\\sigma_m^d}}\\right). \\non \\end{equation}\\] If \\(x_i = \\mu_m\\) for some \\(1\\le i\\le N\\) and \\(1\\le m \\le M\\) , then it's density term reduces to \\[\\begin{equation} \\alpha_m \\sigma_m^{-\\frac{d}{2}}\\non \\end{equation}\\] and we can let \\(\\sigma_m\\ra 0\\) so that \\(\\prod_{i=1}^Nf(x_i)\\ra\\infty\\) . The case when \\(M=1\\) (a single Gaussian model without mixture) worth some discussions. For simplicity let's consider the 1-dimensional case, that is, \\(d=1\\) . Given \\(N > 1\\) observations, the density becomes proportional to \\[\\begin{equation} \\frac{1}{\\sigma^N}e^{-\\frac{\\sum_{i=1}^N(x_i-\\mu)^2}{2\\sigma^2}}.\\non \\end{equation}\\] In this case, as long as \\(x_i\\neq \\mu\\) for at least one \\(i\\in \\{1,...,N\\}\\) , the density does not explode when \\(\\sigma\\ra 0\\) because the exponential term dominates the convergence rate.","title":"Ex. 6.11"},{"location":"ESL-Solution/6-Kernel-Smoothing-Methods/ex6-12/","text":"Ex. 6.12 Write a computer program to perform a local linear discriminant analysis. At each query point \\(x_0\\) , the training data receive weights \\(K_\\lambda(x_0, x_i)\\) from a weighted kernel, and the ingredients for the linear decision boundaries (see Section 4.3) are computed by weighted averages. Try out your program on the zipcode data, and show the training and test errors for a series of five pre-chosen values of \\(\\lambda\\) . The zipcode data are available from the book website . Soln. 6.12 We choose RBF kernel \\[\\begin{equation} K_\\lambda(x,y) = \\exp(-\\lambda\\|x-y\\|^2),\\non \\end{equation}\\] with five different choices of \\(\\lambda \\in\\{ 0.001, 0.01, 0.05, 0.1, 1\\}\\) . The linear discriminant function is defined as (see (4.10) in the text) \\[\\begin{equation} \\delta_k(x_0) = x_0^T\\Sigma^{-1}(x_0)\\mu_k(x_0) - \\frac{1}{2}\\mu_k^T(x_0)\\Sigma^{-1}(x_0)\\mu_k(x_0) + \\log(\\pi_k)\\non \\end{equation}\\] in which we estimate unknown parameters as \\[\\begin{eqnarray} \\hat{\\pi}_k &=& N_k/N, \\text{ where } N_k \\text{ is the number of class-$k$ observations}\\non\\\\ \\hat{\\mu}_k(x_0) &=& \\sum_{g_i=k} K_\\lambda(x_0, x_i)x_i/N_k\\non\\\\ \\hat{\\Sigma}(x_0) &=& \\sum_{k=1}^K\\sum_{g_i=k}K_\\lambda(x_0, x_i)(x_i-\\hat{\\mu}_k(x_0))(x_i-\\hat{\\mu}_k(x_0))^T/(N-K).\\non \\end{eqnarray}\\] We summarize training and test error rates in Table 1 below. TODO rerun the script to update the numbers in the table Model Train Error Rate Test Error Rate LDA 6.20% 11.45% QDA 7.71% 18.19% Local LDA ( \\(\\lambda = 0.001\\) ) 0.0% 11.11% Local LDA ( \\(\\lambda = 0.01\\) ) 0.5% 3.0% Local LDA ( \\(\\lambda = 0.1\\) ) 0.65% 3.30% Local LDA ( \\(\\lambda = 1\\) ) 0.94% 3.85% Table 1: Misclassification Rates for Local LDA Models Code: 6.12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import pathlib import numpy as np from LocalLDA import LocalLinearDiscriminantAnalysis # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # get original data train = np . genfromtxt ( DATA_PATH . joinpath ( \"zip_train.csv\" ), dtype = float , delimiter = ',' , skip_header = True ) test = np . genfromtxt ( DATA_PATH . joinpath ( \"zip_test.csv\" ), dtype = float , delimiter = ',' , skip_header = True ) # prepare training and testing data x_train = train [:, 1 :] y_train = train [:, 0 ] x_test = test [:, 1 :] y_test = test [:, 0 ] # clf = LinearDiscriminantAnalysis() # clf = QuadraticDiscriminantAnalysis() # clf.fit(x_train, y_train) # y_pred = clf.predict(x_train) # # err=0 # for i in range(len(y_pred)): # if y_pred[i] != y_train[i]: # err += 1 # # print(err/len(y_pred)) clf = LocalLinearDiscriminantAnalysis ( gamma = 0.01 ) err = 0 for i in range ( x_test . shape [ 0 ]): print ( 'running for {} -th point, current error rate: {} ' . format ( i , err / ( i + 1 ))) pred = clf . predict ( x_test [ i ], x_train , y_train ) if pred != y_test [ i ]: err += 1 print ( err / x_test . shape [ 0 ]) Code: LocalLDA 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 import numpy as np from sklearn.metrics.pairwise import rbf_kernel from sklearn.base import BaseEstimator from sklearn.utils.multiclass import unique_labels def _class_weighted_means ( x0 , X , y , gamma ): \"\"\"Compute class weighted means . Parameters ---------- x0: a local point X : array-like of shape (n_samples, n_features) Input data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. gamma: float, exp(-gamma * (x-y)^2) Returns ------- means : array-like of shape (n_classes, n_features) Class weighted means. \"\"\" classes , y = np . unique ( y , return_inverse = True ) cnt = np . bincount ( y ) x0 = x0 . reshape ( 1 , - 1 ) weights = rbf_kernel ( X , Y = x0 , gamma = gamma ) W = np . diag ( weights . ravel ()) X_new = np . dot ( W , X ) means = np . zeros ( shape = ( len ( classes ), X_new . shape [ 1 ])) np . add . at ( means , y , X_new ) means /= cnt [:, None ] return means def _class_cov ( x0 , X , y , gamma ): \"\"\"Compute weighted within-class covariance matrix. The per-class covariance are weighted by the class priors. Parameters ---------- X : array-like of shape (n_samples, n_features) Input data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. Returns ------- cov : array-like of shape (n_features, n_features) Weighted within-class covariance matrix \"\"\" classes = np . unique ( y ) cov = np . zeros ( shape = ( X . shape [ 1 ], X . shape [ 1 ])) N = X . shape [ 0 ] K = len ( classes ) if N <= K : raise ValueError ( 'number of samples must be greater than number of classes' ) for idx , group in enumerate ( classes ): Xg = X [ y == group , :] Xg -= _class_weighted_means ( x0 , X , y , gamma = gamma )[ idx ] cov += np . dot ( Xg . T , Xg ) return cov / ( N - K ) class LocalLinearDiscriminantAnalysis ( BaseEstimator ): def __init__ ( self , gamma = 0.5 ): \"\"\" Parameters ---------- gamma \"\"\" self . gamma = gamma def predict ( self , x0 , X , y ): \"\"\" Parameters ---------- x0 X y Returns ------- \"\"\" X , y = self . _validate_data ( X , y , ensure_min_samples = 2 , estimator = self , dtype = [ np . float64 , np . float32 ]) self . classes_ = unique_labels ( y ) n_samples , _ = X . shape n_classes = len ( self . classes_ ) if n_samples <= n_classes : raise ValueError ( \"The number of samples must be more \" \"than the number of classes.\" ) cov = _class_cov ( x0 , X , y , self . gamma ) cov_inv = np . linalg . inv ( cov ) means = _class_weighted_means ( x0 , X , y , self . gamma ) deltas = [] for i in range ( n_classes ): delta = np . dot ( x0 . T , np . dot ( cov_inv , means [ i ])) delta += - 0.5 * np . dot ( means [ i ] . T , np . dot ( cov_inv , means [ i ])) _ , y_ = np . unique ( y , return_inverse = True ) cnt = np . bincount ( y_ ) pi = cnt [ i ] / n_samples delta += np . log ( pi ) deltas . append ( delta ) deltas = np . asarray ( deltas ) y_pred = self . classes_ . take ( deltas . argmax ()) return y_pred","title":"Ex. 6.12"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-01/","text":"Ex. 7.1 Derive the estimate of in-sample error (7.24). Soln. 7.1 It suffices to show that \\[\\begin{equation} \\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) = d\\sigma^2_\\epsilon.\\non \\end{equation}\\] Note that for a linear fit, we have \\(\\hat y = \\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^Ty\\) , so \\[\\begin{eqnarray} \\text{Cov}(\\hat y, y) &=& \\text{Cov}(\\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^Ty, y)\\non\\\\ &=& \\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\text{Cov}(y, y)\\non\\\\ &=& \\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\sigma^2_\\epsilon.\\non \\end{eqnarray}\\] Therefore, by cyclic property of trace operator, \\[\\begin{eqnarray} \\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) &=& \\text{trace}(\\bb{X}(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T)\\sigma^2_\\epsilon\\non\\\\ &=&\\text{trace}(\\bb{X}^T\\bb{X}(\\bb{X}^T\\bb{X})^{-1})\\sigma^2_\\epsilon\\non\\\\ &=&\\text{trace}(\\bb{I}_d)\\sigma^2_\\epsilon\\non\\\\ &=&d\\sigma^2_\\epsilon.\\non \\end{eqnarray}\\]","title":"Ex. 7.1"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-02/","text":"Ex. 7.2 For 0-1 loss with \\(Y\\in \\{0, 1\\}\\) and \\(\\text{Pr}(Y=1|x_0) = f(x_0)\\) , show that \\[\\begin{eqnarray} \\text{Err}(x_0) &=& \\text{Pr}(Y\\neq \\hat G(x_0)|X=x_0)\\non\\\\ &=&\\text{Err}_B(x_0) + |2f(x_0)-1|\\text{Pr}(\\hat G(x_0)\\neq G(x_0)|X = x_0),\\non \\end{eqnarray}\\] where \\(\\hat G(x) = \\bb{1}(\\hat f(x) > 1/2)\\) , \\(G(x) = \\bb{1}(f(x) > 1/2)\\) is the Bayes classifier, and \\(\\text{Err}_B(x_0) = \\text{Pr}(Y\\neq G(x_0)|X=x_0)\\) , the irreducible Bayes error at \\(x_0\\) . Using the approximation \\(\\hat f(x_0)\\sim N(E\\hat f(x_0), \\text{Var}(\\hat f(x_0))\\) , show that \\[\\begin{equation} \\text{Pr}(Y\\neq \\hat G(x_0)|X=x_0) \\approx \\Phi\\left(\\frac{\\text{sign}(\\frac{1}{2}-f(x_0))(E\\hat f(x_0) - \\frac{1}{2})}{\\sqrt{\\text{Var}(\\hat f(x_0))}}\\right).\\non \\end{equation}\\] In the above \\[\\begin{equation} \\Phi(t) = \\frac{1}{\\sqrt{2\\pi}}\\int_\\infty^t\\exp(-t^2/2)dt,\\non \\end{equation}\\] the cumulative Gaussian distribution function, This is an increasing function, with value 0 at \\(t=-\\infty\\) and value 1 at \\(t=+\\infty\\) . We can think of \\(\\text{sign}(\\frac{1}{2}-f(x_0))(E\\hat f(x_0) - \\frac{1}{2})\\) as a kind of boundary-bias term, as it depends on the true \\(f(x_0)\\) only through which side of the boundary \\((\\frac{1}{2})\\) that it lies. Notice also that the bias and variance combine in a multiplicative rather than additive fashion. If \\(E\\hat f(x_0)\\) is on the same side of \\((\\frac{1}{2})\\) , then the bias is negative, and decreasing the variance will decrease the misclassification error. On the other hand, if \\(E\\hat f(x_0)\\) is on the opposite side of \\((\\frac{1}{2})\\) to \\(f(x_0)\\) , then the bias is positive and it pays to increase the variance! Such an increase will improve the chance that \\(\\hat f(x_0)\\) falls on the correct side of \\((\\frac{1}{2})\\) ( On bias, variance, 0/1\u2014loss, and the curse-of-dimensionality ). Soln. 7.2 First consider the case when \\(f(x_0) \\ge 1/2\\) , we have \\(G(x_0) = 1\\) , and \\[\\begin{eqnarray} \\text{Err}(x_0) &=& \\text{Pr}(Y \\neq \\hat G(x_0)|X=x_0)\\non\\\\ &=&\\text{Pr}(Y=1|X=x_0)\\text{Pr}(\\hat G(x_0) =0|X=x_0)\\non\\\\ && + \\ \\text{Pr}(Y=0|X=x_0)\\text{Pr}(\\hat G(x_0) =1|X=x_0)\\non\\\\ &=&f(x_0) \\text{Pr}(\\hat G(x_0) =0|X=x_0)\\non\\\\ && + \\ (1-f(x_0))(1-\\text{Pr}(\\hat G(x_0) =0|X=x_0))\\non\\\\ &=&1-f(x_0) + (2f(x_0)-1)\\text{Pr}(\\hat G(x_0) =0|X=x_0)\\non\\\\ &=&\\text{Err}_B(x_0) + |2f(x_0)-1|\\text{Pr}(\\hat G(x_0)\\neq G(x_0)|X = x_0).\\non \\end{eqnarray}\\] Similar arguments hold for the case when \\(f(x_0) < 1/2\\) and \\(G(x_0) = 0\\) . Therefore, we have showed \\[\\begin{equation} \\text{Err}(x_0) = \\text{Err}_B(x_0) + |2f(x_0)-1|\\text{Pr}(\\hat G(x_0)\\neq G(x_0)|X = x_0).\\non \\end{equation}\\] For the second part, again, we first consider the case when \\(f(x_0) \\ge 1/2\\) (thus \\(G(x_0) = 1\\) ). In such case, we have \\[\\begin{eqnarray} \\text{Pr}(\\hat G(x_0)\\neq G(x_0)|X=x_0) &=& \\text{Pr}(\\hat G(x_0) = 0| X= x_0)\\non\\\\ &=&\\text{Pr}(\\hat f(x_0) < \\frac{1}{2})\\non\\\\ &=&\\text{Pr}\\left(\\frac{\\hat f(x_0) - E\\hat f(x_0)}{\\sqrt{\\text{Var}(\\hat f(x_0))}} < \\frac{\\frac{1}{2} - E\\hat f(x_0)}{\\sqrt{\\text{Var}(\\hat f(x_0))}}\\right)\\non\\\\ &\\approx&\\Phi\\left(\\frac{\\text{sign}(\\frac{1}{2}-f(x_0))(E\\hat f(x_0) - \\frac{1}{2})}{\\sqrt{\\text{Var}(\\hat f(x_0))}}\\right).\\non \\end{eqnarray}\\] Similar arguments hold for the case when \\(f(x_0) \\ge 1/2\\) as well.","title":"Ex. 7.2"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-03/","text":"Ex. 7.3 Let \\(\\boldsymbol{\\hat f} = \\bb{S}\\bb{y}\\) be a linear smoothing of \\(\\bb{y}\\) . (a) If \\(S_{ii}\\) is the \\(i\\) th diagonal element of \\(\\bb{S}\\) , show that for \\(\\bb{S}\\) arising from least squares projections and cubic smoothing splines, the cross-validated residual can be written as \\[\\begin{equation} \\label{eq:ex73-a} y_i - \\hat f^{-i}(x_i) = \\frac{y_i - \\hat f(x_i)}{1-S_{ii}}. \\end{equation}\\] (b) Use this result to show that \\(|y_i-\\hat f^{-i}(x_i)| \\ge |y_i - \\hat f(x_i)|\\) . (c) Find general conditions on any smoother \\(\\bb{S}\\) to make result \\(\\eqref{eq:ex73-a}\\) hold. Soln. 7.3 Without loss of generality, we assume \\[\\begin{equation} \\bb{S} = \\bb{X}(\\bb{X}^T\\bb{X} + \\lambda \\bm{\\Omega})^{-1}\\bb{X}^T.\\non \\end{equation}\\] For least squares we have \\(\\lambda=0\\) , and for cubic smoothing we have \\(\\lambda\\ge 0\\) . See Chapters 3 & 5 in the text for more details. (a) We have \\[\\begin{eqnarray} S_{ii} &=& x_i^T(\\bb{X}^T\\bb{X} + \\lambda \\bm{\\Omega})^{-1}x_i,\\non\\\\ \\hat f(x_i) &=& x_i^T(\\bb{X}^T\\bb{X} + \\lambda \\bm{\\Omega})^{-1}\\bb{X}^T\\bb{y}.\\non \\end{eqnarray}\\] Let \\(\\bb{X}_{-i}\\) and \\(\\bb{y}_{-i}\\) be the corresponding results with \\(x_i\\) removed, then we have \\[\\begin{eqnarray} \\hat f^{-i}(x_i) &=& x_i^T(\\bb{X}_{-i}^T\\bb{X}_{-i} + \\lambda \\bm{\\Omega})^{-1}\\bb{X}_{-i}^T\\bb{y}_{-i}\\non\\\\ &=&x_i^T(\\bb{X}^T\\bb{X} - x_ix_i^T + \\lambda \\bm{\\Omega})^{-1}(\\bb{X}^T\\bb{y}-x_iy_i).\\label{eq:73-1} \\end{eqnarray}\\] Let \\(\\bb{A} = (\\bb{X}^T\\bb{X} + \\lambda\\Omega)\\) , by Woodbury matrix identity , we have \\[\\begin{eqnarray} (\\bb{A}-x_ix_i^T)^{-1} &=& \\bb{A}^{-1} + \\frac{\\bb{A}^{-1}x_ix_i^T\\bb{A}^{-1}}{1-x_i^T\\bb{A}^{-1}x_i}.\\non \\end{eqnarray}\\] Therefore, \\(\\eqref{eq:73-1}\\) becomes \\[\\begin{eqnarray} \\hat f^{-1}(x_i) &=& x_i^T\\left(\\bb{A}^{-1} + \\frac{\\bb{A}^{-1}x_ix_i^T\\bb{A}^{-1}}{1-x_i^T\\bb{A}^{-1}x_i}\\right)(\\bb{X}^T\\bb{y}-x_iy_i)\\non\\\\ &=&\\left(x_i^T\\bb{A}^{-1} + \\frac{S_{ii}x_i^T\\bb{A}^{-1}}{1-S_{ii}}\\right)(\\bb{X}^T\\bb{y}-x_iy_i)\\non\\\\ &=&x_i^T\\bb{A}^{-1}\\bb{X}^T\\bb{y} - x_i^T\\bb{A}^{-1}x_iy_i + \\frac{S_{ii}x_i^T\\bb{A}^{-1}\\bb{X}^T\\bb{y}}{1-S_{ii}} - \\frac{S_{ii}x_i^T\\bb{A}^{-1}x_iy_i}{1-S_{ii}}\\non\\\\ &=&\\hat f(x_i) -y_iS_{ii} + \\frac{S_{ii}\\hat f(x_i)}{1-S_{ii}} - \\frac{y_iS_{ii}^2}{1-S_{ii}}\\non\\\\ &=&\\frac{\\hat f(x_i) - y_i S_{ii}}{1-S_{ii}}.\\non \\end{eqnarray}\\] Therefore by simple algebra we have \\(\\eqref{eq:ex73-a}\\) . (b) Note that \\(\\bb{S} = \\bb{X}(\\bb{X}^T\\bb{X} + \\lambda \\bm{\\Omega})^{-1}\\bb{X}^T\\) is positive-semidefinite and has eigen-decomposition \\[\\begin{equation} \\bb{S} = \\sum_{k=1}^N\\rho_k(\\lambda)\\bb{u}_k\\bb{u}_k^T.\\non \\end{equation}\\] See Section 5.4.1 in the text for more details. Therefore, we know that \\(\\bb{S}\\bb{S} \\preceq \\bb{S}\\) , so that \\[\\begin{equation} 0\\le (S^2)_{ii} = \\sum_{k\\neq i}S_{ik}^2 + S_{ii}^2\\le S_{ii}\\non \\end{equation}\\] from which we know \\(0\\le S_{ii} \\le 1\\) . By \\(\\eqref{eq:ex73-a}\\) we have \\(|y_i-\\hat f^{-i}(x_i)| \\ge |y_i - \\hat f(x_i)|\\) . (c) For general linear smoother \\(\\boldsymbol{\\hat f} = \\bb{S}\\bb{y}\\) , if \\(\\bb{S}\\) only depends on \\(\\bb{X}\\) and other tuning parameters (i.e., independent of \\(\\bb{y}\\) ), \\(\\eqref{eq:ex73-a}\\) still holds. To see that, note that if we replace \\(y_i\\) with \\(\\hat f^{-i}(x_i)\\) (obtained by \\(\\eqref{eq:73-1}\\) ) in \\(\\textbf{y}\\) and denote the new vector by \\(\\textbf{y}'\\) , \\(\\textbf{S}\\) is not changed. Thus we have \\[\\begin{eqnarray} \\hat f^{-i}(x_i) &=&(\\textbf{S}\\textbf{y}')_i\\non\\\\ &=& \\sum_{i\\neq j}S_{ij}\\textbf{y}'_j + S_{ii}\\hat f^{-i}(x_i)\\non\\\\ &=&\\hat f(x_i) - S_{ii}y_i + S_{ii}\\hat f^{-i}(x_i),\\non \\end{eqnarray}\\] therefore we obtain \\(\\eqref{eq:ex73-a}\\) .","title":"Ex. 7.3"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-04/","text":"Ex. 7.4 Consider the in-sample prediction error (7.18) and the training error \\(\\overline{\\text{err}}\\) in the case of squared-error loss: \\[\\begin{eqnarray} \\text{Err}_{\\text{in}} &=& \\frac{1}{N}\\sum_{i=1}^NE_{Y^0}(Y_i^0-\\hat f(x_i))^2\\non\\\\ \\overline{\\text{err}} &=& \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat f(x_i))^2.\\non \\end{eqnarray}\\] Add and subtract \\(f(x_i)\\) and \\(E\\hat f(x_i)\\) in each expression and expand. Hence establish that the average optimism in the training error is \\[\\begin{equation} \\frac{2}{N}\\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i),\\non \\end{equation}\\] as given in (7.21). Soln. 7.4 We start with \\(\\text{Err}_{\\text{in}}\\) . Let's denote \\(\\hat y_i = \\hat f(x_i)\\) and write \\[\\begin{equation} Y_i^0-\\hat f(x_i) = Y_i^0-f(x_i) + f(x_i)-E\\hat y_i + E\\hat y_i -\\hat y_i\\non \\end{equation}\\] so that \\[\\begin{eqnarray} \\text{Err}_{\\text{in}} &=& \\frac{1}{N}\\sum_{i=1}^NE_{Y^0}\\left(Y_i^0-f(x_i) + f(x_i)-E\\hat y_i + E\\hat y_i -\\hat y_i\\right)^2\\non\\\\ &=&\\frac{1}{N}\\sum_{i=1}^NA_i + B_i + C_i + D_i + E_i + F_i,\\non \\end{eqnarray}\\] where \\[\\begin{eqnarray} A_i &=& E_{Y^0} (Y_i^0-f(x_i))^2\\non\\\\ B_i &=& E_{Y^0} (f(x_i) - E\\hat y_i)^2 = (f(x_i) - E\\hat y_i)^2\\non\\\\ C_i &=& E_{Y^0} (E\\hat y_i-\\hat y_i)^2 = (E\\hat y_i-\\hat y_i)^2\\non\\\\ D_i &=& 2E_{Y^0} (Y_i^0-f(x_i))(f(x_i) - E\\hat y_i)\\non\\\\ E_i &=& 2E_{Y^0} (Y_i^0-f(x_i))(E\\hat y_i-\\hat y_i)\\non\\\\ F_i &=& 2E_{Y^0} (f(x_i) - E\\hat y_i)(E\\hat y_i-\\hat y_i) = 2(f(x_i) - E\\hat y_i)(E\\hat y_i-\\hat y_i)\\non \\end{eqnarray}\\] Similarly for \\(\\overline{\\text{err}}\\) we have \\[\\begin{equation} y_i-\\hat f(x_i) = y_i - f(x_i) +f(x_i) - E\\hat y_i + E\\hat y_i -\\hat y_i\\non \\end{equation}\\] and \\[\\begin{eqnarray} \\overline{\\text{err}} &=& \\frac{1}{N}\\sum_{i=1}^{N}(y_i - f(x_i) +f(x_i) - E\\hat y_i + E\\hat y_i -\\hat y_i)^2\\non\\\\ &=&\\frac{1}{N}\\sum_{i=1}^N G_i + B_i + C_i + H_i + J_i + F_i,\\non \\end{eqnarray}\\] where \\[\\begin{eqnarray} G_i &=& (y_i-f(x_i))^2\\non\\\\ H_i &=& 2(y_i-f(x_i))(f(x_i) - E\\hat y_i)\\non\\\\ J_i &=& 2(y_i-f(x_i))(E\\hat y_i -\\hat y_i).\\non \\end{eqnarray}\\] Therefore, we have \\[\\begin{eqnarray} E_\\bb{y}(\\text{op}) &=& E_\\bb{y}(\\text{Err}_{\\text{in}} - \\overline{\\text{err}})\\non\\\\ &=&\\frac{1}{N}\\sum_{i=1}^NE_\\bb{y}[(A_i-G_i) + (D_i-H_i) + (E_i-J_i)].\\non \\end{eqnarray}\\] For \\(A_i\\) and \\(G_i\\) , the expectaion over \\(\\bb{y}\\) captures unpredictable error and thus \\(E_\\bb{y}(A_i-G_i) = 0\\) . Similarly we have \\(E_\\bb{y}D_i = E_\\bb{y}H_i = E_\\bb{y}E_i =0\\) , and thus \\[\\begin{eqnarray} E_\\bb{y}(\\text{op}) &=& - \\frac{2}{N}\\sum_{i=1}^NJ_i\\non\\\\ &=& - \\frac{2}{N}\\sum_{i=1}^NE_\\bb{y}(y_i-f(x_i))(E\\hat y_i -\\hat y_i)\\non\\\\ &=&\\frac{2}{N}\\sum_{i=1}^N [E_\\bb{y}(y_i\\hat y_i) - E_\\bb{y}y_iE_\\bb{y}\\hat y_i]\\non\\\\ &=&2\\text{Cov}(y_i, \\hat y_i).\\non \\end{eqnarray}\\]","title":"Ex. 7.4"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-05/","text":"Ex. 7.5 For a linear smoother \\(\\hat{\\mathbf{y}} = \\bb{S}\\bb{y}\\) , show that \\[\\begin{equation} \\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) = \\text{trace}(\\bb{S})\\sigma_\\epsilon^2,\\non \\end{equation}\\] which justifies its use as the effective number of parameters. Soln. 7.5 \\[\\begin{eqnarray} \\sum_{i=1}^N\\text{Cov}(\\hat y_i, y_i) &=& \\text{trace}(\\text{Cov}(\\hat{\\mathbf{y}}, \\bb{y}))\\non\\\\ &=&\\text{trace}(\\text{Cov}(\\bb{S}\\bb{y}, \\bb{y}))\\non\\\\ &=&\\text{trace}(\\bb{S}\\text{Cov}(\\bb{y}, \\bb{y}))\\non\\\\ &=&\\text{trace}(\\bb{S}\\text{Var}(\\bb{y}))\\non\\\\ &=&\\text{trace}(\\bb{S})\\sigma_\\epsilon^2.\\non \\end{eqnarray}\\] Remark This exercise is similar to Ex. 7.1 .","title":"Ex. 7.5"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-06/","text":"Ex. 7.6 Show that for an additive-error model, the effective degrees-of-freedom for the \\(k\\) -nearest-neighbors regression fit is \\(N/k\\) . Soln. 7.6 Note that for this \\(k\\) -nearest-neighbors model, it's a linear smoother. To see that, note \\[\\begin{equation} \\hat Y(x) = \\frac{1}{k}\\sum_{i: x_i\\in N_k(x)}y_i = \\frac{1}{k}\\sum_{i=1}^N\\eta_i y_i\\non \\end{equation}\\] where \\(\\eta_i = 1\\) if \\(x_i\\in N_k(x)\\) and 0 otherwise. So we can write \\[\\begin{equation} \\hat Y = \\frac{1}{k}\\bb{S}\\bb{y}\\non \\end{equation}\\] in which \\(\\bb{S}\\) is a binary matrix with diagonal elements being 1 since the nearest one (itself) must be included in estimation. Therefore, the effective df is simply \\[\\begin{equation} \\frac{1}{k}\\text{trace}(\\bb{S}) = \\frac{N}{k}.\\non \\end{equation}\\]","title":"Ex. 7.6"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-07/","text":"Ex. 7.7 Use the approximation \\(1/(1-x)^2 \\approx 1 + 2x\\) to expose the relationship between \\(C_p/\\text{AIC}\\) (7.26) and GCV (7.52), the main difference being the model used to estimate the noise variance \\(\\sigma_\\epsilon^2\\) . Soln. 7.7 By (7.52) in the text, we have \\[\\begin{eqnarray} \\text{GCV}(\\hat f) &=& \\frac{1}{N}\\sum_{i=1}^N\\left(\\frac{y_i-\\hat f(x_i)}{1-\\text{trace}(\\bb{S})/N}\\right)^2\\non\\\\ &\\approx& \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat f(x_i))^2\\left(1 + \\frac{2\\text{trace}(\\bb{S})}{N}\\right)\\non\\\\ &=&\\overline{\\text{err}} + \\frac{2\\text{trace}(\\bb{S})}{N^2}\\sum_{i=1}^N(y_i-\\hat f(x_i))^2\\non\\\\ &\\approx&\\overline{\\text{err}} + \\frac{2\\text{trace}(\\bb{S})}{N}\\hat\\sigma^2_\\epsilon.\\non \\end{eqnarray}\\] For \\(C_p/\\text{AIC}\\) , by (7.26) in the text, we have \\[\\begin{equation} C_p = \\overline{\\text{err}} + 2 \\cdot \\frac{d}{N}\\hat\\sigma^2_\\epsilon.\\non \\end{equation}\\] Recall that \\(\\text{trace}(\\bb{S})\\) is the effective degree-of-freedom \\(d\\) , therefore \\(C_p/\\text{AIC}\\) and GCV have almost the same expression and the main difference is how to estimate the noise variance \\(\\sigma^2_\\epsilon\\) .","title":"Ex. 7.7"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-08/","text":"Ex. 7.8 Show that the set of functions \\(\\{I(\\sin(\\alpha x) > 0)\\}\\) can shatter the following points on the line: \\[\\begin{equation} z^1 = 10^{-1}, \\dots, z^l = 10^{-l},\\non \\end{equation}\\] for any \\(l\\) . Hence the VC dimension of the class \\(\\{I(\\sin(\\alpha x) > 0)\\}\\) is infinite. Soln. 7.8 Consider the labeled dataset \\(\\{z^{-i}, y_i\\}\\) where \\(y_i\\in {-1, 1}\\) and \\(i=1,..., n\\) . We set \\[\\begin{eqnarray} \\alpha &=& \\pi\\left(1 + \\sum_{i=1}^n\\frac{1-y_i}{2}10^i\\right)\\non\\\\ &=&\\pi\\left(1 + \\sum_{i:y_i=-1}10^i\\right).\\non \\end{eqnarray}\\] We first show that \\(\\sin(\\alpha x)\\) can correctly predict the negative labels, that is, \\(y_i=-1\\) . For any point \\(x_j = 10^{-j}\\) with \\(y_j=-1\\) , we have \\[\\begin{eqnarray} \\alpha x_j &=& \\pi 10 ^{-j}\\left(1+\\sum_{i:y_i=-1}10^i\\right)\\non\\\\ &=&\\pi 10^{-j}\\left(1+10^j+\\sum_{i: y_i=-1, i\\neq j}10^i\\right)\\non\\\\ &=&\\pi\\left(10^{-j} + 1 + \\sum_{i:y_i=-1, i\\neq j}10^{i-j}\\right)\\non\\\\ &=&\\pi\\left(10^{-j} + 1 + \\sum_{i:y_i=-1, i > j}10^{i-j} + \\sum_{i:y_i=-1, i < j}10^{i-j}\\right).\\non \\end{eqnarray}\\] For \\(i>j\\) , \\(10^{i-j}\\) is even and so their sum, so \\(\\sum_{i:y_i=-1, i > j}10^{i-j}\\) can be written as \\(2k\\) for some \\(k\\in\\mathbb{N}\\) . Therefore we can write \\[\\begin{equation} \\alpha x_j = \\pi\\left(10^{-j}+1+\\sum_{i:y_i=-1, i < j}10^{i-j}\\right) + 2k\\pi.\\non \\end{equation}\\] For \\(i < j\\) , we have \\[\\begin{equation} \\sum_{i:y_i=-1, i < j}10^{i-j} < \\sum_{i=1}^\\infty 10^{-i} = \\frac{1}{9}.\\non \\end{equation}\\] Let \\(\\epsilon = 10^{-j} + \\sum_{i:y_i=-1, i < j}10^{i-j}\\) , we know \\(0 < \\epsilon < 1\\) . Thus \\[\\begin{equation} \\pi < \\pi(1+\\epsilon) < 2\\pi,\\non \\end{equation}\\] so that \\[\\begin{equation} \\alpha x_j = \\pi(1+\\epsilon) + 2k\\pi \\in ((2k+1)\\pi, 2(k+1)\\pi).\\non \\end{equation}\\] Thus \\(\\sin(\\alpha x_j) < 0\\) for all \\(j\\) such that \\(y_j=-1\\) . Next we show that \\(\\sin(\\alpha x)\\) can correctly predict the positive labels. For any point \\(x_j=10^{-j}\\) with \\(y_j=1\\) , we have \\[\\begin{eqnarray} \\alpha x_j &=& \\pi 10^{-j}\\left(1 + \\sum_{i: y_i=1}10^i\\right)\\non\\\\ &=&\\pi 10^{-j}\\left(1+\\sum_{i: y_i=1, i\\neq j}10^i\\right)\\non\\\\ &=&\\pi\\left(10^{-j} + \\sum_{i:y_i=1, i > j}10^{i-j} + \\sum_{i:y_i=1, i < j}10^{i-j} \\right)\\non\\\\ &=&\\pi\\epsilon + 2k\\pi.\\non \\end{eqnarray}\\] Thus we have \\(\\alpha x_j \\in (2k\\pi, (2k+1)\\pi)\\) and \\(\\sin(\\alpha x_j) > 0\\) . The proof holds for any \\(n\\in\\mathbb{N}\\) , thus the VC dimension of the class \\(\\{I(\\sin(\\alpha x) > 0)\\}\\) is infinite.","title":"Ex. 7.8"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-09/","text":"Ex. 7.9 For the prostate data of Chapter 3, carry out a best-subset linear regression analysis, as in Table 3.3 (third column from left). Compute the AIC, BIC, five- and tenfold cross-validation, and bootstrap .632 estimates of prediction error. Discuss the results. Warning Still in progress.","title":"Ex. 7.9"},{"location":"ESL-Solution/7-Model-Assessment-and-Selection/ex7-10/","text":"Ex. 7.10 Referring to the example in Section 7.10.3, suppose instead that all of the \\(p\\) predictors are binary, and hence there is no need to estimate split points. The predictors are independent of the class labels as before. Then if \\(p\\) is very large, we can probably find a predictor that splits the entire training data perfectly, and hence would split the validation data (one-fifth of data) perfectly as well. This predictor would therefore have zero cross-validation error. Does this mean that cross-validation does not provide a good estimate of test error in this situation? [This question was suggested by Li Ma.] Warning This problem has not been worked yet.","title":"Ex. 7.10 (TODO)"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-1/","text":"Ex. 8.1 Let \\(r(y)\\) and \\(q(y)\\) be the probability density functions. Jensen's inequality states that for a random variable \\(X\\) and a convex function \\(\\phi(x)\\) , \\(E[\\phi(x)]\\ge \\phi(E[X])\\) . Use Jensen's inequality to show that \\[\\begin{equation} E_q\\log[r(Y)/q(Y)]\\non \\end{equation}\\] is maximized as a function of \\(r(y)\\) when \\(r(y) = q(y)\\) . Hence show that \\(R(\\theta, \\theta)\\ge R(\\theta', \\theta)\\) as stated below equation (8.46). Soln. 8.1 Note that \\(-\\log(x)\\) is convex, by Jensen's inequality, we have \\[\\begin{eqnarray} E_q[-\\log[r(Y)/q(Y)]] &\\ge& -\\log[E_q[r(Y)/q(Y)]]\\non\\\\ &=&-\\log\\left[\\int\\frac{r(y)}{q(y)}q(y)dy\\right]\\non\\\\ &=&-\\log\\left[\\int r(y)dy\\right]\\non\\\\ &=&-\\log(1)\\non\\\\ &=&0,\\non \\end{eqnarray}\\] therefore we have \\[\\begin{equation} E_q[\\log(r(Y)/q(Y))]\\le 0 = E_q[\\log(q(Y)/q(Y))].\\non \\end{equation}\\] So the expectation is maximized when \\(r = q\\) . For equation (8.46) in the text, we have \\[\\begin{eqnarray} R(\\theta', \\theta) - R(\\theta, \\theta) &=& E[\\ell_1(\\theta;\\bb{Z}^m|\\bb{Z})|\\bb{Z}, \\theta] - E[\\ell(\\theta;\\bb{Z}^m|\\bb{Z})|\\bb{Z}, \\theta]\\non\\\\ &=&E_{\\text{Pr}(\\bb{Z}^m|\\bb{Z}, \\theta)}\\left(\\log \\frac{\\text{Pr}(\\bb{Z}^m|\\bb{Z}, \\theta')}{\\text{Pr}(\\bb{Z}^m|\\bb{Z}, \\theta)}\\right)\\non\\\\ &\\le&0.\\non \\end{eqnarray}\\]","title":"Ex. 8.1"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-2/","text":"Ex. 8.2 Consider the maximization of the log-likelihood (8.48), over distributions \\(\\tilde P(\\bb{Z}^m)\\) such that \\(\\tilde P(\\bb{Z}^m)\\ge 0\\) and \\(\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m) = 1\\) . Use Lagrange multipliers to show that the solution is the conditional distribution \\(\\tilde P(\\bb{Z}^m) = \\text{Pr}(\\bb{Z}^m|\\bb{Z}, \\theta')\\) , as in (8.49). Soln. 8.2 We first write \\[\\begin{eqnarray} F(\\theta', \\tilde P) &=& E_{\\tilde P}[\\ell_0(\\theta';\\bb{T})] - E_{\\tilde P}[\\log \\tilde P(\\bb{Z}^m)]\\non\\\\ &=& \\sum_{\\bb{Z}^m} \\ell_0(\\theta';\\bb{T})\\tilde P(\\bb{Z}^m)-\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m)\\log \\tilde P(\\bb{Z}^m).\\non \\end{eqnarray}\\] With the constraint \\(\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m) = 1\\) , the Lagrange multiplier of \\(F(\\theta', \\tilde P)\\) with \\(\\theta'\\) fixed is \\[\\begin{equation} L(\\tilde P, \\lambda) = \\sum_{\\bb{Z}^m} \\ell_0(\\theta';\\bb{T})\\tilde P(\\bb{Z}^m)-\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m)\\log \\tilde P(\\bb{Z}^m) - \\lambda\\left(\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m)-1\\right).\\non \\end{equation}\\] Further we set \\[\\begin{equation} \\label{eq:ex8-2lag} \\frac{\\partial L(\\tilde P, \\lambda)}{\\partial \\tilde P} = \\ell_0(\\theta';\\bb{T}) - \\left(\\log \\tilde P(\\bb{Z}^m) + 1\\right) +\\lambda =0 \\end{equation}\\] so that \\[\\begin{equation} \\tilde P(\\bb{Z}^m) =\\exp(\\ell_0(\\theta';\\bb{T}) + \\lambda -1).\\non \\end{equation}\\] Recall the constraint that \\(\\sum_{\\bb{Z}^m}\\tilde P(\\bb{Z}^m) = 1\\) , we get \\[\\begin{equation} \\sum_{\\bb{Z}^m}\\exp(\\ell_0(\\theta';\\bb{T}) + \\lambda -1) =1, \\non \\end{equation}\\] which yields \\[\\begin{equation} \\lambda = 1 - \\log\\left(\\text{Pr}(\\bb{Z}|\\theta')\\right).\\non \\end{equation}\\] Plugging \\(\\lambda\\) above into \\(\\eqref{eq:ex8-2lag}\\) we get \\[\\begin{equation} \\tilde P(\\bb{Z}^m) = \\text{Pr}(\\bb{Z}^m|\\bb{Z}, \\theta').\\non \\end{equation}\\]","title":"Ex. 8.2"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-3/","text":"Ex. 8.3 Justify the estimate (8.50), using the relationship \\[\\begin{equation} \\text{Pr}(A) = \\int \\text{Pr}(A|B)d(\\text{Pr}(B)).\\non \\end{equation}\\] Soln. 8.3 From the relationship above, we have \\[\\begin{eqnarray} \\widehat{\\text{Pr}}_{U_k}(u) = \\int \\text{Pr}_{U_k|U_l: l\\neq k}(u)d\\text{Pr}_{U_l: l \\neq k}.\\non \\end{eqnarray}\\] The integral is thus estimated by a law-of-large-number way to be \\[\\begin{equation} \\frac{1}{M-m+1}\\sum_{t=m}^M\\text{Pr}\\left(u|U_l^{(t)}, l\\neq k\\right).\\non \\end{equation}\\]","title":"Ex. 8.3"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-4/","text":"Ex. 8.4 Consider the bagging method of Section 8.7. Let our estimate \\(\\hat f(x)\\) be the \\(B\\) -spline smoother \\(\\hat \\mu(x)\\) of Section 8.2.1. Consider the parametric bootstrap of equation (8.6), applied to this estimator. Show that if we bag \\(\\hat f(x)\\) , using the parametric bootstrap to generate the bootstrap samples, the bagging estimate \\(\\hat f_{\\text{bag}}(x)\\) converges to the original estimate \\(\\hat f(x)\\) as \\(B\\ra\\infty\\) . Soln. 8.4 By definition of bagging we get \\[\\begin{equation} \\label{eq:ex8-4bag} \\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat f^\\ast_b(x) \\end{equation}\\] where \\[\\begin{equation} \\hat f^\\ast_b(x) = Sy^\\ast = S(\\hat f(x) + \\epsilon_b)\\non \\end{equation}\\] \\[\\begin{equation} S = N(N^TN)^{-1}N^T\\non \\end{equation}\\] and \\(\\epsilon_b\\sim N(0,\\sigma^2)\\) for \\(B\\) -spline smoother. Note that \\(S^2 = S\\) , we obtain \\[\\begin{equation} \\hat f^\\ast_b(x) = S(\\hat f(x) + \\epsilon_b) = S(Sy+\\epsilon_b) = Sy + S\\epsilon_b.\\non \\end{equation}\\] Therefore \\(\\eqref{eq:ex8-4bag}\\) reduces to \\[\\begin{equation} \\hat f_{\\text{bag}}(x) = Sy + S\\left(\\frac{1}{B}\\sum_{b=1}^B\\epsilon_b\\right).\\non \\end{equation}\\] From here it's easy to see that \\[\\begin{equation} \\lim_{B\\ra\\infty} \\hat f_{\\text{bag}}(x) = Sy = \\hat f(x).\\non \\end{equation}\\]","title":"Ex. 8.4"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-5/","text":"Ex. 8.5 Suggest generalizations of each of the loss functions in Figure 10.4 to more than two classes, and design an appropriate plot to compare them. Soln. 8.5 Following the idea of Multi-class adaboost (see Ex. 10.5 as well), for a \\(K\\) -class classification problem, consider the coding \\(Y=(Y_1,...,Y_K)^T\\) with \\[\\begin{equation} Y_k = \\begin{cases} 1, & \\text{ if } G=\\mathcal{G}_k\\\\ -\\frac{1}{K-1}, & \\text{ otherwise. } \\end{cases}\\non \\end{equation}\\] Let \\(f=(f_1,...,f_K)^T\\) with \\(\\sum_{k=1}^Kf_k=0\\) . The exponential loss is defined by \\[\\begin{equation} L(Y,f) = \\exp\\left(-\\frac{1}{K}Y^Tf\\right).\\non \\end{equation}\\] Similarly, the multinomial deviance loss is defined by \\[\\begin{equation} L(Y,f) = \\log\\left(1+\\exp\\left(-\\frac{1}{K}Y^Tf\\right)\\right).\\non \\end{equation}\\] For misclassification loss, we can further restrict \\(f\\) to be in the same form as \\(Y\\) , that is, \\[\\begin{equation} f_k = \\begin{cases} 1, & \\text{ if } G=\\mathcal{G}_k\\\\ -\\frac{1}{K-1}, & \\text{ otherwise. } \\end{cases}\\non \\end{equation}\\] When \\(K=2\\) , this coincides with the decision boundary \\(f(x)=0\\) . Therefore, we can let the loss be \\[\\begin{equation} L(Y,f) = \\textbf{1}(Y^Tf \\ge 0).\\non \\end{equation}\\] Similar to misclassification loss, the square error would be \\[\\begin{equation} L(Y, f) = \\|Y-f\\|_2^2.\\non \\end{equation}\\] The support vector error is \\[\\begin{equation} L(Y, f) = (1-Y^Tf)_+.\\non \\end{equation}\\] As for the plot, it suffices to change the \\(x\\) -axis in Figure 10.4 from \\(yf\\) to \\(Y^Tf\\) .","title":"Ex. 8.5"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-6/","text":"Ex. 8.6 Consider the bone mineral density data of Figure 5.6. (a) Fit a cubic smooth spline to the relative change in spinal BMD, as a function of age. Use cross-validation to estimate the optimal amount of smoothing. Construct pointwise 90% confidence bands for the underlying function. (b) Compute the posterior mean and covariance for the true function via (8.28), and compare the posterior bands to those obtained in (a). (c) Compute 100 bootstrap replicates of the fitted curves, as in the bottom left panel of Figure 8.2. Compare the results to those obtained in (a) and (b). Soln. 8.6 The relevant text are Section 5.5.2 and Section 8.2.1 in the text. We used 10-fold cross validation and chose the degree of freedom to be 14 in this example. Fitted cubic spline and bands from various methods are plotted in Figure 1 for men's bone mineral density data. Figure 1: Fitted Cubic Spline and 90% Standard Error Bands Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 import pathlib import numpy as np import pandas as pd from numpy.linalg import inv from numpy.linalg import multi_dot from sklearn.base import BaseEstimator from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error from sklearn.utils import resample import statsmodels.api as sm from patsy import dmatrix import plotly.graph_objects as go # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # prepare data data = pd . read_csv ( DATA_PATH . joinpath ( \"boneMineralDensity.csv\" ), header = 0 ) data_men = data . loc [ data [ 'gender' ] == 'male' ] data_men = data_men . sort_values ( 'age' ) X_men = data_men . loc [:, 'age' ] . to_numpy () y_men = data_men . loc [:, 'spnbmd' ] . to_numpy () data_women = data . loc [ data [ 'gender' ] == 'female' ] data_women = data_women . sort_values ( 'age' ) X_women = data_women . loc [:, 'age' ] . to_numpy () y_women = data_women . loc [:, 'spnbmd' ] . to_numpy () # define a cubic smooth spline estimator class CubicSmoothSpline ( BaseEstimator ): def __init__ ( self , df = 10 ): self . df = df self . H = None self . fitNatural = None self . pred = None def fit ( self , X , y = None ): self . H = dmatrix ( 'cr(x, df= {} )' . format ( self . df ), { 'x' : X }, return_type = \"dataframe\" ) self . fitNatural = sm . GLM ( y , self . H ) . fit () return self def predict ( self , X ): self . pred = self . fitNatural . predict ( dmatrix ( 'cr(xp, df= {} )' . format ( self . df ), { 'xp' : X })) return self . pred # (a) cross validation param_grid = [{ 'df' : np . arange ( 5 , 21 )}] css = CubicSmoothSpline () grid_search = GridSearchCV ( css , param_grid , cv = 10 , scoring = 'neg_mean_squared_error' ) grid_search . fit ( X_men , y_men ) final_model = grid_search . best_estimator_ final_df = final_model . df print ( \"The degree of freedom chosen by 10-fold CV is: {} \" . format ( final_df )) # calculate point-wise variance final_model . fit ( X_men , y_men ) final_model . predict ( X_men ) y_men_pred = final_model . pred sigma_square = mean_squared_error ( y_men_pred , y_men ) H = np . asarray ( final_model . H ) m_Sigma = sigma_square * ( inv ( np . matmul ( H . transpose (), H ))) m_nc = multi_dot ([ H , m_Sigma , H . transpose ()]) pt_var_nc = m_nc . diagonal () pt_std_nc = np . sqrt ( pt_var_nc ) upper = y_men_pred + 1.65 * pt_std_nc lower = y_men_pred - 1.65 * pt_std_nc # plot fig = go . Figure () fig . add_trace ( go . Scatter ( x = X_men , y = y_men , mode = 'markers' , name = 'Men Raw Data' , line_color = '#993399' )) fig . add_trace ( go . Scatter ( x = X_men , y = y_men_pred , mode = 'lines' , name = 'Fitted Smoothing Spline' , line_color = '#993399' )) fig . add_trace ( go . Scatter ( x = X_men , y = upper , mode = 'lines' , name = 'Upper Bound in (a)' )) fig . add_trace ( go . Scatter ( x = X_men , y = lower , mode = 'lines' , name = 'Lower Bound in (a)' )) # (b): from (8.28) with tau=10 and \\Sigma = Identity matrix tau = 10 n = H . shape [ 1 ] m_Sigma_2 = sigma_square * ( inv ( np . matmul ( H . transpose (), H ) + sigma_square / tau * np . identity ( n ))) m_nc_2 = multi_dot ([ H , m_Sigma_2 , H . transpose ()]) pt_var_nc_2 = m_nc_2 . diagonal () pt_std_nc_2 = np . sqrt ( pt_var_nc_2 ) upper_2 = y_men_pred + 1.65 * pt_std_nc_2 lower_2 = y_men_pred - 1.65 * pt_std_nc_2 fig . add_trace ( go . Scatter ( x = X_men , y = upper_2 , mode = 'lines' , name = 'Upper Bound in (b)' )) fig . add_trace ( go . Scatter ( x = X_men , y = lower_2 , mode = 'lines' , name = 'Lower Bound in (b)' )) # (c) bootstrap method B = 100 fitted_curve_list = [] for b in np . arange ( B ): data_sample = resample ( data_men , replace = True ) data_sample = data_sample . sort_values ( 'age' ) X_sample = data_sample . loc [:, 'age' ] . to_numpy () y_sample = data_sample . loc [:, 'spnbmd' ] . to_numpy () final_model . fit ( X_sample , y_sample ) y_pred = final_model . predict ( X_sample ) fitted_curve_list . append ( y_pred ) bs = np . stack ( fitted_curve_list ) upper_3 = np . percentile ( bs , 90 , axis = 0 , interpolation = 'nearest' ) lower_3 = np . percentile ( bs , 10 , axis = 0 , interpolation = 'nearest' ) fig . add_trace ( go . Scatter ( x = X_men , y = upper_3 , mode = 'lines' , name = 'Upper Bound in (c)' )) fig . add_trace ( go . Scatter ( x = X_men , y = lower_3 , mode = 'lines' , name = 'Lower Bound in (c)' )) fig . update_layout ( xaxis_title = \"Age\" , yaxis_title = \"Relative Change in Spinal BMD\" , ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) fig . show ()","title":"Ex. 8.6"},{"location":"ESL-Solution/8-Model-Inference-and-Averaging/ex8-7/","text":"Ex. 8.7 EM as a minorization algorithm ( Hunter and Lange, 2004 ; The MM alternative to EM ). A function \\(g(x, y)\\) to said to minorize a function \\(f(x)\\) is \\[\\begin{equation} g(x,y) \\le f(x), \\ g(x,x) = f(x)\\non \\end{equation}\\] for all \\(x, y\\) in the domain. This is useful for maximizing \\(f(x)\\) since is easy to show that \\(f(x)\\) is non-decreasing under the update \\[\\begin{equation} x^{s+1} = \\underset{x}{\\operatorname{argmax}}g(x,x^s)\\non. \\end{equation}\\] There are analogous definitions for majorization , for minimizing a function \\(f(x)\\) . The resulting algorithms are known as \\(MM\\) algorithms, for Minorize-Maximize or Majorize-Minimize . Show that the EM algorithm (Section 8.5.2) is an example of an MM algorithm, using \\(Q(\\theta', \\theta) + \\log \\text{Pr}(\\bb{Z}|\\theta) - Q(\\theta, \\theta)\\) to minorize the observed data log-likelihood \\(\\ell(\\theta';\\bb{Z})\\) . (Note that only the first term involves the relevant parameter \\(\\theta'\\) ). Soln. 8.7 Denote \\[\\begin{eqnarray} g(\\theta',\\theta) &=& Q(\\theta', \\theta) + \\log \\text{Pr}(\\bb{Z}|\\theta) - Q(\\theta, \\theta)\\non\\\\ f(\\theta) &=& \\ell(\\theta';\\bb{Z}).\\non \\end{eqnarray}\\] It suffices to show that (a) \\(g(\\theta', \\theta) \\le f(\\theta')\\) for any \\(\\theta, \\theta'\\) . (b) \\(g(\\theta, \\theta) = f(\\theta)\\) for any \\(\\theta\\) . Since (b) is trivial from our definitions, the rest proof focuses on (a). Recall (8.47) in the text and Ex. 8.1 , we know \\(R(\\theta', \\theta)\\le R(\\theta, \\theta)\\) so \\[\\begin{eqnarray} f(\\theta') - f(\\theta) &=& \\ell(\\theta';\\bb{Z}) - \\ell(\\theta;\\bb{Z})\\non\\\\ &=& [Q(\\theta', \\theta) - Q(\\theta, \\theta)] - [R(\\theta', \\theta) - R(\\theta, \\theta)]\\non\\\\ &\\ge& [Q(\\theta', \\theta) - Q(\\theta, \\theta)].\\non \\end{eqnarray}\\] A simple algebra shows that \\(g(\\theta', \\theta) \\le f(\\theta')\\) and the proof is complete.","title":"Ex. 8.7"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-1/","text":"Ex. 9.1 Show that a smoothing spline fit of \\(y_i\\) to \\(x_i\\) preserves the linear part of the fit. In other words, if \\(y_i = \\hat y_i + r_i\\) , where \\(\\hat y_i\\) represents the linear regression fits, and \\(\\bb{S}\\) is the smoothing matrix, then \\(\\bb{S}\\bb{y} = \\boldsymbol{\\hat y} + \\bb{S}\\bb{r}\\) . Show that the same is true for local linear regression (Section 6.1.1). Hence argue that the adjustment step in the second line of (2) in Algorithm 9.1 is unnecessary. Soln. 9.1 We need to show that \\(\\textbf{S}\\boldsymbol{\\hat y}=\\boldsymbol{\\hat y}\\) . Recall (5.9) in the text with \\(\\lambda = 0\\) and \\(y_i\\) replaced with \\(\\hat y_i\\) . On the one hand, the solution is trivially seen as \\(\\hat f(x_i)=\\hat y_i\\) , i.e., \\(\\hat f = \\boldsymbol{\\hat y}\\) . On the other hand, since \\(\\hat f = \\textbf{S}\\boldsymbol{\\hat y}\\) solves (5.9) (or equivalently (5.18)), we know that \\(\\textbf{S}\\boldsymbol{\\hat y}=\\boldsymbol{\\hat y}\\) . For local linear regression (see (6.8) in the text), we have \\[\\begin{equation} \\textbf{S} = \\textbf{X}(\\textbf{X}^T\\textbf{W}\\textbf{X})^{-1}\\textbf{X}^T\\textbf{W},\\non \\end{equation}\\] so that \\[\\begin{eqnarray} \\textbf{S}\\boldsymbol{\\hat y} &=& \\textbf{X}(\\textbf{X}^T\\textbf{W}\\textbf{X})^{-1}\\textbf{X}^T\\textbf{W}\\textbf{X}(\\bX^T\\bX)^{-1}\\bX^T\\by\\non\\\\ &=&\\textbf{X}(\\bX^T\\bX)^{-1}\\bX^T\\by\\non\\\\ &=&\\boldsymbol{\\hat y}.\\non \\end{eqnarray}\\] The second step in (2) of Algorithm 9.1 is not needed, since the smoothing spline fit to a mean-zero response has mean zero.","title":"Ex. 9.1"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-2/","text":"Ex. 9.2 Let \\(\\bb{A}\\) be a known \\(k \\times k\\) matrix, \\(\\bb{b}\\) be a matrix known \\(k\\) -vector, and \\(\\bb{z}\\) be an unknown \\(k\\) -vector. A Gauss-Seidel algorithm for solving the linear system of equations \\(\\bb{A}\\bb{z} = \\bb{b}\\) works by successively solving for element \\(z_j\\) in the \\(j\\) th equation, fixing all other \\(z_j\\) 's at their current guesses. This process is repeated for \\(j=1,2,...,k,1,2,...,k,...,\\) until convergence ( Matrix computations ). (a) Consider an additive model with \\(N\\) observations and \\(p\\) terms, with the \\(j\\) th term to be fit by a linear smoother \\(\\bb{S}_j\\) . Consider the following system of equations: \\[\\begin{equation} \\begin{pmatrix} \\bb{I} & \\bb{S}_1 & \\bb{S}_1 & \\cdots & \\bb{S}_1 \\\\ \\bb{S}_2 & \\bb{I} & \\bb{S}_2 & \\cdots & \\bb{S}_2 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bb{S}_p & \\bb{S}_p & \\bb{S}_p & \\cdots & \\bb{I} \\end{pmatrix} \\begin{pmatrix} \\bb{f}_1\\\\ \\bb{f}_2\\\\ \\vdots \\\\ \\bb{f}_p \\end{pmatrix} = \\begin{pmatrix} \\bb{S}_1\\by\\\\ \\bb{S}_2\\by\\\\ \\vdots\\\\ \\bb{S}_p\\by \\end{pmatrix}.\\non \\end{equation}\\] Here each \\(\\bb{f}_j\\) is an \\(N\\) -vector of evaluations of the \\(j\\) th function at the data points, and \\(\\by\\) is an \\(N\\) -vector of the response values. Show that backfitting is a blockwise Gauss-Seidel algorithm for solving system of equations. (b) Let \\(\\bb{S}_1\\) and \\(\\bb{S}_2\\) be symmetric smoothing operators (matrices) with eigenvalues in \\([0,1)\\) . Consider a backfitting algorithm with response vector \\(\\by\\) and smoothers \\(\\bb{S}_1\\) , \\(\\bb{S}_2\\) . Show that with any starting values, the algorithm converges and give a formula for the final iterates. Soln. 9.2 (a) For \\(j\\) -th equation, we have \\[\\begin{equation} \\bb{S}_j\\bb{f}_1 + ... \\bb{S}_j\\bb{f}_{j-1} + \\bb{f}_j + ... + \\bb{S}_j\\bb{f}_p = \\bb{S}_j\\by,\\non \\end{equation}\\] so that \\[\\begin{equation} \\bb{f}_j = \\bb{S}_j\\left(\\by - \\sum_{k\\neq j}\\bb{f}_k\\right)\\non \\end{equation}\\] which has the same as the second step in Algorithm 9.1. Therefore we can regard backfitting as a blockwise Gauss-Seidel algorithm. (b) Denote \\(\\bb{f}_1(k)\\) and \\(\\bb{f}_2(k)\\) as the value for the \\(k\\) -th iteration with initial values \\(\\bb{f}_1(0)\\) and \\(\\bb{f}_2(0)\\) , we have \\[\\begin{eqnarray} \\bb{f}_1(k) &=& \\bb{S}_1(\\by-\\bb{f}_2(k-1))\\non\\\\ \\bb{f}_2(k) &=& \\bb{S}_2(\\by-\\bb{f}_1(k-1)).\\non \\end{eqnarray}\\] Therefore it's easy to derive \\[\\begin{eqnarray} \\bb{f}_2(k) &=& \\bb{S}_2\\bb{S}_1\\bb{f}_2(k-1) + \\bb{S}_2\\by-\\bb{S}_2\\bb{S}_1\\by\\non\\\\ &=&(\\bb{S}_2\\bb{S}_1)^2\\bb{f}_2(k-2) + (\\bb{S}_2\\bb{S}_1+\\bb{I})(\\bb{S}_2\\by-\\bb{S}_2\\bb{S}_1\\by)\\non\\\\ &=&\\cdots\\non\\\\ &=&(\\bb{S}_2\\bb{S}_1)^k\\bb{f}_2(0) + [(\\bb{S}_2\\bb{S}_1)^{k-1} + \\cdots + \\bb{S}_2\\bb{S}_1+\\bb{I}](\\bb{S}_2\\by-\\bb{S}_2\\bb{S}_1\\by).\\non \\end{eqnarray}\\] Thus, as \\(k\\ra\\infty\\) , we have \\(\\bb{f}_2(k)\\) converges to \\[\\begin{equation} (\\bb{I}-\\bb{S}_2\\bb{S}_1)^{-1}(\\bb{S}_2-\\bb{S}_2\\bb{S}_1)\\by.\\non \\end{equation}\\] Similarly for \\(\\bb{f}_1(k)\\) , it converges to \\[\\begin{equation} (\\bb{I}-\\bb{S}_1\\bb{S}_2)^{-1}(\\bb{S}_1-\\bb{S}_1\\bb{S}_2)\\by.\\non \\end{equation}\\]","title":"Ex. 9.2"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-3/","text":"Ex. 9.3 Backfitting equations . Consider a backfitting procedure with orthogonal projections, and let \\(\\bb{D}\\) be the overall regression matrix whose columns span \\(V=\\mathcal{L}_{\\text{col}}(\\bb{S}_1)\\oplus \\mathcal{L}_{\\text{col}}(\\bb{S}_2)\\oplus\\cdots\\oplus \\mathcal{L}_{\\text{col}}(\\bb{S}_p)\\) , where \\(\\mathcal{L}_{\\text{col}}(\\bb{S})\\) denotes the column space of a matrix \\(\\bb{S}\\) . Show that the estimating equations \\[\\begin{equation} \\begin{pmatrix} \\bb{I} & \\bb{S}_1 & \\bb{S}_1 & \\cdots & \\bb{S}_1 \\\\ \\bb{S}_2 & \\bb{I} & \\bb{S}_2 & \\cdots & \\bb{S}_2 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\bb{S}_p & \\bb{S}_p & \\bb{S}_p & \\cdots & \\bb{I} \\end{pmatrix} \\begin{pmatrix} \\bb{f}_1\\\\ \\bb{f}_2\\\\ \\vdots \\\\ \\bb{f}_p \\end{pmatrix} = \\begin{pmatrix} \\bb{S}_1\\by\\\\ \\bb{S}_2\\by\\\\ \\vdots\\\\ \\bb{S}_p\\by \\end{pmatrix}\\non \\end{equation}\\] are equivalent to the least squares normal equations \\(\\bb{D}^T\\bb{D}\\beta = \\bb{D}^T\\by\\) where \\(\\beta\\) is the vector of coefficients. Soln. 9.3 Recall \\(\\bb{S}_j\\) has the form \\(\\bb{N}_j(\\bb{N}_j^T\\bb{N}_j)^{-1}\\bb{N}_j^T\\) where the design matrix \\(\\bb{N}_j\\) are generated by basis spline functions. We write \\(\\bb{D}=(\\bb{N}_1, \\bb{N}_2,..., \\bb{N}_p)\\) . The estimating equation above can be rewritten as \\[\\begin{equation} \\bb{f}_j + \\bb{S}_j\\left(\\sum_{k\\neq j}\\bb{f}_k\\right) = \\bb{S}_j\\by.\\non \\end{equation}\\] Plug in \\(\\bb{f}_j = \\bb{N}_j\\beta_j\\) and \\(\\bb{S}_j=\\bb{N}_j(\\bb{N}_j^T\\bb{N}_j)^{-1}\\bb{N}_j^T\\) and left multiply \\(\\bb{N}_j^T\\) , we obtain \\[\\begin{equation} (\\bb{N}_j^T\\bb{N}_j)\\beta_j + \\bb{N}_j^T\\sum_{k\\neq j}\\bb{N}_k\\beta_k = \\bb{N}_j^T\\by,\\non \\end{equation}\\] which is \\[\\begin{equation} \\bb{N}_j^T\\left(\\sum_{k}\\bb{N}_k\\beta_k\\right)=\\bb{N}_j^T\\by\\non \\end{equation}\\] equivalent to \\(\\bb{D}^T\\bb{D}\\beta = \\bb{D}^T\\by\\) .","title":"Ex. 9.3"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-4/","text":"Ex. 9.4 Suppose the same smoother \\(\\bb{S}\\) is used to estimate both terms in a two-term additive model (i.e., both variables are identical). Assume that \\(\\bb{S}\\) is symmetric with eigenvalues in \\([0,1)\\) . Show that the backfitting residual converges to \\((\\bb{I} + \\bb{S})^{-1}(\\bb{I}-\\bb{S})\\by\\) , and that the residual sum of squares converges upward. Can the residual sum of squares converge upward in less structured situations? How does this fit compare to the fit with a single term fit by \\(\\bb{S}\\) ? [ Hint : Use the eigen-decomposition of \\(\\bb{S}\\) to help with this comparison.] Soln. 9.4 This follows directly from Ex. 9.2 , where the fitted values are both shown to be \\((\\bb{I}-\\bb{S}^2)^{-1}(\\bb{S}-\\bb{S}^2)\\by\\) . Then, the residual is \\[\\begin{eqnarray} \\by - 2(\\bb{I}-\\bb{S}^2)^{-1}(\\bb{S}-\\bb{S}^2)\\by &=&(\\bb{I}-\\bb{S}^2)^{-1}[(\\bb{I}-\\bb{S}^2)\\by - 2(\\bb{S}-\\bb{S}^2)\\by]\\non\\\\ &=&(\\bb{I}-\\bb{S}^2)^{-1}(\\bb{I} - \\bb{S})^2\\by\\non\\\\ &=&(\\bb{I}+\\bb{S})^{-1}(\\bb{I}-\\bb{S})\\by.\\non \\end{eqnarray}\\] Consider the eigen-decomposition of \\(\\bb{S}\\) (e.g., (5.19) in the text), \\[\\begin{equation} \\bb{S} = \\sum_{k=1}^N\\rho_k\\bb{u}_k\\bb{u}_k^T,\\non \\end{equation}\\] with \\(\\rho_k\\in [0,1)\\) . Then the residual can be rewritten as \\[\\begin{equation} \\sum_{k=1}^N\\bb{u}_k\\frac{1-\\rho_k}{1+\\rho_k}\\bb{u}_k^T\\by.\\non \\end{equation}\\] If we use a single term fit by \\(\\bb{S}\\) , the residual is simply \\((\\bb{I}-\\bb{S})\\by\\) and is rewritten as \\[\\begin{equation} \\sum_{k=1}^N\\bb{u}_k(1-\\rho_k)\\bb{u}_k^T\\by.\\non \\end{equation}\\] We see that two-term fit has less residuals compared to one-term fit, which is expected intuitively.","title":"Ex. 9.4"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-5/","text":"Ex. 9.5 Degrees of freedom of a tree . Given data \\(y_i\\) with mean \\(f(x_i)\\) and variance \\(\\sigma^2\\) , and a fitting operation \\(\\by \\rightarrow \\hat{\\by}\\) , let's define the degrees of freedom of a fit by \\(\\sum_{i}\\text{cov}(y_i, \\hat y_i)/\\sigma^2\\) . Consider a fit \\(\\hat{\\by}\\) estimated by a regression tree, fit to a set of predictors \\(X_1, X_2,...,X_p\\) . (a) In terms of the number of terminal nodes \\(m\\) , give a rough formula for the degrees of freedom of the fit. (b) Generate 100 observations with predictors \\(X_1, X_2, ..., X_{10}\\) as independent standard Gaussian variates and fix these values. (c) Generate response values also as standard Gaussian ( \\(\\sigma^2=1\\) ), independent of predictors. Fit regression trees to the data of fixed size 1, 5, and 10 terminal nodes and hence estimate the degrees of the freedom of each fit. [Do ten simulations of the response and average the results, to get a good estimate of degrees of freedom.] (d) Compare your estimates of degrees of freedom in (a) and (c) and discuss. (e) If the regression tree fit were a linear operation, we could write \\(\\hat{\\by} = \\bb{S}\\by\\) for some matrix \\(\\bb{S}\\) . Then the degrees of freedom would be \\(\\text{tr}(\\bb{S})\\) . Suggest a way to compute an approximate \\(\\bb{S}\\) matrix for a regression tree, compute it and compare the resulting degrees of freedom to those in (a) and (c). Soln. 9.5 (a) Suppose we have a single terminal node, i.e., \\(m=1\\) . Then \\(\\hat y_i = \\bar y\\) , so that \\[\\begin{equation} \\sum_{i}\\text{cov}(y_i, \\hat y_i)/\\sigma^2 = n \\frac{\\sigma^2/n}{\\sigma^2} =1.\\non \\end{equation}\\] Consider the case when \\(m=2\\) with two regions \\(R_1\\) and \\(R_2\\) . For \\(i\\in R_1\\) , \\(\\hat y_i = \\frac{1}{|R_1|}\\sum_{i\\in R_1}y_i\\) . Similar for those in \\(R_2\\) . We have \\[\\begin{eqnarray} &&\\sum_{i}\\text{cov}(y_i, \\hat y_i)/\\sigma^2\\non\\\\ &=& \\sum_{i\\in R_1} \\frac{\\sigma^2/|R_1|}{\\sigma^2} + \\sum_{i\\in R_2} \\frac{\\sigma^2/|R_2|}{\\sigma^2}\\non\\\\ &=&2.\\non \\end{eqnarray}\\] Similar arguments apply to the general \\(m\\) , thus we guess the degrees of freedom is \\(m\\) . See On measuring and correcting the effects of data mining and model selection for more details on generalized degrees of freedom (GDF). (b) See Code below. The code and numbers below need revisit. (c) We estimated degrees of freedom to be 1, 5.73 and 10.66 respectively. See Code below. (d) We see that our rough guess of degrees of freedom is close to but underestimate the ones obtained from simulations. (e) In fact, the idea we used in (a) is assuming we are doing a linear operation. When \\(m=1\\) , \\(\\bb{S}\\) is a \\(n\\times n\\) matrix having equal elements \\(1/n\\) . For general \\(m\\) , we have \\(s_{ij} = m/n\\) if \\(i\\) th and \\(j\\) th observations fall into the same region, and \\(s_{ij}=0\\) otherwise. Therefore, \\(\\text{tr}(\\bb{S})=m\\) . There should be other better perspectives on this problem. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np from sklearn.tree import DecisionTreeRegressor np . random . seed ( 42 ) p = 10 mean = np . zeros ( p ) cov = np . identity ( p ) n = 100 X = np . random . multivariate_normal ( mean , cov , n ) N = 20 y = np . random . normal ( 0 , 1 , size = ( n , N )) y_pred = np . copy ( y ) numLeaves = 10 for i in range ( N ): y_cur = y [:, i ] reg = DecisionTreeRegressor ( max_leaf_nodes = numLeaves ) reg . fit ( X , y_cur ) print ( 'Depth: {} , Number of leaves: {} ' . format ( reg . get_depth (), reg . get_n_leaves ())) y_pred_cur = reg . predict ( X ) y_pred [:, i ] = y_pred_cur df = 0 for i in range ( y . shape [ 0 ]): df += np . cov ( y [ i , :], y_pred [ i , :], ddof = 0 )[ 0 ][ 1 ] print ( 'Estimated degrees of freedom with {} terminal nodes is: {} ' . format ( numLeaves , df ))","title":"Ex. 9.5"},{"location":"ESL-Solution/9-Additive-Models-and-Trees/ex9-6/","text":"Ex. 9.6 Consider the ozone data of Figure 6.9. (a) Fit an additive model to the cube root of ozone concentration as a function of temperature, wind speed, and radiation. Compare your results to those obtained via the trellis display in Figure 6.9. (b) Fit trees, MARS, and PRIM to the same data, and compare the results to those found in (a) and in Figure 6.9. Soln. 9.6 We include Figure 1 as an example (similar to Figure 6.9 in the text). It seems that, compared to three-dimensional smoothing example in Figure 6.9, fitted curves from GAM and MARS are more irregular, so does the one from the tree method. Figure 1: Ozone Data Example with GAM, Tree and MARS Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 import pathlib import numpy as np import pandas as pd import prim from pyearth import Earth from pygam import LinearGAM , s from plotly.subplots import make_subplots import plotly.graph_objects as go from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # ozone data data = pd . read_csv ( DATA_PATH . joinpath ( \"ozone.csv\" ), header = 0 ) X = data . loc [:, 'radiation' : 'wind' ] y = pd . DataFrame ( data . loc [:, 'ozone' ]) y [ 'ozone' ] = np . power (( y [ 'ozone' ]), 1 / 3 ) # fit GAM reg_gam = LinearGAM ( s ( 0 ) + s ( 1 ) + s ( 2 )) . fit ( X , y ) # reg_gam.summary() y_pred_gam = reg_gam . predict ( X ) print ( 'MSE for GAM is {:.2f} ' . format ( mean_squared_error ( y_pred_gam , y ))) # fit tree reg_tree = DecisionTreeRegressor ( max_leaf_nodes = 5 ) reg_tree . fit ( X , y ) y_pred_tree = reg_tree . predict ( X ) print ( 'MSE for tree is {:.2f} ' . format ( mean_squared_error ( y_pred_tree , y ))) # fit MARS reg_mars = Earth () reg_mars . fit ( X , y ) # print(reg_mars.summary()) y_pred_mars = reg_mars . predict ( X ) print ( 'MSE for MARS is {:.2f} ' . format ( mean_squared_error ( y_pred_mars , y ))) # plot 4 * 4 data_plot = X data_plot [ 'y' ] = y data_plot [ 'y_pred_gam' ] = y_pred_gam data_plot [ 'y_pred_mars' ] = y_pred_mars data_plot [ 'y_pred_tree' ] = y_pred_tree data_pct = data_plot data_pct = data_pct . sort_values ( 'radiation' ) data_pct = data_pct [ data_pct [ 'wind' ] < data_pct [ 'wind' ] . quantile ( 0.75 )] data_pct = data_pct [ data_pct [ 'temperature' ] < data_pct [ 'temperature' ] . quantile ( 0.75 )] # Create traces fig = go . Figure () fig . add_trace ( go . Scatter ( x = data_pct [ 'radiation' ], y = data_pct [ 'y' ], mode = 'markers' , marker = dict ( color = 'LightSkyBlue' , size = 20 ), name = 'raw' )) fig . add_trace ( go . Scatter ( x = data_pct [ 'radiation' ], y = data_pct [ 'y_pred_gam' ], mode = 'lines' , name = 'GAM' )) fig . add_trace ( go . Scatter ( x = data_pct [ 'radiation' ], y = data_pct [ 'y_pred_tree' ], mode = 'lines' , name = 'Tree' )) fig . add_trace ( go . Scatter ( x = data_pct [ 'radiation' ], y = data_pct [ 'y_pred_mars' ], mode = 'lines' , name = 'MARS' )) fig . update_layout ( xaxis_title = \"radiation\" , yaxis_title = \"Cubic Root Ozone\" , ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) fig . show ()","title":"Ex. 9.6"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-01/","text":"Ex. 10.1 Derive expression (10.12) for the update parameter in AdaBoost. Soln. 10.1 Plug \\(G_m\\) in (10.11) into (10.9), and solve for \\(\\beta\\) by taking its partial derivative and then setting it to be zero, we get \\[\\begin{equation} \\sum_{i=1}^Nw_i^{(m)}y_iG(x_i)\\exp(-\\beta y_i G(x_i)) = 0,\\non \\end{equation}\\] which is \\[\\begin{equation} \\sum_{y_i = G(x_i)}w_i^{(m)}\\exp(-\\beta) - \\sum_{y_i \\neq G(x_i)}w_i^{(m)}\\exp(\\beta) = 0.\\non \\end{equation}\\] Multiplying \\(\\exp{(\\beta)}\\) on both sides and by a little algebra we get \\[\\begin{eqnarray} \\exp(2\\beta) &=& \\frac{\\sum_{y_i = G(x_i)}w_i^{(m)}}{\\sum_{y_i \\neq G(x_i)}w_i^{(m)}}\\non\\\\ &=&\\frac{1-\\text{err}_m}{\\text{err}_m},\\non \\end{eqnarray}\\] where \\(\\text{err}_m\\) is the minimized weighted error rate \\[\\begin{equation} \\text{err}_m = \\frac{\\sum_{i=1}^Nw_i^{(m)}\\bb{1}(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N w_i^{(m)}}.\\non \\end{equation}\\] Therefore, we get (10.12) below \\[\\begin{equation} \\beta = \\frac{1}{2}\\log \\frac{1-\\text{err}_m}{\\text{err}_m}.\\non \\end{equation}\\]","title":"Ex. 10.1"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-02/","text":"Ex. 10.2 Prove result (10.16), that is, the minimizer of the population version of the AdaBoost criterion, is one-half of the log odds. Soln. 10.2 We are looking for \\[\\begin{equation} \\underset{f(x)}{\\operatorname{argmin}}E_{Y|x}(e^{-Yf(x)})\\non \\end{equation}\\] where \\(Y\\in \\{1, -1\\}\\) . Denoting \\(z = f(x)\\) , we have \\[\\begin{equation} \\underset{z}{\\operatorname{argmin}}P(Y=1|x)e^{-z} + P(Y=-1|x)e^{z}\\non \\end{equation}\\] Similarly as Ex. 10.1 , we take derivative w.r.t. \\(z\\) and set it to 0 \\[\\begin{equation} P(Y=1|x)e^{-z} + P(Y=-1|x)e^{z} = 0.\\non \\end{equation}\\] Solving for \\(z = f(x)\\) we get \\[\\begin{equation} f(x) = \\frac{1}{2}\\log{\\frac{P(Y=1|x)}{P(Y=-1|x)}},\\non \\end{equation}\\] that is, one-half of the log odds.","title":"Ex. 10.2"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-03/","text":"Ex. 10.3 Show that the marginal average (10.47) recovers additive and multiplicative functions (10.50) and (10.51), while the conditional expectation (10.49) does not. Soln. 10.3 The marginal average (10.47) is defined as \\[\\begin{equation} f_S(X_S) = E_{X_C}f(X_S, X_C).\\non \\end{equation}\\] Note that it's different from the conditional expectation (10.49) \\[\\begin{equation} \\label{eq:10cond} \\tilde f_X(X_S) = E[f(X_S, X_C)|X_S]. \\end{equation}\\] Assuming the marginal density for \\(X_C\\) is \\(\\phi\\) . When \\(f(X) = h_1(X_S) + h_2(X_C)\\) , we have \\[\\begin{eqnarray} f_S(X_S) &=& \\int f(X_S, c)\\phi(c)dc\\non\\\\ &=&\\int [h_1(X_S) + h_2(c)]\\phi(c)dc\\non\\\\ &=&\\int h_1(X_S)\\phi(c)dc + \\int h_2(c)\\phi(c)dc\\non\\\\ &=&h_1(X_S)\\int\\phi(c)dc + \\int h_2(c)\\phi(c)dc\\non\\\\ &=&h_1(X_S) + \\int h_2(c)\\phi(c)dc\\non \\end{eqnarray}\\] where the last equation comes by noting \\(\\int \\phi(c)dc = 1\\) . Similar arguments apply to \\(f(X) = h_1(X_S)\\cdot h_2(X_C)\\) . However for the conditional expectation \\(\\eqref{eq:10cond}\\) , when \\(f(X) = h_1(X_S) + h_2(X_C)\\) we get \\[\\begin{equation} \\tilde f_S(X_S) = h_1(X_S) + E[h_2(X_C)|X_S].\\non \\end{equation}\\] When \\(f(X) = h_1(X_S)\\cdot h_2(X_C)\\) , we get \\[\\begin{equation} \\tilde f_S(X_S) = h_1(X_S)\\cdot E[h_2(X_C)|X_S].\\non \\end{equation}\\]","title":"Ex. 10.3"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-04/","text":"Ex. 10.4 (a) Write a program implementing AdaBoost with trees. (b) Redo the computations for the example of Figure 10.2. Plot the training error as well as test error, and discuss its behavior. (c) Investigate the number of iterations needed to make the test error finally start to rise. (d) Change the setup of this example as follows: define two classes, with the features in Class 1 being \\(X_1, X_2, ..., X_{10}\\) , standard independent Gaussian variates. In Class 2, the features \\(X_1, X_2,...,X_{10}\\) are also standard independent Gaussian, but conditioned on the event \\(\\sum_{j} X_j^2 > 12\\) . Now the classes have significant overlap in feature space. Repeat the AdaBoost experiments as in Figure 10.2 and discuss the results. Soln. 10.4 As shown in Figure 1, the train and test errors decrease as the number of iterations increases. For train error, it becomes close to zero when the number of iterations exceeds ~150. For test error, it stablizes at ~0.07 when the number of iterations exceeds 300. I didn't observe a rise on test error in my simulation, where the largest number of iterations is 800. Figure 1: Train and Test Errors of AdaBoost with Trees Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import numpy as np from sklearn.ensemble import AdaBoostClassifier import plotly.graph_objects as go p = 10 mean = np . zeros ( p ) cov = np . identity ( p ) def classify ( X ): if np . linalg . norm ( X ) > np . sqrt ( 9.34 ): return 1 else : return - 1 n_train = 1000 X_train = np . random . multivariate_normal ( mean , cov , n_train ) y_train = np . apply_along_axis ( classify , 1 , X_train ) n_test = 10000 X_test = np . random . multivariate_normal ( mean , cov , n_test ) y_test = np . apply_along_axis ( classify , 1 , X_test ) num_iterations = 800 test_errors = [] train_errors = [] for i in range ( num_iterations ): i += 800 clf = AdaBoostClassifier ( n_estimators = i + 1 , random_state = 0 ) clf . fit ( X_train , y_train ) train_error = 1 - clf . score ( X_train , y_train ) train_errors . append ( train_error ) test_error = 1 - clf . score ( X_test , y_test ) test_errors . append ( test_error ) print ( 'Num. of Iteration is {} , Train Error is: {} , Test Error is: {} ' . format ( i + 1 , train_error , test_error )) # Create traces fig = go . Figure () fig . add_trace ( go . Scatter ( x = np . arange ( num_iterations ), y = np . asarray ( train_errors ), mode = 'lines' , name = 'Train error' )) fig . add_trace ( go . Scatter ( x = np . arange ( num_iterations ), y = np . asarray ( test_errors ), mode = 'lines' , name = 'Test error' )) fig . update_layout ( xaxis_title = \"Boosting Iterations\" , yaxis_title = \"Train and Test Error\" , ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) fig . show ()","title":"Ex. 10.4"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-05/","text":"Ex. 10.5 Multiclass exponential loss ( Multi-class adaboost ). For a \\(K\\) -class classification problem, consider the coding \\(Y=(Y_1,...,Y_K)^T\\) with \\[\\begin{equation} Y_k = \\begin{cases} 1, & \\text{ if } G=\\mathcal{G}_k\\\\ -\\frac{1}{K-1}, & \\text{ otherwise. } \\end{cases}\\non \\end{equation}\\] Let \\(f=(f_1,...,f_K)^T\\) with \\(\\sum_{k=1}^Kf_k=0\\) , and define \\[\\begin{equation} L(Y,f) = \\exp\\left(-\\frac{1}{K}Y^Tf\\right).\\non \\end{equation}\\] (a) Using Lagrange multipliers, derive the population minimizer \\(f^\\ast\\) of \\(L(Y, f)\\) , subject to the zero-sum constraint, and relate these to the class probabilities. (b) Show that a multiclass boosting using this loss function leads to a reweighting algorithm similar to AdaBoost, as in Section 10.4. Soln. 10.5 (a) We follow the similar arguments in Ex. 10.2 . We're interested in \\[\\begin{equation} \\underset{f(x)}{\\operatorname{argmin}} E_{Y|x} \\left[\\exp\\left(-\\frac{1}{K}\\big(Y_1f_1(x) + ... + Y_Kf_K(x)\\big)\\right)\\right]\\non \\end{equation}\\] subject to \\[\\begin{equation} f_1(x) + ... + f_K(x) = 0.\\non \\end{equation}\\] The Lagrange multiplier, denoted as \\(H(f_1,...,f_k,\\lambda)\\) , becomes \\[\\begin{eqnarray} &&P(Y=1|x)\\cdot \\exp\\left(-\\frac{1}{K}f_1(x)-\\frac{1}{K-1}(f_2(x) +...+f_K(x))\\right)\\non\\\\ &+&P(Y=2|x)\\cdot \\exp\\left(-\\frac{1}{K}f_2(x)-\\frac{1}{K-1}(f_1(x) + f_3(x)...+f_K(x))\\right)\\non\\\\ &+&...\\non\\\\ &+&P(Y=K|x)\\cdot \\exp\\left(-\\frac{1}{K}f_K(x)-\\frac{1}{K-1}(f_1(x) + f_2(x)...+f_{K-1}(x))\\right)\\non\\\\ &-&\\lambda (f_1(x) +...+f_K(x)) \\label{eq:10.5multi} \\end{eqnarray}\\] Note that \\(f_1(x) + ... + f_K(x) = 0\\) , so \\(\\eqref{eq:10.5multi}\\) simplifies to \\[\\begin{eqnarray} &&P(Y=1|x)\\cdot \\exp\\left(-\\frac{1}{K-1}f_1(x)\\right)\\non\\\\ &+&P(Y=2|x)\\cdot \\exp\\left(-\\frac{1}{K-1}f_2(x)\\right)\\non\\\\ &+&...\\non\\\\ &+&P(Y=K|x)\\cdot \\exp\\left(-\\frac{1}{K-1}f_K(x)\\right)\\non\\\\ &-&\\lambda (f_1(x) +...+f_K(x)). \\end{eqnarray}\\] For \\(k=1, ..., K\\) , setting \\(\\frac{\\partial H}{\\partial f_k(x)} = 0\\) and \\(\\frac{\\partial H}{\\partial \\lambda} = 0\\) yields \\[\\begin{eqnarray} -\\frac{1}{K-1}P(Y=k|x)\\cdot \\exp\\left(-\\frac{1}{K-1}f_k(x)\\right) -\\lambda &=& 0, \\text{ for } k =1,...,K\\non\\\\ f_1(x) + ... + f_K(x) &=& 0.\\non \\end{eqnarray}\\] Note that we have \\(K+1\\) equations with \\(K+1\\) unknowns. We first represent \\(f_k(x)\\) in terms of \\(\\lambda\\) by the first set of equations above: \\[\\begin{eqnarray} f^\\ast_k(x) &=&-(K-1)\\log\\left(\\frac{(K-1)\\lambda}{P(Y=k|x)}\\right)\\non\\\\ &=&(K-1)\\log\\left(P(Y=k|x)\\right) - (K-1)\\log((K-1)\\lambda)\\label{eq:fkx} \\end{eqnarray}\\] Then by \\(\\sum_{k=1}^Kf^\\ast_k(x)=0\\) we have \\[\\begin{equation} \\sum_{k=1}^K\\Big((K-1)\\log\\left(P(Y=k|x)\\right) - (K-1)\\log((K-1)\\lambda)\\Big) = 0\\non \\end{equation}\\] so that \\[\\begin{equation} \\label{eq:10lambda} \\lambda = \\frac{1}{K-1}\\prod_{k=1}^KP(Y=k|x)^{\\frac{1}{K}}. \\end{equation}\\] Plug \\(\\lambda\\) above back to \\(\\eqref{eq:fkx}\\) we obtain that for \\(k = 1,...,K\\) , \\[\\begin{equation} f^{\\ast}_k(x) = (K-1)\\log\\left(P(Y=k|x)\\right) - \\frac{K-1}{K}\\sum_{\\tilde k=1}^K\\log P(Y=\\tilde k|x).\\non \\end{equation}\\] Once \\(f^\\ast_k(x)\\) are estimated for \\(k=1,...,K\\) , we are able to estimate \\(P(Y=k|x)\\) in terms of \\(f^\\ast_k(x)\\) . From \\(\\eqref{eq:fkx}\\) and \\(\\eqref{eq:10lambda}\\) , we obtain for \\(k=1,...,K\\) , \\[\\begin{equation} \\label{eq:pyk} P(Y=k|x) = \\exp\\left(\\frac{f^\\ast_k(x)}{K-1}\\right) \\cdot \\left(\\prod_{j=1}^KP(Y=j|x)^{\\frac{1}{K}}\\right). \\end{equation}\\] Summing \\(P(Y=k|x)\\) for \\(k=1,...,K\\) we obtain \\[\\begin{equation} 1 = \\left(\\prod_{j=1}^KP(Y=j|x)^{\\frac{1}{K}}\\right) \\cdot \\Bigg(\\sum_{k=1}^K\\exp\\left(\\frac{f^\\ast_k(x)}{K-1}\\right)\\Bigg),\\non \\end{equation}\\] thus \\[\\begin{equation} \\left(\\prod_{j=1}^KP(Y=j|x)^{\\frac{1}{K}}\\right) = \\Bigg(\\sum_{k=1}^K\\exp\\left(\\frac{f^\\ast_k(x)}{K-1}\\right)\\Bigg)^{-1}. \\end{equation}\\] Plugging equation above back to \\(\\eqref{eq:pyk}\\) we get \\[\\begin{equation} P(Y=k|x) = \\frac{\\exp\\left(\\frac{f^\\ast_k(x)}{K-1}\\right)}{\\sum_{k=1}^K\\exp\\left(\\frac{f^\\ast_k(x)}{K-1}\\right)}.\\non \\end{equation}\\] (b) Following the idea of two-class AdaBoost, we start with a Stagewise Additive Modeling using a Multi-class Exponential loss function (SAMME). SAMME Note that SAMME shares the same simple modular structure of AdaBoost with a simple but subtle different in (c), specifically, the extra term \\(\\log(K-1)\\) . The link between exponential loss function and SAMME follows the same arguments in Section 10.4. Specifically, we have as (10.12), \\[\\begin{equation} \\beta_m = \\frac{(K-1)^2}{K}\\left(\\log\\frac{1-\\text{err}_m}{\\text{err}_m} + \\log (K-1)\\right).\\non \\end{equation}\\]","title":"Ex. 10.5"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-06/","text":"Ex. 10.6 McNemar test ( An Introduction to Categorical Data Analysis ). We support the test error rates on the spam data to be 5.5% for a generalized additive model (GAM), and 4.5% for gradient boosting (GBM), with a test sample of size 1536. (a) Show that the standard error of these estimates is about 0.6%. Since the same test data are used for both methods, the error rates are correlated, and we cannot perform a two-sample t-test. We can compare the methods directly on each test observation, leading to the summary in Table below. GAM GBM Correct Error Correct 1434 18 Error 33 51 The McNemar test focuses on the discordant errors, 33 vs. 18. (b) Conduct a test to show that GAM makes significantly more errors than gradient boosting, with a two-sided \\(p\\) -value of 0.036. Soln. 10.6 (a) The standard error for a binomial estimate is \\(\\sqrt{p(1-p)/n}\\) . It's straightforward to verify that \\[\\begin{eqnarray} \\sqrt{\\frac{0.055\\cdot(1-0.055)}{1536}} \\approx 0.6\\%\\non\\\\ \\sqrt{\\frac{0.045\\cdot(1-0.045)}{1536}} \\approx 0.6\\%\\non \\end{eqnarray}\\] (b) The McNemar test statistic is \\[\\begin{equation} \\chi^2 = \\frac{(33-18)^2}{33+18}\\approx 4.41,\\non \\end{equation}\\] which has a 1 degree of freedom, thus the \\(p-\\) value is 0.036. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from statsmodels.stats.contingency_tables import mcnemar # define contingency table table = [[ 1434 , 18 ], [ 33 , 51 ]] result = mcnemar ( table , exact = False , correction = False ) print ( 'statistic= %.3f , p-value= %.3f ' % ( result . statistic , result . pvalue )) # interpret the p-value alpha = 0.05 if result . pvalue > alpha : print ( 'Same proportions of errors (fail to reject H0)' ) else : print ( 'Different proportions of errors (reject H0)' )","title":"Ex. 10.6"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-07/","text":"Ex. 10.7 Derive expression (10.32). Soln. 10.7 We are looking for the optimum of \\[\\begin{eqnarray} \\hat \\gamma_{jm} &=& \\underset{\\gamma_{jm}}{\\operatorname{argmin}}\\sum_{x_i\\in R_{jm}}e^{-y_if_{m-1}(x_i)-y_i\\gamma_{jm}}\\non\\\\ &=&\\underset{\\gamma_{jm}}{\\operatorname{argmin}}\\sum_{x_i\\in R_{jm}}w_i^{(m)}e^{-y_i\\gamma_{jm}},\\non \\end{eqnarray}\\] where \\(w_i^{(m)} = e^{-y_if_{m-1}(x_i)}\\) . Let \\[\\begin{equation} F(\\gamma_{jm}) = \\sum_{x_i\\in R_{jm}}w_i^{(m)}e^{-y_i\\gamma_{jm}}.\\non \\end{equation}\\] We have \\[\\begin{eqnarray} \\frac{\\partial F}{\\partial \\gamma_{jm}} &=& \\sum_{x_i\\in R_{jm}}w_i^{(m)}e^{-y_i\\gamma_{jm}}\\cdot (-y_i)\\non\\\\ &=&-\\sum_{x_i\\in R_{jm}, y_i = 1}w_i^{(m)}e^{-\\gamma_{jm}} + \\sum_{x_i\\in R_{jm}, y_i = -1}w_i^{(m)}e^{\\gamma_{jm}}.\\non \\end{eqnarray}\\] Setting it to be zero and solve for \\(\\gamma_{jm}\\) we obtain \\[\\begin{equation} \\hat\\gamma_{jm} = \\frac{1}{2}\\log \\frac{\\sum_{x_i\\in R_{jm}}w_i^{(m)}\\bb{1}(y_i=1)}{\\sum_{x_i\\in R_{jm}}w_i^{(m)}\\bb{1}(y_i=-1)}.\\non \\end{equation}\\]","title":"Ex. 10.7"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-08/","text":"Ex. 10.8 Consider a \\(K\\) -class problem where the targets \\(y_{ik}\\) are coded as 1 if observation \\(i\\) is in class \\(k\\) and zero otherwise. Suppose we have a current model \\(f_k(x), k=1,...,K\\) , with \\(\\sum_{k=1}^Kf_k(x)=0\\) (see (10.21) in Section 10.6). We wish to update the model for observations in a region \\(R\\) in predictor space, by adding constants \\(f_k(x)+\\gamma_k\\) , with \\(\\gamma_K=0\\) . (a) Write down the multinomial log-likelihood for this problem, and its first and second derivatives. (b) Using only the diagonal of the Hessian matrix in (a), and starting from \\(\\gamma_k=0 \\forall k\\) , show that a one-step approximate Newton update for \\(\\gamma_k\\) is \\[\\begin{equation} \\gamma_k^1 = \\frac{\\sum_{x_i\\in R}(y_{ik}-p_{ik})}{\\sum_{x_i\\in R}p_{ik}(1-p_{ik})}, k =1,...,K-1,\\non \\end{equation}\\] where \\(p_{ik}=\\exp(f_k(x_i))/\\exp(\\sum_{l=1}^Kf_l(x_i))\\) . (c) We prefer our update to sum to zero, as the current model does. Using symmetry arguments, show that \\[\\begin{equation} \\hat{\\gamma}_k = \\frac{K-1}{K}(\\gamma_k^1 - \\frac{1}{K}\\sum_{l=1}^K\\gamma_l^1), k= 1,...,K \\non \\end{equation}\\] is an appropriate update, where \\(\\gamma_k^1\\) is defined as in (10.57) for all \\(k=1,...,K\\) . Soln. 10.8 (a) The multinomial log-likelihood for a single training sample is \\[\\begin{equation} L(y, p(x), \\gamma) = \\sum_{k=1}^K\\bb{1}(y=G_k)(f_k(x) + \\gamma_k) - \\log\\left(\\sum_{j=1}^Ke^{f_j(x)+\\gamma_j}\\right).\\non \\end{equation}\\] For \\(k=1,...,K-1\\) , we have \\[\\begin{equation} \\frac{\\partial L(y, p(x),\\gamma)}{\\partial \\gamma_k} = \\sum_{k=1}^K\\bb{1}(y=G_k) - \\frac{e^{f_k(x)+\\gamma_k}}{\\sum_{j=1}^K e^{f_j(x)+\\gamma_j}},\\non \\end{equation}\\] and \\[\\begin{equation} \\frac{\\partial^2 L(y, p(x),\\gamma)}{\\partial \\gamma_k\\partial \\gamma_k} = \\frac{e^{f_k(x)+\\gamma_k}}{\\sum_{j=1}^K e^{f_j(x)+\\gamma_j}} - \\frac{e^{2f_k(x)+2\\gamma_k}}{(\\sum_{j=1}^K e^{f_j(x)+\\gamma_j})^2}.\\non \\end{equation}\\] Note that this is the diagonal of the Hessian matrix. For \\(k\\neq k'\\in \\{1,...,K-1\\}\\) , we have \\[\\begin{equation} \\frac{\\partial^2 L(y, p(x),\\gamma)}{\\partial \\gamma_k\\partial \\gamma_k'} = - \\frac{e^{f_k(x)+\\gamma_k}e^{f_{k'}(x)+\\gamma_{k'}}}{(\\sum_{j=1}^K e^{f_j(x)+\\gamma_j})^2}.\\non \\end{equation}\\] (b) By Newton's method we have \\[\\begin{equation} \\gamma_k^1 = \\gamma_k^0 - \\frac{\\partial L(y, p(x),\\gamma_k^0)}{\\partial \\gamma_k}\\Bigg/\\frac{\\partial^2 L(y, p(x),\\gamma_k^0)}{\\partial \\gamma_k\\partial \\gamma_k}.\\non \\end{equation}\\] Note that we need to sum over all \\(x_i \\in R\\) for first and second derivatives obtained in (a). Therefore we get \\[\\begin{eqnarray} \\gamma_k^1 &=& \\gamma_k^0 - \\sum_{x_i\\in R}\\frac{\\partial L(y, p(x_i),\\gamma_k^0)}{\\partial \\gamma_k}\\Bigg/\\sum_{x_i\\in R}\\frac{\\partial^2 L(y, p(x_i),\\gamma_k^0)}{\\partial \\gamma_k\\partial \\gamma_k}\\non\\\\ &=& 0 - \\sum_{x_i\\in R}\\left(\\sum_{k=1}^K\\bb{1}(y_i=G_k) - \\frac{e^{f_k(x_i)}}{\\sum_{j=1}^K e^{f_j(x_i)}}\\right)\\Bigg/\\left(\\frac{e^{f_k(x_i)}}{\\sum_{j=1}^K e^{f_j(x_i)}} - \\frac{e^{2f_k(x_i)}}{(\\sum_{j=1}^K e^{f_j(x_i)})^2}\\right)\\non\\\\ &=&-\\frac{\\sum_{x_i\\in R}(y_{ik}-p_{ik})}{\\sum_{x_i\\in R}p_{ik}(1-p_{ik})}\\non \\end{eqnarray}\\] where \\(p_{ik} = e^{f_k(x_i)}/\\sum_{j=1}^Ke^{f_j(x_i)}\\) . (c) Given \\(\\gamma_k^1\\) in part (b), we only need to subtract the average of \\(\\sum_{l=1}^K\\gamma_l^1\\) to each \\(\\gamma_k^1\\) to make them sum up to 0. Thus we have \\[\\begin{equation} \\hat\\gamma_k = \\gamma_k^1-\\frac{1}{K}\\sum_{l=1}^K\\gamma_l^1.\\non \\end{equation}\\] We can, of course, multiply any \\(\\alpha > 0\\) to \\(\\hat\\gamma_k\\) , as the textbook chooses \\(\\frac{K-1}{K}\\) : \\[\\begin{equation} \\hat\\gamma_k = \\frac{K-1}{K}\\left(\\gamma_k^1-\\frac{1}{K}\\sum_{l=1}^K\\gamma_l^1\\right).\\non \\end{equation}\\] It's straightforward to verify that \\(\\sum_{k=1}^K\\hat\\gamma_k=0\\) .","title":"Ex. 10.8"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-09/","text":"Ex. 10.9 Consider a \\(K\\) -class problem where that targets \\(y_{ik}\\) are coded as 1 if observation \\(i\\) is in class \\(k\\) and zero otherwise. Using the multinomial deviance loss function (10.22) and the symmetric logistic transform, use the arguments leading to the gradient boosting Algorithm 10.3 to derive Algorithm 10.4. Hint : See Ex. 10.8 for step 2(b) iii. Soln. 10.9 It's enough to show following items: (1) From (10.21) in the textbook we get Algorithm 10.4 (a). (2) From Table 10.2 and Algorithm 10.3 (a) in the textbook we get Algorithm 10.4 (b) i. (3) From Ex. 10.8 (c) we get Algorithm 10.4 (b) iii. To see that, it suffices to show that \\[\\begin{equation} \\sum_{x_i\\in R_{ikm}}|r_{ikm}|(1-|r_{ikm}|) = \\sum_{x_{ikm}\\in R_{ikm}}p_{k}(x_i)(1-p_k(x_i)).\\non \\end{equation}\\] Note that when \\(y_{ik} = 1\\) , \\(r_{ikm} = 1 - p_k(x_i)\\) and when \\(y_{ik} = 0\\) , \\(r_{ikm} = - p_k(x_i)\\) . Therefore in both cases, we have \\[\\begin{equation} |r_{ikm}|(1-|r_{ikm}|) = p_{k}(x_i)(1-p_k(x_i)).\\non \\end{equation}\\]","title":"Ex. 10.9"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-10/","text":"Ex. 10.10 Show that for \\(K=2\\) class classification, only one tree needs to be grown at each gradient-boosting iteration. Soln. 10.10 For classification the loss function (for a single training sample) is the multinomial deviance \\[\\begin{eqnarray} L(y, p(x)) &=& -\\sum_{k=1}^K\\bb{1}(y=G_k)\\log p_k(x)\\non\\\\ &=&-\\sum_{k=1}^K\\bb{1}(y=G_k)f_k(x) +\\log\\left(\\sum_{l=1}^Ke^{f_l(x)}\\right),\\non \\end{eqnarray}\\] where \\[\\begin{equation} p_k(x) = \\frac{e^{f_k(x)}}{\\sum_{l=1}^Ke^{f_l(x)}}.\\non \\end{equation}\\] Then \\(K\\) least square trees are constructed at each iteration, with each tree is fit to its respective negative gradient \\[\\begin{equation} \\bb{1}(y=G_k) - p_k(x) \\text{ for } k=1,...,K.\\non \\end{equation}\\] When \\(K=2\\) , we have \\(p_1(x) + p_2(x) = 1\\) . When we build the first least square tree \\(T_1\\) , it is fit to \\[\\begin{equation} \\bb{1}(y=G_1) - p_1(x),\\non \\end{equation}\\] which is the negative of the target of the second tree \\(T_2\\) : \\[\\begin{equation} \\bb{1}(y=G_2) - p_2(x).\\non \\end{equation}\\] To see that, note \\[\\begin{equation} \\bb{1}(y=G_1) - p_1(x) = (1-\\bb{1}(y=G_2)) - (1-p_2(x)) = -(\\bb{1}(y=G_2) - p_2(x)).\\non \\end{equation}\\] Therefore, once we build \\(T_1\\) , we can flip the sign and get \\(T_2\\) .","title":"Ex. 10.10"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-11/","text":"Ex. 10.11 Show how to compute the partial dependence function \\(f_{\\mathcal{S}}(X_{\\mathcal{S}})\\) in (10.47) efficiently. Soln. 10.11 In general, to calculate partial dependence as in (10.47), given a \\(x_s\\in X_{\\mathcal{S}}\\) , we need to make \\(N\\) predictions for \\(N\\) samples \\((x_s, x_{i\\mathcal{C}}) (i=1,...,N)\\) and take an average. For decision trees, note that each node of the fitted tree remembers how many training samples went through it during the training, and thus we can use associated ratios to derive the final average. That means, we only need to traverse the tree for once. Please see Efficient Partial Dependence Plots with decision trees for a detailed description and scikit-learn's implementation .","title":"Ex. 10.11"},{"location":"ESL-Solution/_10-Boosting-and-Additive-Trees/ex10-12/","text":"Ex. 10.12 Referring to (10.49), let \\(\\mathcal{S}=\\{1\\}\\) and \\(\\mathcal{C} = \\{2\\}\\) , with \\(f(X_1, X_2)=X_1\\) . Assume \\(X_1\\) and \\(X_2\\) are bivariate Gaussian, each with mean zero, variance one, and \\(E(X_1X_2)=\\rho\\) . Show that \\(E(f(X_1, X_2)|X_2)=\\rho X_2\\) , even though \\(f\\) is not a function of \\(X_2\\) . Soln. 10.12 We need to show that \\[\\begin{equation} E[X_1|X_2] = \\rho X_2.\\non \\end{equation}\\] Note that \\[\\begin{eqnarray} E[(X_1-\\rho X_2)X_2] &=& E[X_1X_2] - \\rho E[X_2^2]\\non\\\\ &=&\\rho - \\rho\\non\\\\ &=&0,\\non \\end{eqnarray}\\] thus \\((X_1-\\rho X_2)\\) and \\(X_2\\) are uncorrelated. Note also that both \\(X_1-\\rho X_2\\) and \\(X_2\\) are jointly normal, it follows that \\(X_1-\\rho X_2\\) and \\(X_2\\) are independent (see, e.g., Section 4.7 in Introduction to probability ). Thus, we have \\[\\begin{eqnarray} E[X_1|X_2] &=& E[X_1-\\rho X_2 + \\rho X_2|X_2]\\non\\\\ &=&E[X_1-\\rho X_2|X_2] +E[\\rho X_2|X_2]\\non\\\\ &=&E[X_1-\\rho X_2] + \\rho X_2\\non\\\\ &=&\\rho X_2.\\non \\end{eqnarray}\\]","title":"Ex. 10.12"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-1/","text":"Ex. 11.1 Establish the exact correspondence between the projection pursuit regression model (11.1) and the neural network (11.5). In particular, show that the single-layer regression network is equivalent to a PPR model with \\(g_m(\\omega_m^Tx) = \\beta_m\\sigma(\\alpha_{0m}+s_m(\\omega_m^Tx))\\) , where \\(\\omega_m\\) is the \\(m\\) th unit vector. Establish a similar equivalence for a classification network. Soln. 11.1 Let \\(K=1\\) , from (11.5) we have \\[\\begin{eqnarray} f(X) &=& g(T) \\non\\\\ &=&g(\\beta_0+\\beta^TZ)\\non\\\\ &=&g(\\beta_0 + \\sum_{m=1}^M\\beta_m\\sigma(\\alpha_{0m} + \\alpha_m^TX)).\\non \\end{eqnarray}\\] Consider \\(\\beta_0\\) added as the bias term into \\(X\\) , and assume as usual that \\(g\\) is the identity function, we have \\[\\begin{equation} f(X) = \\sum_{m=0}^M\\beta_m\\sigma(\\alpha_{0m} + \\alpha_m^TX).\\non \\end{equation}\\] Comparing with (11.1), we have \\[\\begin{equation} g_m(\\omega_m^TX) = \\beta_m\\sigma(\\alpha_{0m} + \\|\\alpha_m\\|(\\omega_m^TX))\\non \\end{equation}\\] where \\(\\omega_m = \\alpha_m/\\|\\alpha_m\\|\\) . For a classification network, let \\(K > 1\\) . Assume that (see (11.6) in the text) \\[\\begin{equation} g_k(T) = \\frac{e^{T_k}}{\\sum_{l=1}^Ke^{T_l}},\\non \\end{equation}\\] by similar calculations above we get \\[\\begin{equation} f_k(X) = \\frac{e^{\\sum_{m=0}^M\\beta_{mk}\\sigma(\\alpha_{0m} + \\|\\alpha_m\\|(\\omega_m^TX))}}{\\sum_{l=1}^Ke^{\\sum_{m=0}^M\\beta_{ml}\\sigma(\\alpha_{0m} + \\|\\alpha_m\\|(\\omega_m^TX))}}.\\non \\end{equation}\\] Note that \\(\\sum_{k=1}^Kf_k(X) = 1\\) . Instead of model \\(f_k(X)\\) , it's more convenient to model the log ratio \\(\\log(f_k(X)/f_K(X))\\) , which is then simplified to \\[\\begin{equation} \\sum_{m=0}^M(\\beta_{km}-\\beta_{Km})\\sigma(\\alpha_{0m} + \\|\\alpha_m\\|(\\omega_m^TX)),\\non \\end{equation}\\] which is in the PPR form of (11.1).","title":"Ex. 11.1"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-2/","text":"Ex. 11.2 Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function \\(g_k(t) = t\\) . Suppose that the weights \\(\\alpha_m\\) from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs. Soln. 11.2 One implicit assumption of this exercise is that the neural network uses sigmoid activation function (actually, any general activation function with the property that it's almost linear near origin). Recall that the sigmoid activation function is \\[\\begin{equation} \\sigma(\\nu) = \\frac{1}{1 + e^{-\\nu}}.\\non \\end{equation}\\] Let \\(\\mu(x) = \\frac{1}{2}(1+x)\\) . Note that \\[\\begin{eqnarray} \\lim_{x\\ra 0}\\mu(x)/\\sigma(x) &=& \\lim_{x\\ra 0}\\frac{1}{2}(1+x)(1+e^{-x})\\non\\\\ &=&\\lim_{x\\ra 0}\\frac{1}{2}\\left(1 + e^{-x} + x + xe^{-x}\\right)\\non\\\\ &=&1.\\non \\end{eqnarray}\\] When \\(\\alpha_m\\) are nearly zero, so are \\(\\alpha_m^TX\\) . Note that if we add bias term into and enlarge \\(X\\) (that is, each row of \\(X\\) has 1 in the first position), then \\(Z_m = \\sigma(\\alpha_m^TX) \\sim \\frac{1}{2}(1 + \\alpha_m^TX)\\) . Therefore the resulting model is nearly linear in \\(X\\) , since \\(g_k\\) is identity.","title":"Ex. 11.2"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-3/","text":"Ex. 11.3 Derive the forward and backward propagation equations for the cross-entropy loss function. Soln. 11.3 For cross-entropy (deviance) we have \\[\\begin{equation} \\label{eq:11-3a} R(\\theta) = -\\sum_{i=1}^N\\sum_{k=1}^Ky_{ik}\\log(f_k(x_i)),\\non \\end{equation}\\] and the corresponding classifier is \\(G(x) = \\underset{k}{\\operatorname{argmax}}f_k(x)\\) . Let \\(z_{mi} = \\sigma(\\alpha_{0m} + \\alpha_m^Tx_i)\\) from (11.5) in the text. Let \\(z_i = (z_{1i}, z_{2i}, ..., z_{Mi})\\) . Then we have \\[\\begin{eqnarray} R(\\theta) &=& \\sum_{i=1}^NR_i\\non\\\\ &=&\\sum_{i=1}^N\\sum_{k=1}^K\\left(-y_{ik}\\log(f_k(x_i))\\right),\\non \\end{eqnarray}\\] with derivatives \\[\\begin{eqnarray} \\frac{\\partial R_i}{\\partial \\beta_{km}} &=& -\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)z_{mi},\\non\\\\ \\frac{\\partial R_i}{\\partial \\alpha_{ml}} &=& -\\sum_{k=1}^K\\frac{y_{ik}}{f_k(x_i)}g'_k(\\beta_k^Tz_i)\\beta_{km}\\sigma'(\\alpha_{0m} + \\alpha_m^Tx_i)x_{il}.\\label{eq:11-3b} \\end{eqnarray}\\] Given these derivatives, a gradient descent update at the \\((r+1)\\) st iteration has the form \\[\\begin{eqnarray} \\beta_{km}^{(r+1)} &=& \\beta_{km}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\beta^{(r)}_{km}},\\non\\\\ \\alpha_{ml}^{(r+1)} &=& \\alpha_{ml}^{(r)} - \\gamma_r\\sum_{i=1}^N\\frac{\\partial R_i}{\\partial \\alpha^{(r)}_{ml}},\\non \\end{eqnarray}\\] where \\(\\gamma_r\\) is the learning rate . We write \\(\\eqref{eq:11-3b}\\) as \\[\\begin{eqnarray} \\frac{\\partial R_i}{\\partial \\beta_{km}} &=& \\delta_{ki}z_{mi},\\non\\\\ \\frac{\\partial R_i}{\\partial \\alpha_{ml}} &=& s_{mi}x_{il}.\\non \\end{eqnarray}\\] From their definitions, we have \\[\\begin{equation} s_{mi} = \\sigma'(\\alpha_{0m}+\\alpha_m^Tx_i)\\sum_{k=1}^K\\beta_{km}\\delta_{ki},\\non \\end{equation}\\] known as the back-propagation equations .","title":"Ex. 11.3"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-4/","text":"Ex. 11.4 Consider a neural network for a \\(K\\) class outcome that uses cross-entropy loss. If the network has no hidden layer, show that the model is equivalent to the multinomial logistic model described in Chapter 4. Soln. 11.4 From (11.5) in the text, if there are no hidden layers, then \\[\\begin{equation} T_k = \\beta_{0k} + \\beta_k^TX, \\ k=1,...,K\\non \\end{equation}\\] thus by (11.6) in the text \\[\\begin{eqnarray} f_k(X) &=& g_k(X)\\non\\\\ &=& \\frac{\\exp(\\beta_{0k} + \\beta_k^TX)}{\\sum_{l=1}^K\\exp(\\beta_{0l} + \\beta_l^TX)}.\\non \\end{eqnarray}\\] If we normalize these probabilities by \\[\\begin{equation} f_k(x) \\leftarrow f_k(x)/f_K(x) \\cdot \\frac{1}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{0l} + \\beta_l^TX)},\\ k=1,...,K \\non \\end{equation}\\] we get exactly the multinomial logistic model studied in Ex. 4.4 .","title":"Ex. 11.4"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-5/","text":"Ex. 11.5 (a) Write a program to fit a single hidden layer neural network (ten hidden units) via back-propagation and weight decay. (b) Apply it to 100 observations from the model \\[\\begin{equation} Y = \\sigma(a_1^TX) + (a_2^TX)^2 + 0.30 \\cdot Z,\\non \\end{equation}\\] where \\(\\sigma\\) is the sigmoid function, \\(Z\\) is standard normal, \\(X^T=(X_1, X_2)\\) , each \\(X_j\\) being independent standard normal, and \\(a_1^T=(3,3)\\) , \\(a_2^T=(3, -3)\\) . Generate a test sample of size 1000, and plot the training and test error curves as a function of the number of training epochs, for different values of the weight decay parameter. Discuss the overfitting behavior in each case. (c) Vary the number of hidden units in the network, from 1 up to 10, and determine the minimum number needed to perform well for this task. Soln. 11.5 Figure 1 below shows the error rate versus number of epoches for a network with 10 hidden units. Figure 1: Error Rates for A Neural Network Regressor Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 import numpy as np import pandas as pd import plotly.graph_objects as go from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error from sklearn.neural_network import MLPRegressor def sigmoid ( x ): return 1 / ( 1 + np . exp ( - x )) def get_data ( size ): mean = [ 0 , 0 ] cov = [[ 1 , 0 ], [ 0 , 1 ]] a1 = np . array ([ 3 , 3 ]) a2 = np . array ([ 3 , - 3 ]) X = np . random . multivariate_normal ( mean , cov , size = size ) Z = np . random . normal ( loc = 0 , scale = 1 , size = size ) Y = sigmoid ( np . dot ( a1 , X . T )) + np . square ( np . dot ( a2 , X . T )) + 0.3 * Z Y = Y . T return [ X , Y ] X_train , y_train = get_data ( 100 ) X_test , y_test = get_data ( 1000 ) # scale data scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) # NN regressor train_epochs = np . arange ( 1000 , 100000 , 500 ) weight_decay_coefs = [ 0.001 ] hidden_units = np . arange ( 10 , 11 ) hidden_unit_list = [] train_error_list = [] test_error_list = [] epoch_list = [] weight_decay_list = [] for weight_decay_coef in weight_decay_coefs : for train_epoch in train_epochs : for hidden_unit in hidden_units : print ( 'alpha is : {} , epoch is {} , hidden unit is {} ' . format ( weight_decay_coef , train_epoch , hidden_unit )) regr = MLPRegressor ( random_state = 1 , hidden_layer_sizes = ( hidden_unit ,), max_iter = train_epoch , alpha = weight_decay_coef , activation = 'logistic' ) . fit ( X_train , y_train ) y_train_pred = regr . predict ( X_train ) y_test_pred = regr . predict ( X_test ) train_error = mean_squared_error ( y_train , y_train_pred ) test_error = mean_squared_error ( y_test , y_test_pred ) hidden_unit_list . append ( hidden_unit ) weight_decay_list . append ( weight_decay_coef ) epoch_list . append ( train_epoch ) train_error_list . append ( train_error ) test_error_list . append ( test_error ) df = pd . DataFrame ({ 'num. hidden unit' : hidden_unit_list , 'num. epoch' : epoch_list , 'weight decay' : weight_decay_list , 'train error' : train_error_list , 'test error' : test_error_list }) print ( df ) df_tmp = df . loc [ df [ 'num. hidden unit' ] == 10 ] df_tmp = df_tmp [ df_tmp [ 'weight decay' ] == 0.001 ] # Create traces fig = go . Figure () fig . add_trace ( go . Scatter ( x = df_tmp [ 'num. epoch' ], y = df_tmp [ 'train error' ], mode = 'lines' , name = 'train error' )) fig . add_trace ( go . Scatter ( x = df_tmp [ 'num. epoch' ], y = df_tmp [ 'test error' ], mode = 'lines' , name = 'test error' )) fig . update_layout ( xaxis_title = \"Num. of Epoches\" , yaxis_title = \"Error Rate\" , ) fig . update_layout ( legend = dict ( yanchor = \"top\" , y = 0.99 , xanchor = \"center\" , x = 0.5 )) fig . show ()","title":"Ex. 11.5"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-6/","text":"TODO","title":"Ex. 11.6 (TODO)"},{"location":"ESL-Solution/_11-Neural-Networks/ex11-7/","text":"TODO","title":"Ex. 11.7 (TODO)"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-01/","text":"Ex. 12.1 Show that the criteria (12.25) and (12.8) are equivalent. Soln. 12.1 For (12.8), the problem (denoted as \\(P_1\\) ) is \\[\\begin{eqnarray} \\label{eq:12-1a} P_1: &&\\min_{\\beta, \\beta_0}\\ \\ \\ \\frac{1}{2} \\|\\beta\\|^2 + C\\sum_{i=1}^N\\xi_i \\non \\\\ &&\\text{subject to} \\ \\ \\ \\xi_i\\ge 0, \\ \\xi_i \\ge 1 - y_if(x_i) \\ \\forall i\\non \\end{eqnarray}\\] For (12.25), the problem (denoted as \\(P'_2\\) ) with \\(\\lambda=\\frac{1}{C}\\) is \\[\\begin{eqnarray} \\label{eq:12-1b} P'_2: &&\\min_{\\beta, \\beta_0}\\ \\ \\ \\frac{1}{C}\\left[ \\frac{1}{2} \\|\\beta\\|^2 + C\\sum_{i=1}^N\\eta_i \\right]\\non \\\\ &&\\text{subject to} \\ \\ \\ \\eta_i = [1-y_if(x_i)]_+ \\ \\forall i\\non \\end{eqnarray}\\] The objective functions for both problems are the same, up to constant \\(\\frac{1}{C}\\) . Without loss of generality, we rewrite \\(P'_2\\) as \\(P_2\\) : \\[\\begin{eqnarray} \\label{eq:12-1bb} P_2: &&\\min_{\\beta, \\beta_0}\\ \\ \\ \\frac{1}{2} \\|\\beta\\|^2 + C\\sum_{i=1}^N\\eta_i \\non \\\\ &&\\text{subject to} \\ \\ \\ \\eta_i = [1-y_if(x_i)]_+ \\ \\forall i\\non \\end{eqnarray}\\] For any optimal solution \\(\\{\\hat\\beta, \\hat\\beta_0, \\hat\\eta_i\\}\\) of \\(P_2\\) , it also satisfies the constraints in \\(P_1\\) , because \\(\\hat\\eta_i=[1-y_i\\hat f(x_i)]_+\\ge 0\\) and \\(\\hat\\eta_i=[1-y_i\\hat f(x_i)]_+\\ge 1-y_i\\hat f(x_i)\\) for any \\(i\\) . Therefore, we know the optimal value of \\(P_2\\) is greater than or equal to that of \\(P_1\\) . To show the other way around, let's go back to (12.10) - (12.16) in the text which uniquely characterize the solution to \\(P_1\\) . Consider an optimal solution \\(\\{\\hat\\beta, \\hat\\beta_0, \\hat\\xi_i\\}\\) of \\(P_1\\) . If \\(\\hat\\xi_i > 1-y_i\\hat f(x_i)\\) , then \\(\\hat\\alpha_i=0\\) by (12.14), and \\(C-\\mu_i=0\\) by (12.12), so that \\(\\xi_i\\) is eliminated in (12.9). In other words, the \\(i\\) -th data sample is not a support vector . Such \\(\\hat\\xi_i\\) have no impact on the optimal solution, therefore the focus is on \\(\\hat\\xi_i = 1-y_i\\hat f(x_i) = [1-y_i\\hat f(x_i)]_+ = \\hat \\eta_i\\) since \\(\\hat \\xi_i\\ge 0\\) . This reduces to constraints in \\(P_2\\) . Therefore, the two problems have the same solution.","title":"Ex. 12.1"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-02/","text":"Ex. 12.2 Show that the solution to (12.29) is the same as the solution to (12.25) for a particular kernel. Soln. 12.2 Define the kernel \\(K\\) by \\(K(x,y) := h(x)^Th(y)\\) for any \\(x, y\\in \\mathbb{R}^p\\) . Let \\(\\beta = \\sum_{i=1}^N\\alpha_ih(x_i)\\) , then (12.28) in the text reduces to \\[\\begin{eqnarray} f(x) &=& \\beta_0 + \\sum_{i=1}^N\\alpha_ih(x)^Th(x_i)\\non\\\\ &=&\\beta_0 + h(x)^T\\sum_{i=1}^N\\alpha_ih(x_i)\\non\\\\ &=&\\beta_0 + h(x)^T\\beta.\\non \\end{eqnarray}\\] Further note that \\[\\begin{eqnarray} \\|\\beta\\|^2 &=& \\beta^T\\beta\\non\\\\ &=&\\left(\\sum_{i=1}^N\\alpha_ih(x_i)\\right)^T\\left(\\sum_{i=1}^N\\alpha_ih(x_i)\\right)\\non\\\\ &=&\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jK(x_i, x_j)\\non\\\\ &=&\\alpha^TK\\alpha.\\non \\end{eqnarray}\\] Therefore, the solution to (12.29) is the same as the solution to (12.25).","title":"Ex. 12.2"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-03/","text":"Ex. 12.3 Consider a modification to (12.43) where you do not penalize the constant. Formulate the problem, and characterize its solution. Soln. 12.3 This exercise is closely related to Section 5.8.1 and 12.3.3. Specifically, without the constant term \\(\\beta_0\\) , (12.43) can be rewritten as \\[\\begin{equation} \\label{ex:12-3a} \\min_{\\bm{\\beta}}V(\\by, \\bb{K}\\bm{\\beta}) + \\frac{\\lambda}{2}\\|\\bm{\\beta}\\|^2, \\end{equation}\\] where \\(\\bb{K}_{ij} = K(x_i, x_j)\\) as usual. For general error measure \\(V\\) , the optimization problem \\(\\eqref{ex:12-3a}\\) can be solved numerically.","title":"Ex. 12.3"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-04/","text":"Ex. 12.4 Suppose you perform a reduced-subspace linear discriminant analysis for a \\(K\\) -group problem. You compute the canonical variables of dimension \\(L\\le K-1\\) given by \\(z = U^Tx\\) , where \\(U\\) is the \\(p\\times L\\) matrix of discriminat coefficients, and \\(p > K\\) is the dimension of \\(x\\) . (a) If \\(L=K-1\\) show that \\[\\begin{equation} \\|z-\\bar z_k\\|^2 - \\|z-\\bar z_{k'}\\|^2 = \\|x-\\bar x_k\\|^2_W - \\|x-\\bar x_{k'}\\|^2_W,\\non \\end{equation}\\] where \\(\\|\\cdot\\|_W\\) denotes Mahalanobis distance with respect to the covariance \\(W\\) . (b) If \\(L < K-1\\) , show that the same expression on the left measures the difference in Mahalanobis squared distances for the distributions projected onto the subspace spanned by \\(U\\) . Soln. 12.4 Consider the SVD \\(W=\\hat U D \\hat U^T\\) , and write \\(\\hat U = (\\hat U_L : \\hat U_\\bot)\\) where \\(\\hat U_L\\) represents the first \\(L\\le K-1\\) columns and \\(\\hat U_\\bot\\) the corresponding complement. It is easy to verify that \\[\\begin{eqnarray} W^{-1} &=& (\\hat U_LD^{-1/2})(\\hat U_LD^{-1/2})^T + (\\hat U_\\bot D^{-1/2})(\\hat U_\\bot D^{-1/2})^T\\non\\\\ &:=& U_LU_L^T + U_\\bot U_\\bot ^T.\\non \\end{eqnarray}\\] Therefore, we have \\[\\begin{eqnarray} \\|x-\\bar x_k\\|_W^2 &=& \\|U_L^T(x-\\bar x_k)\\|^2 + \\|U_\\bot^T(x-\\bar x_k)\\|^2\\non\\\\ &=&\\|z-\\bar z_k\\|^2+ \\|U_\\bot^T(x-\\bar x_k)\\|^2.\\non \\end{eqnarray}\\] When \\(L=K-1\\) , the second term vanishes, and we recover (a). When \\(L<K-1\\) , the first term \\(\\|z-\\bar z_k\\|^2\\) is just the Mahalanobis squared distances for the distributions projected onto the subspace spanned by \\(U\\) .","title":"Ex. 12.4"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-05/","text":"Ex. 12.5 The data in phoneme.subset , available from this book's website consists of digitized log-periodograms for phonemes uttered by 60 speakers, each speaker having produced phonemes from each of five classes. It is appropriate to plot each vector of 256 \u201cfeatures\u201d against the frequencies 0\u2013255. (a) Produce a separate plot of all the phoneme curves against frequency for each class. (b) You plan to use a nearest prototype classification scheme to classify the curves into phoneme classes. In particular, you will use a K-means clustering algorithm in each class ( kmeans() in R), and then classify observations to the class of the closest cluster center. The curves are high-dimensional and you have a rather small sample-size-to-variables ratio. You decide to restrict all the prototypes to be smooth functions of frequency. In particular, you decide to represent each prototype \\(m\\) as \\(m=B\\theta\\) where \\(B\\) is a \\(256 \\times J\\) matrix of natural spline basis functions with \\(J\\) knots uniformly chosen in (0,255) and boundary knots at 0 and 255. Describe how to proceed analytically, and in particular, how to avoid costly high-dimensional fitting procedures. (Hint: It may help to restrict \\(B\\) to be orthogonal.) (c) Implement your procedure on the phoneme data, and try it out. Divide the data into a training set and a test set (50-50), making sure that speakers are not split across sets (why?). Use \\(K = 1,3,5,7\\) centers per class, and for each use \\(J = 5,10,15\\) knots (taking care to start the K-means procedure at the same starting values for each value of \\(J\\) ), and compare the results.","title":"Ex. 12.5 (TODO)"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-06/","text":"Ex. 12.6 Suppose that the regression procedure used in FDA (Section 12.5.1) is a linear expansion of basis functions \\(h_m(x), m=1,...,M\\) . Let \\(\\bb{D}_\\pi = \\bb{Y}^T\\bb{Y}/N\\) be the diagonal matrix of class proportions. (a) Show that the optimal scoring problem (12.52) can be written in vector notation as \\[\\begin{equation} \\min_{\\theta, \\beta} \\|\\bb{Y}\\theta - \\bb{H}\\beta\\|^2,\\non \\end{equation}\\] where \\(\\theta\\) is a vector of \\(K\\) real numbers, and \\(\\bb{H}\\) is the \\(N\\times M\\) matrix of evaluations \\(h_j(x_i)\\) . (b) Suppose that the normalization on \\(\\theta\\) is \\(\\theta^T\\bb{D}_\\pi 1 = 0\\) and \\(\\theta^T\\bb{D}_\\pi\\theta=1\\) . Interpret these normalizations in terms of the original scored \\(\\theta(g_i)\\) . (c) Show that, with this normalization, (12.65) can be partially optimized w.r.t. \\(\\beta\\) , and leads to \\[\\begin{equation} \\max_\\theta \\theta^T\\bb{Y}^T\\bb{S}\\bb{Y}\\theta,\\non \\end{equation}\\] subject to the normalization constraints, where \\(\\bb{S}\\) is the projection operator corresponding to the basis matrix \\(H\\) . (d) Suppose that the \\(h_j\\) include the constant function. Show that the largest eigenvalue of \\(\\bb{S}\\) is 1. (e) Let \\(\\Theta\\) be a \\(K\\times K\\) matrix of scores (in columns), and suppose the normalization is \\(\\Theta\\bb{D}_\\pi\\Theta=\\bb{I}\\) . Show that the solution to (12.53) is given by the complete set of eigenvectors of \\(\\bb{S}\\) ; the first eigenvector is trivial, and takes care of the centering of the scores. The remainder characterize the optimal scoring solution. Soln. 12.6 (a) Let \\(\\bb{Y}\\) be an \\(N\\times K\\) indicator response matrix with each row corresponding to the encoding of \\(K\\) classes, see Section 12.5.1. Let \\(h(x_i) = (h_1(x_i),...,h_M(x_i))^T\\) for \\(i=1,...,N\\) . Then we have \\[\\begin{equation} \\bb{Y}\\theta - \\bb{H}\\beta = \\begin{pmatrix} \\theta_{j_1}\\non\\\\ \\theta_{j_2}\\non\\\\ \\vdots\\non\\\\ \\theta_{j_K} \\end{pmatrix} - \\begin{pmatrix} h(x_1)^T\\non\\\\ h(x_2)^T\\non\\\\ \\vdots\\non\\\\ h(x_N)^T \\end{pmatrix}\\beta,\\non \\end{equation}\\] where \\(j_1, j_2, ...,j_K\\) are determined by \\(\\bb{Y}\\) 's encoding. Then we only need to choose \\(\\theta\\) such that \\(\\theta_{j_i} = g(\\theta_i)\\) for \\(i=1,...,N\\) , so that \\[\\begin{equation} \\bb{Y}\\theta - \\bb{H}\\beta = \\begin{pmatrix} \\theta(g_1)-h(x_1)^T\\beta\\non\\\\ \\theta(g_2)-h(x_2)^T\\beta\\non\\\\ \\vdots\\non\\\\ \\theta(g_N)-h(x_N)^T\\beta\\non \\end{pmatrix}\\non \\end{equation}\\] The rest of proof follows directly. (b) Since \\[\\begin{equation} \\bb{Y}\\theta = \\begin{pmatrix} \\theta_{g_1}\\non\\\\ \\theta_{g_2}\\non\\\\ \\vdots\\non\\\\ \\theta_{g_K} \\end{pmatrix}:= \\theta_g,\\non \\end{equation}\\] we have \\[\\begin{eqnarray} \\theta^T\\bb{D}_\\pi 1 &=& \\frac{1}{N}(\\bb{Y}\\theta)^T\\bb{Y}1\\non\\\\ &=&\\frac{1}{N}\\theta_g^T1\\non\\\\ &=&\\frac{\\sum_{i=1}^N\\theta(g_i)}{N}=0.\\non \\end{eqnarray}\\] Similarly we know \\(\\theta^T\\bb{D}_\\pi\\theta=1\\) implies that \\[\\begin{equation} \\frac{\\sum_{i=1}^N\\theta(g_i)^2}{N} = 1.\\non \\end{equation}\\] (c) With \\(\\theta\\) fixed, the minimizing \\(\\beta\\) for the optimal scoring problem is the least square estimate \\[\\begin{equation} \\hat\\beta = (\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\bb{Y}\\theta.\\non \\end{equation}\\] Then the minimization objective becomes, by noting the constraint \\(\\theta^T\\bb{D}_\\pi\\theta=1\\) , \\[\\begin{equation} \\|\\bb{Y}\\theta - \\bb{H}(\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\bb{Y}\\theta\\| = N - \\theta^T\\bb{Y}^T\\bb{H}(\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\bb{Y}\\theta, \\non \\end{equation}\\] which is equivalent to solving \\[\\begin{equation} \\max_\\theta \\theta^T\\bb{Y}^T\\bb{S}\\bb{Y}\\theta,\\non \\end{equation}\\] where \\[\\begin{equation} \\bb{S} = \\bb{H}(\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\non \\end{equation}\\] is the projection operator (see, e.g., p.153 in the text) corresponding to the basis matrix \\(\\bb{H}\\) . (d) Note that \\(\\bb{S}\\) is idempotent, i.e., \\(\\bb{S}\\bb{S} = \\bb{S}\\) , so the largest eigenvalue of \\(\\bb{S}\\) is 1. (e) From (c) we know that the problem formulated in (12.53) can be written as an eigenvalue problem \\[\\begin{eqnarray} && \\max_\\Theta \\ \\Theta^T\\bb{Y}^T\\bb{S}\\bb{Y}\\Theta\\non\\\\ && s.t. \\ \\Theta^T\\bb{D}_\\pi\\Theta=\\bb{I}\\non\\\\ && \\ \\ \\ \\ \\ \\ \\ \\ \\Theta^T\\bb{D}_\\pi\\bb{I} = \\bb{0}\\non \\end{eqnarray}\\] By classical results for generalized eigenvalue problems (see, e.g., Eigenvalue and generalized eigenvalue problems: Tutorial ), we know the solution to (12.53) is given by the eigenvectors of \\(\\bb{S}\\) .","title":"Ex. 12.6"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-07/","text":"Ex. 12.7 Derive the solution to the penalized optimal scoring problem (12.57). Soln. 12.7 The idea to solve the penalized optimal scoring problem (12.57) is essentially the same as that in Ex. 12.6 , we only need to change \\(\\bb{S}\\) from \\(\\bb{H}(\\bb{H}^T\\bb{H})^{-1}\\bb{H}^T\\) to \\(\\bb{H}(\\bb{H}^T\\bb{H}+\\Omega)^{-1}\\bb{H}^T\\) .","title":"Ex. 12.7"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-08/","text":"Ex. 12.8 Show that coefficients \\(\\beta_\\ell\\) found by optimal scoring are proportional to the discriminant directions \\(\\nu_\\ell\\) found by linear discriminant analysis. Soln. 12.8 From Ex. 12.6 we know that the solution of optimal scoring problem (12.52) (with \\(\\bb{H}\\) being identity matrix) is the same (up to coefficient) as ordinary least squares. From Ex. 4.2 we know that the latter is proportional to LDA directions. Therefore, the optimal scoring coefficients are also proportional to LDA directions. An alternative way is to link optimal scoring and discriminant directions via canonical correlation analysis, see Section 3.4 in \\cite{hastie1995penalized} for more details.","title":"Ex. 12.8"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-09/","text":"Ex. 12.9 Let \\(\\hat{\\bb{Y}}=\\bb{X}\\hat{\\bb{B}}\\) be the fitted \\(N\\times K\\) indicator response matrix after linear regression on the \\(N\\times p\\) matrix \\(\\bb{X}\\) , where \\(p > K\\) . Consider the reduced features \\(x_i^\\ast = \\hat{\\bb{B}}^Tx_i\\) . Show that LDA using \\(x_i^\\ast\\) is equivalent to LDA in the original space. Soln. 12.9 This is the same as Ex. 4.3 .","title":"Ex. 12.9"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-10/","text":"Ex. 12.10 Kernels and linear discriminant analysis . Suppose you wish to carry out a linear discriminant analysis (two classes) using a vector of transformations of the input variable \\(h(x)\\) . Since \\(h(x)\\) is high-dimensional, you will use a regularized within-class covariance matrix \\(\\bb{W}_h + \\gamma\\bb{I}\\) . Show that the model can be estimated using only the inner products \\(K(x_i, x_{i'})=\\langle h(x_i), h(x_{i'}) \\rangle\\) . Hence the kernel property of support vector machines is also shared by regularized linear discriminant analysis. Soln. 12.10 This problem is two-fold. First, with the kernel \\(K\\) , LDA can be solved using the same logic discussed Section 12.3.3 as SVM, or more generally in Section 5.8. See Ex. 18.13 for solving logistic regression as well. Second, the regularized LDA (may arise for high-dimensional problems) share the same logic as normal LDA in finding solutions. See Ex. 12.6 - Ex. 12.7 for comparison in optimal scoring problem (which has the solution proportional to LDA directions). In addition, see Ex. 18.10 for it's connection to the maximal data piling direction .","title":"Ex. 12.10"},{"location":"ESL-Solution/_12-Flexible-Discriminants/ex12-11/","text":"Ex. 12.11 The MDA procedure models each class as a mixture of Gaussians. Hence each mixture center belongs to one and only one class. A more general model allows each mixture center to be shared by all classes. We take the joint density of labels and features to be \\[\\begin{equation} P(G, X) = \\sum_{r=1}^R\\pi_rP_r(G,X),\\non \\end{equation}\\] a mixture of joint densities. Furthermore we assume \\[\\begin{equation} P_r(G,X) = P_r(G)\\phi(X;\\mu_r,\\Sigma).\\non \\end{equation}\\] This model consists of regions centered at \\(\\mu_r\\) , and for each there is a class profile \\(P_r(G)\\) . The posterior class distribution is given by \\[\\begin{equation} P(G=k|X=x) = \\frac{\\sum_{r=1}^R\\pi_rP_r(G=k)\\phi(x;\\mu_r,\\Sigma)}{\\sum_{r=1}^R\\pi_r\\phi(x;\\mu_r,\\Sigma)},\\non \\end{equation}\\] where the denominator is the marginal distribution \\(P(X)\\) . (a) Show that this model (called MDA2) can be viewed as a generalization of MDA since \\[\\begin{equation} P(X|G=k) = \\frac{\\sum_{r=1}^R\\pi_rP_r(G=k)\\phi(x;\\mu_r,\\Sigma)}{\\sum_{r=1}^R\\pi_rP_r(G=k)}\\non \\end{equation}\\] where \\(\\pi_{rk} = \\pi_rP_r(G=k)/\\sum_{r=1}^R\\pi_rP_r(G=k)\\) corresponds to the mixing proportions for the \\(k\\) th class. (b) Derive the EM algorithm for MDA2. (c) Show that if the initial weight matrix is constructed as in MDA, involving separate \\(k\\) -means clustering in each class, then the algorithm for MDA2 is identical to the original MDA procedure. Soln. 12.11 (a) We have \\[\\begin{eqnarray} P(X=x|G=k) &=& \\frac{P(G=k, X=x)}{P(G=k)}\\non\\\\ &=&\\frac{\\sum_{r=1}^R\\pi_rP_r(G=k)\\phi(x;\\mu_r,\\Sigma)}{\\sum_{r=1}^R\\pi_rP_r(G=k)}\\non\\\\ &=&\\sum_{r=1}^R\\pi_{kr}\\phi(x;\\mu_r, \\Sigma),\\non \\end{eqnarray}\\] where \\(\\pi_{kr} = \\pi_rP_r(G=k)/\\sum_{r=1}^R\\pi_rP_r(G=k)\\) . Intuitively, we can think that, in MDA, there are total of \\(R=\\sum_{k=1}^KR_k\\) mixture centers shared by all the \\(K\\) classes, with additional conditions that the joint densities of class \\(k\\) and center \\(r\\notin \\{R_k\\}\\) are zero, where \\(\\{R_k\\}\\) represents the set of mixtures assigned to class \\(k\\) . (b) The EM algorithm is similar to that of MDA. Specifically, we have E-step : Given current parameters ( \\(\\pi_r, P_r(G=k), \\mu_r, \\Sigma\\) ), compute the responsibility \\(W(c_{kr}|x_i, g_i)\\) for each of class- \\(k\\) observation \\(x_i\\) \\[\\begin{equation} W(c_{kr}|x_i, g_i) = \\frac{\\pi_r P_r(G=k)\\phi(x_i; \\mu_r, \\Sigma)}{\\sum_{l=1}^R\\pi_l P_l(G=k)\\phi(x_i;\\mu_l, \\Sigma)}.\\non \\end{equation}\\] M-step : Compute the weighted MLEs for the parameters of each of the component Gaussians within each of the classes, using the weights from the E-step. (c) For MDA, the \\(k\\) -means clustering initialization partitions the observations into \\(R_k\\) disjoint groups, thus constructing an initial weight matrix consisting of zeros and ones. Note that from (a), MDA2 is a generalization of MDA. Once \\(k\\) -means clustering initialization is done for MDA2, we can think the total \\(R\\) mixtures for MDA2 are partitioned in a summation \\(R=\\sum_{k=1}^KR_k\\) , then the following steps for MDA and MDA2 from here become identical.","title":"Ex. 12.11"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-1/","text":"Ex. 13.1 Consider a Gaussian mixture model where the covariance matrices are assumed to be scalar: \\(\\Sigma_r =\\sigma\\bb{I} \\ \\forall r=1,...,R\\) , and \\(\\sigma\\) is a fixed parameter. Discuss the analogy between the \\(K\\) -means clustering algorithm and the EM algorithm for fitting this mixture model in detail. Show that in the limit \\(\\sigma\\ra0\\) the two methods coincide. Soln. 13.1 The analogy between EM algorithm (see Section 8.5 for details) and \\(K\\) -means algorithm is summarized in Section 13.2.3. In short, EM algorithm can be regarded as a soft version of \\(K\\) -means for Gaussian mixture model. The EM algorithm uses responsibilities to make a soft assignment of each data point to one of the clusters. When \\(\\sigma\\) is fixed, responsibility of data point \\(i\\) assigning to cluster \\(k\\) is given by \\[\\begin{equation} r_i^{(k)} = \\frac{\\exp(-\\frac{1}{2\\sigma^2}\\|y_i - \\mu_k\\|^2)}{\\sum_{l=1}^K\\exp(-\\frac{1}{2\\sigma^2}\\|y_i-\\mu_l\\|^2)}.\\non \\end{equation}\\] It's easy to very that, as \\(\\sigma\\ra 0\\) , \\(r_i^{(k)}\\ra 1\\) for the cluster \\(k\\) that is closest to \\(y_i\\) and \\(r_i^{(k)}\\ra 0\\) for other clusters. Therefore \\(K\\) -means algorithm is recovered as \\(\\sigma \\ra 0\\) .","title":"Ex. 13.1"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-2/","text":"Ex. 13.2 Derive formula (13.7) for the median radius of the 1-nearest-neighborhood. Soln. 13.2 This exercise is similar to Ex. 2.3 . Recall \\(\\nu_p r^p\\) is the volume of the sphere of radius \\(r\\) in \\(p\\) dimension. Consider the unit cube \\([-\\frac{1}{2}, \\frac{1}{2}]^p\\) , and a point \\(a\\) uniformly distributed in it. The probability that \\(a\\) falls outside of the superball \\(b\\) which centers at origin and has radius \\(0<r<1\\) is \\[\\begin{equation} 1-\\text{volume}(b) = 1- \\nu_pr^p.\\non \\end{equation}\\] Now for \\(N\\) independently and uniformly distributed data points, the probability of the 1-nearest-neighborhood of origin (i.e., the point that is the closest to the origin) falls outside of the superball is \\[\\begin{equation} (1- \\nu_pr^p)^N.\\non \\end{equation}\\] To find the median of \\(R\\) (the radius of a 1-nearest-neighborhood of origin), we set above equal to \\(\\frac{1}{2}\\) : \\[\\begin{equation} (1-\\nu_pR^p)^N = \\frac{1}{2}.\\non \\end{equation}\\] Solving for \\(R\\) we get \\[\\begin{equation} R = \\nu_p^{-1/p}\\left(1-\\frac{1}{2}^{1/N}\\right)^{1/p}.\\non \\end{equation}\\]","title":"Ex. 13.2"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-3/","text":"Ex. 13.3 Let \\(E^\\ast\\) be the error rate of the Bayes rule in a \\(K\\) -class problem, where the true class probabilities are given by \\(p_k(x)\\) , \\(k=1,...,K\\) . Assuming the test point and training point have identical features \\(x\\) , prove (13.5) \\[\\begin{equation} \\sum_{k=1}^Kp_k(x)(1-p_k(x)) \\le 2(1-p_{k^\\ast}(x))-\\frac{K}{K-1}(1-p_{k^\\ast}(x))^2,\\non \\end{equation}\\] where \\(k^\\ast = \\underset{k}{\\operatorname{argmax}}p_k(x)\\) . Hence argue that the error rate of the 1-nearest-neighbor rule converge in \\(L_1\\) , as the size of the training set increases, to a value \\(E_1\\) , bounded above by \\[\\begin{equation} E^\\ast\\left(2-E^\\ast\\frac{K}{K-1}\\right).\\non \\end{equation}\\] [This statement of the theorem of \\cite{cover1967nearest} is taken from Chapter 6 of \\cite{ripley1996pattern}, where a short proof is also given]. Soln. 13.3 We write \\[\\begin{equation} \\sum_{k=1}^Kp_k(x)(1-p_k(x)) = 1 -\\sum_{k=1}^Kp_k(x)^2 = 1-p_{k^\\ast}(x)^2 - \\sum_{i\\neq k^\\ast}^K p_i(x)^2,\\non \\end{equation}\\] and we need to minimize \\(\\sum_{i\\neq k^\\ast}^K p_i(x)^2\\) . It's easy to see (consider its Lagrangian) that it's minimized when each \\(p_i(x)\\) for \\(i\\neq k^\\ast\\) are equal. With the constraint that \\(\\sum_{i=1}^Kp_i(x)=1\\) , we get \\(p_i(x) = \\frac{1}{K-1}(1-p_{k^\\ast}(x))\\) for \\(i\\neq k^\\ast\\) . Plug it into the equation with simple algebra we obtain \\[\\begin{eqnarray} \\sum_{k=1}^Kp_k(x)(1-p_k(x)) &\\le& 1-p_{k^\\ast}(x)^2 - (K-1)\\left(\\frac{1}{K-1}(1-p_{k^\\ast}(x))\\right)^2\\non\\\\ &=&1-p_{k^\\ast}(x)^2 - \\frac{(1-p_{k^\\ast}(x))^2}{K-1}\\non\\\\ %&=&1-p_{k^\\ast}(x)^2 + (1-p_{k^\\ast}(x))^2 - (1-p_{k^\\ast}(x))^2 - \\frac{(1-p_{k^\\ast}(x))^2}{K-1}\\non\\\\ &=&2(1-p_{k^\\ast}(x)) - \\frac{K}{K-1}(1-p_{k^\\ast}(x))^2.\\label{eq:13-3b} \\end{eqnarray}\\] By definition we have \\[\\begin{equation} E_1 = \\int \\left[\\sum_{k=1}^Kp_k(x)(1-p_k(x))\\right]p(x)dx\\non \\end{equation}\\] and \\[\\begin{equation} E^\\ast = \\int \\left[1-p_{k^\\ast}(x)\\right]p(x)dx.\\non \\end{equation}\\] Therefore, by \\(\\eqref{eq:13-3b}\\) we obtain \\[\\begin{equation} E_1 \\le E^\\ast\\left(2-E^\\ast\\frac{K}{K-1}\\right).\\non \\end{equation}\\]","title":"Ex. 13.3"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-4/","text":"Ex. 13.4 Consider an image to be a function \\(F(x):\\mathbb{R}^2\\ra\\mathbb{R}^1\\) over the two-dimensional spatial domain (paper coordinates). Then \\(F(c+x_0+\\bb{A}(x-x_0))\\) represents an affine transformation of the image \\(F\\) , where \\(\\bb{A}\\) is a \\(2\\times 2\\) matrix. (a) Decompose \\(\\bb{A}\\) (via Q-R) in such a way that parameters identifying the four affine transformations (two scale, shear and rotation) are clearly identified. (b) Using the chain rule, show that the derivative of \\(F(c+x_0+\\bb{A}(x-x_0))\\) w.r.t. each of these parameters can be represented in terms of the two spatial derivatives of \\(F\\) . (c) Using a two-dimensional kernel smoother (Chapter 6), describe how to implement this procedure when the images are quantized to \\(16\\times16\\) pixels. Soln. 13.4 (a) In this representation, \\(c\\) accounts for location shifts, \\(x_0\\) is the center of rotation, scaling and shear and \\(\\bb{A}\\in \\mathbb{R}^{2\\times 2}\\) is a transformation matrix with QR decomposition \\(\\bb{A}=R(\\theta)T\\) , where \\(R\\) is a rotation matrix and \\(T\\) is an upper-triangular scale/shear matrix. (b) The first order Taylor series approximation has the form \\[\\begin{eqnarray} F^T(x, c, A) &=& F(x) + \\sum_{\\alpha \\in\\{c, \\theta, T\\}}\\frac{\\partial F}{\\partial \\alpha}(\\alpha-\\alpha_0)\\non\\\\ &=&F(x) + \\nabla F(x)^T\\sum_{\\alpha\\in \\{c, \\theta, T\\}}\\frac{\\partial Z(x, x_0, c, A)}{\\alpha}(\\alpha-\\alpha_0),\\non \\end{eqnarray}\\] where \\(Z(x, x_0, c, A) := c+ x_0 + \\bb{A}(x-x_0)\\) . This leads to the following six derivative (tangent) function \\(F_\\alpha(x)\\) : x-location : \\(\\alpha = c_1\\) and \\(F_\\alpha = F_x(z) = \\frac{\\partial F(z)}{\\partial x}\\) y-location : \\(\\alpha = c_2\\) and \\(F_\\alpha = F_y(z) = \\frac{\\partial F(z)}{\\partial y}\\) x-scale : \\(\\alpha = T_{11}\\) and \\(F_\\alpha = (x-x_0)F_x(z)\\) y-scale : \\(\\alpha = T_{22}\\) and \\(F_\\alpha = (y-y_0)F_y(z)\\) Rotation : \\(\\alpha = \\theta\\) and \\(F_\\alpha = (y-y_0)F_x(z) - (x-x_0)F_y(z)\\) Shear : \\(\\alpha = T_{12}\\) and \\(F_\\alpha = (y-y_0)F_x(z) + (x-x_0)F_y(z)\\) (c) As we move to the digitized \\(16 \\times 16\\) pixels, we need to evaluate \\(F_x\\) and \\(F_y\\) at the same set of lattice points. We can just smooth the image first and then use the first differences to approximate \\(F_x\\) and \\(F_y\\) . Please refer to \\cite{hastie1997metrics} for detailed discussions.","title":"Ex. 13.4"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-5/","text":"Ex. 13.5 Let \\(\\bb{B}_i, i=1,2,...,N\\) be square \\(p\\times p\\) positive semi-definite matrices and let \\({\\bar{\\bb B}} = (1/N)\\sum\\bb{B}_i\\) . Write the eigen-decomposition of \\({\\bar{\\bb B}}\\) as \\(\\sum_{\\ell=1}^p\\theta_\\ell e_\\ell e_\\ell^T\\) with \\(\\theta_\\ell\\ge\\theta_{\\ell-1}\\ge \\cdots \\ge\\theta_1\\) . Show that the best rank- \\(L\\) approximation for the \\(\\bb{B}_i\\) , \\[\\begin{equation} \\min_{\\text{rank}(M)=L}\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i-\\bb{M})^2],\\non \\end{equation}\\] is given by \\({\\bar{\\bb B}}_{[L]}=\\sum_{\\ell=1}^L\\theta_\\ell e_\\ell e^T_\\ell\\) . Hint : Write \\(\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i-\\bb{M})^2]\\) as \\[\\begin{equation} \\sum_{i=1}^N\\text{trace}[(\\bb{B}_i-{\\bar{\\bb B}})^2] + \\sum_{i=1}^N\\text{trace}[(\\bb{M}-{\\bar{\\bb B}})^2]).\\non \\end{equation}\\] Soln. 13.5 By properties of trace operator (see \\cite{matbook}), we have \\[\\begin{eqnarray} &&\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i - \\bb{M})^2]\\non\\\\ &=&\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i - \\bar{\\bB})^2 + (\\bar\\bB - \\bb{M})^2] + \\sum_{i=1}^N\\text{trace}[(\\bB_i-\\bar\\bB)(\\bar\\bB-\\bb{M})]\\non\\\\ &=&\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i - \\bar{\\bB})^2 + (\\bar\\bB - \\bb{M})^2]\\non\\\\ &&+\\sum_{i=1}^N\\text{trace}[\\bar\\bB_i(\\bar\\bB-\\bb{M})] - N\\cdot\\text{trace}[\\bar\\bB(\\bar\\bB-\\bb{M})]\\non\\\\ &=&\\sum_{i=1}^N\\text{trace}[(\\bb{B}_i - \\bar{\\bB})^2] +\\sum_{i=1}^N\\text{trace}[(\\bar\\bB - \\bb{M})^2],\\non \\end{eqnarray}\\] since the last summand vanishes. It suffices to show that \\(\\bar\\bB_{[L]}\\) solves \\[\\begin{equation} \\min_{\\text{rank}(\\bb{M})=L}\\text{trace}[(\\bb{M}-\\bar\\bB)^2].\\non \\end{equation}\\] Recall that the Frobenius norm of a matrix \\(M\\) is defined as \\[\\begin{equation} \\|M\\|_F^2 = \\text{trace}[M^TM].\\non \\end{equation}\\] We are essentially solving the low-rank matrix approximation problem under Forbenius norm (in our case, for matrix \\(\\bb{M}-\\bar\\bB\\) ). The desired solution \\(\\bar\\bB_{[L]}=\\sum_{l=1}^L\\theta_le_le_l^T\\) follows directly from Eckart\u2013Young\u2013Mirsky theorem . We stated below for reference. For any matrix \\(M\\in \\mathbb{R}^{m\\times n}\\) (with \\(m\\le n\\) ) with singular values \\(\\sigma_1\\le \\sigma_2 \\le ... \\le \\sigma_m\\) , \\[\\begin{equation} \\min_{\\text{rank}(M_k)=k}\\|M-M_k\\|_F^2 = \\sum_{i=k+1}^m\\sigma_i^2.\\non \\end{equation}\\]","title":"Ex. 13.5"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-6/","text":"Ex. 13.6 Here we consider the problem of shape averaging . In particular, \\(\\bb{L}_i, i=1,...,M\\) are each \\(N\\times 2\\) matrices of points in \\(\\mathbb{R}^2\\) , each sampled from corresponding positions of handwritten (cursive) letters. We seek an affine invariant average \\(\\bb{V}\\) , also \\(N\\times 2\\) , \\(\\bb{V}^T\\bb{V}=I\\) , of the \\(M\\) letters \\(\\bb{L}_i\\) with the following property: \\(\\bb{V}\\) minimizes \\[\\begin{equation} \\sum_{j=1}^M\\min_{\\bb{A}_j}\\|\\bb{L}_j-\\bb{V}\\bb{A}_j\\|^2.\\non \\end{equation}\\] Characterize the solution. This solution can suffer if some of the letters are big and dominate the average. An alternative approach is to minimize instead: \\[\\begin{equation} \\sum_{j=1}^M\\min_{\\bb{A}_j}\\|\\bb{L}_j\\bb{A}_j^\\ast-\\bb{V}\\|^2.\\non \\end{equation}\\] Derive the solution to this problem. How do the criteria differ? Use the SVD of the \\(\\bb{L}_j\\) to simplify the comparison of the two approaches. Soln. 13.6 This exercise is similar to Ex. 14.10. Note the norm \\(\\|\\cdot\\|\\) here is understood as Frobenius norm \\(\\|M\\|^2 = \\text{trace}[M^TM]\\) for matrix \\(M.\\) First consider \\[\\begin{equation} \\min_{\\bb{A}_j}\\|\\bb{L}_j-\\bb{V}\\bb{A}_j\\|^2\\non \\end{equation}\\] with \\(\\bb{V}\\) fixed. By definition, we have \\[\\begin{eqnarray} &&\\|\\bb{L}_j-\\bb{V}\\bb{A}_j\\|^2\\non\\\\ &=& \\text{trace}[(\\bb{L}_j-\\bb{V}\\bb{A}_j)^T(\\bb{L}_j-\\bb{V}\\bb{A}_j)]\\non\\\\ &=&\\text{trace}[(\\bb{L}_j^T-\\bb{A}_j^T\\bb{V}^T)(\\bb{L}_j-\\bb{V}\\bb{A}_j)] \\non\\\\ &=&\\text{trace}[\\bb{L}_j^T\\bb{L}_j - 2\\bb{L}_j^T\\bb{V}\\bb{A}_j + \\bb{A}_j^T\\bb{A}_j].\\non \\end{eqnarray}\\] By properties of trace operator (\\cite{matbook}), we have \\[\\begin{eqnarray} \\frac{\\partial \\|\\bb{L}_j-\\bb{V}\\bb{A}_j\\|^2}{\\partial \\bb{A}_j} = -2 \\bb{V}^T\\bb{L}_j + 2 \\bb{A}_j\\non \\end{eqnarray}\\] so that we know the optimal \\(\\hat{\\bb{A}}_j = \\bb{V}^T\\bb{L}_j\\) . Plug it into the original problem, we then need to minimize \\[\\begin{equation} \\sum_{j=1}^M\\|\\bb{L}_j-\\bb{V}\\bb{V}^T\\bb{L}_j\\|^2\\non \\end{equation}\\] subject to \\(\\bb{V}^T\\bb{V}=\\bb{I}\\) . Note that \\[\\begin{eqnarray} \\sum_{j=1}^M\\|\\bb{L}_j-\\bb{V}\\bb{V}^T\\bb{L}_j\\|^2&=&\\sum_{j=1}^M\\text{trace}[\\bb{L}_j^T(\\bb{I}-\\bb{V}\\bb{V}^T)\\bb{L}_j)]\\non\\\\ &=&\\sum_{j=1}^M\\text{trace}[\\bb{L}_j^T\\bb{L}_j] - \\sum_{j=1}^M\\text{trace}[\\bb{L}_j^T\\bb{V}\\bb{V}^T\\bb{L}_j],\\non \\end{eqnarray}\\] the problem reduces to \\[\\begin{eqnarray} \\max_{\\bb{V}\\in \\mathbb{R}^{N\\times 2}}&&\\sum_{j=1}^M\\text{trace}[\\bb{L}_j^T\\bb{V}\\bb{V}^T\\bb{L}_j]\\non\\\\ \\text{s.t.}&&\\bb{V}^T\\bb{V}=\\bb{I}.\\non \\end{eqnarray}\\] Consider the Lagrangian function \\[\\begin{eqnarray} L(\\bb{V}, \\bb{A})=\\sum_{j=1}^M\\text{trace}[\\bb{L}_j^T\\bb{V}\\bb{V}^T\\bb{L}_j] + \\text{trace}[\\bb{A}(\\bb{V}^T\\bb{V}-\\bb{I})].\\non \\end{eqnarray}\\] We know that \\[\\begin{eqnarray} \\frac{\\partial L(\\bb{V}, \\bb{A})}{\\partial \\bb{V}} =2M[\\bb{L}\\bb{V} + \\bb{V}\\bar{\\bb{A}}],\\non \\end{eqnarray}\\] where \\[\\begin{eqnarray} \\bb{L} = \\frac{1}{M}\\sum_{j=1}^M\\bb{L}_j\\bb{L}_j^T,\\ \\ \\bar{\\bb{A}} = \\frac{\\bb{A}+\\bb{A}^T}{2M},\\non \\end{eqnarray}\\] are both symmetric. Therefore we know that the optimal \\(\\hat{\\bb{V}}\\) satisfies \\[\\begin{equation} \\bb{L}\\bb{V} = \\bb{V}\\cdot (-\\bar{\\bb{A}}),\\non \\end{equation}\\] which is an invariant subspace equation . The equation allow \\(\\hat{\\bb{V}}\\) to be an arbitrary orthogonal basis for the rank-2 subspace. Therefore, \\(\\hat{\\bb{V}}\\) is the \\(N\\times 2\\) matrix formed from the 2 largest eigenvectors of \\(\\bb{L}=\\frac{1}{M}\\sum_{j=1}^M\\bb{L}_j\\bb{L}_j^T\\) . Now we turn to the second part of the exercise where the objective becomes \\[\\begin{equation} \\sum_{j=1}^M\\min_{\\bb{A}_j}\\|\\bb{L}_j\\bb{A}_j^\\ast-\\bb{V}\\|^2.\\non \\end{equation}\\] Following the same logic above, we are able to show that the optimal \\(\\hat{\\bb{V}}\\) is the \\(N\\times 2\\) matrix formed from the 2 largest eigenvectors of \\(\\bar{\\bb{L}} = \\frac{1}{M}\\sum_{j=1}^M\\bb{L}_j(\\bb{L}_j^T\\bb{L}_j)^{-1}\\bb{L}_j^T\\) . For a detailed proof, see Ex. 14.10. The second criteria is a normalized version of the first one.","title":"Ex. 13.6"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-7/","text":"Ex. 13.7 Consider the application of nearest-neighbors to the easy and hard problems in the left panel of Figure 13.5. (a) Replicate the results in the left panel of Figure 13.5. (b) Estimate the misclassification errors using fivefold cross-validation, and compare the error rate curves to those in 1. (c) Consider and ``AIC-like'' penalization of the training set misclassification error. Specifically, add \\(2t/N\\) to the training set misclassification error, where \\(t\\) is the approximate number of parameters \\(N/r\\) , \\(r\\) being the number of nearest-neighbors. Compare plots of the resulting penalized misclassification error to those in 1 and 2. Which method gives a better estimate of the optimal number of nearest-neighbors: cross-validation or AIC? Soln. 13.7 (a) Please see Figure 1 for a replicate of left panel of Figure 13.5 in the text. Figure 1: Replicate of Figure 13.5 (b) and (c) Please see Figure 2 for comparison of estimated error rates using cross validation and those from 1. When the number of neighbor \\(r\\) is small, the AIC penalty \\(2/r\\) is large, so that AIC estimate is much greater than estimates from other two methods. In our simulation, cross-validation gives a better estimate. Estimated Error Rates Using CV and AIC-like Penalty Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 import numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score import plotly.graph_objects as go # Prepare data def generateData ( train_size = 100 , test_size = 1000 , p = 10 ): X_train = np . random . uniform ( 0 , 1 , size = ( train_size , p )) X_test = np . random . uniform ( 0 , 1 , size = ( test_size , p )) y_train_easy = generateYEasy ( X_train ) y_train_hard = generateYHard ( X_train ) y_test_easy = generateYEasy ( X_test ) y_test_hard = generateYHard ( X_test ) data_easy = { 'X_train' : X_train , 'y_train' : y_train_easy , 'X_test' : X_test , 'y_test' : y_test_easy } data_hard = { 'X_train' : X_train , 'y_train' : y_train_hard , 'X_test' : X_test , 'y_test' : y_test_hard } return [ data_easy , data_hard ] def generateYEasy ( X ): y = X [:, 0 ] . copy () y [ y < 0.5 ] = 0 y [ y > 0.5 ] = 1 return y def generateYHard ( X ): y = X [:, 0 ] . copy () for i in range ( X . shape [ 0 ]): prod = 1 for j in range ( 3 ): prod *= ( X [ i , j ] - 0.5 ) if prod > 0 : y [ i ] = 1 else : y [ i ] = 0 return y # a utility function to calculate error rate # of predictions def getErrorRate ( a , b ): return np . sum ( a != b ) / a . size # 1. Replicate Figure 13.5 def getErrorsKNN ( num_neighbor = None , data = None ): clf = KNeighborsClassifier ( n_neighbors = num_neighbor ) clf . fit ( data [ 'X_train' ], data [ 'y_train' ]) y_predict = clf . predict ( data [ 'X_test' ]) return getErrorRate ( y_predict , data [ 'y_test' ]) n_neighbors = np . arange ( 5 , 80 , 10 ) n_iterations = 10 error_mean_easy_list = [] error_std_easy_list = [] error_mean_hard_list = [] error_std_hard_list = [] for n_neighbor in n_neighbors : print ( 'Number of Neighbors is: {} ' . format ( n_neighbor )) tmp_errs_easy = [] tmp_errs_hard = [] for n_ite in range ( n_iterations ): print ( 'Running {} -th realization...' . format ( n_ite + 1 )) [ data_easy , data_hard ] = generateData () tmp_errs_easy . append ( getErrorsKNN ( n_neighbor , data_easy )) tmp_errs_hard . append ( getErrorsKNN ( n_neighbor , data_hard )) error_mean_easy_list . append ( np . mean ( np . asarray ( tmp_errs_easy ))) error_std_easy_list . append ( np . std ( np . asarray ( tmp_errs_easy ))) error_mean_hard_list . append ( np . mean ( np . asarray ( tmp_errs_hard ))) error_std_hard_list . append ( np . std ( np . asarray ( tmp_errs_hard ))) # Create traces fig = go . Figure () easy_color = '#eb4034' hard_color = '#993399' fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_easy_list , mode = 'lines' , name = 'Easy' , line_color = easy_color )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = np . asarray ( error_mean_easy_list ) + np . asarray ( error_std_easy_list ), mode = 'lines+markers' , name = 'Easy (upper limit)' , line_color = easy_color , line = { 'dash' : 'dash' } )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = np . asarray ( error_mean_easy_list ) - np . asarray ( error_std_easy_list ), mode = 'lines+markers' , name = 'Easy (lower limit)' , line_color = easy_color , line = { 'dash' : 'dash' } )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_hard_list , mode = 'lines' , name = 'Hard' , line_color = hard_color )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = np . asarray ( error_mean_hard_list ) + np . asarray ( error_std_hard_list ), mode = 'lines+markers' , name = 'Hard (upper limit)' , line_color = hard_color , line = { 'dash' : 'dash' } )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = np . asarray ( error_mean_hard_list ) - np . asarray ( error_std_hard_list ), mode = 'lines+markers' , name = 'Hard (lower limit)' , line_color = hard_color , line = { 'dash' : 'dash' } )) fig . update_layout ( xaxis_title = \"Number of Neighbors\" , yaxis_title = \"Misclassification Error\" , ) fig . show () # 2 and 3, five-fold CV and AIC error_mean_easy_cv_list = [] error_mean_hard_cv_list = [] error_mean_easy_AIC_list = [] error_mean_hard_AIC_list = [] [ data_easy , data_hard ] = generateData () for n_neighbor in n_neighbors : print ( 'CV=5, Number of Neighbors is: {} ' . format ( n_neighbor )) clf = KNeighborsClassifier ( n_neighbors = n_neighbor ) scores_easy = cross_val_score ( clf , data_easy [ 'X_train' ], data_easy [ 'y_train' ], scoring = 'accuracy' , cv = 5 ) scores_hard = cross_val_score ( clf , data_hard [ 'X_train' ], data_hard [ 'y_train' ], scoring = 'accuracy' , cv = 5 ) error_mean_easy_cv_list . append ( 1 - np . mean ( scores_easy )) error_mean_easy_AIC_list . append ( 1 - np . mean ( scores_easy ) + 2 / n_neighbor ) error_mean_hard_cv_list . append ( 1 - np . mean ( scores_hard )) error_mean_hard_AIC_list . append ( 1 - np . mean ( scores_hard ) + 2 / n_neighbor ) fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_easy_cv_list , mode = 'lines' , name = 'Easy_CV' )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_easy_AIC_list , mode = 'lines' , name = 'Easy_AIC' )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_hard_cv_list , mode = 'lines' , name = 'Hard_CV' )) fig . add_trace ( go . Scatter ( x = n_neighbors , y = error_mean_hard_AIC_list , mode = 'lines' , name = 'Hard_AIC' )) fig . show ()","title":"Ex. 13.7"},{"location":"ESL-Solution/_13-Prototypes-and-Nearest-Neighbors/ex13-8/","text":"Ex. 13.8 Generate data in two classes, with two features. These features are all independent Gaussian variates with standard deviation 1. Their mean vectors are \\((-1,-1)\\) in class 1 and \\((1, 1)\\) in class 2. To each feature vector apply a random rotation of angle \\(\\theta\\) , \\(\\theta\\) chosen uniformly from 0 to \\(2\\pi\\) . Generate 50 observations from each class to form the training set, and 500 in each class as the test set. Apply four different classifiers: (1) Nearest-neighbors. (2) Nearest-neighbors with hints: ten randomly rotated versions of each data point are added to the training set before applying nearest-neighbors. (3) Invariant metric nearest-neighbors, using Euclidean distance invariant to rotations about the origin. (4) Tangent distance nearest-neighbors. In each case choose the number of neighbors by tenfold cross-validation. Compare the results.","title":"Ex. 13.8 (TODO)"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-01/","text":"Ex. 14.1 Weights for clustering . Show that weighted Euclidean distance \\[\\begin{equation} d_e^{(w)}(x_i, x_{i'}) = \\frac{\\sum_{l=1}^pw_l(x_{il}-x_{i'l})^2}{\\sum_{l=1}^pw_l}\\non \\end{equation}\\] satisfies \\[\\begin{equation} d_e^{(w)}(x_i, x_{i'}) = d_e(z_i, z_{i'}) = \\sum_{l=1}^p(z_{il}-z_{i'l})^2,\\non \\end{equation}\\] where \\[\\begin{equation} z_{il} = x_{il}\\cdot \\left(\\frac{w_l}{\\sum_{l=1}^pw_l}\\right)^{1/2}.\\non \\end{equation}\\] Thus weighted Euclidean distance based on \\(x\\) is equivalent to unweighted Euclidean distance based on \\(z\\) . Soln. 14.1 By definition of \\(z_{il}\\) we have \\[\\begin{eqnarray} d_e(z_i, z_{i'}) &=& \\sum_{l=1}^p \\left(\\frac{w_l}{\\sum_{l=1}^pw_l}\\right)(x_{il}-x_{i'l})^2\\non\\\\ &=&\\frac{\\sum_{l=1}^pw_l(x_{il}-x_{i'l})^2}{\\sum_{l=1}^pw_l}\\non\\\\ &=&d_e^{(w)}(x_i, x_{i'}).\\non \\end{eqnarray}\\]","title":"Ex. 14.1"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-02/","text":"Ex. 14.2 Consider a mixture model density in \\(p\\) -dimensional feature space, \\[\\begin{equation} g(x) = \\sum_{k=1}^K\\pi_kg_k(x),\\non \\end{equation}\\] where \\(g_k=N(\\mu_k, \\bb{I}\\cdot \\sigma^2)\\) and \\(\\pi_k\\ge 0 \\ \\forall k\\) with \\(\\sum_{k}\\pi_k = 1\\) . Here \\(\\{\\mu_k, \\pi_k\\}, k=1,...,K\\) and \\(\\sigma^2\\) are unknown parameters. Suppose we have data \\(x_1, x_2, ..., x_N\\sim g(x)\\) and we wish to fit the mixture model. Write down the log-likelihood of the data. Derive an EM algorithm for computing the maximum likelihood estimates (see Section 8.1) Show that if \\(\\sigma\\) has a known value in the mixture model and we take \\(\\sigma\\ra 0\\) , then in a sense this EM algorithm coincides with \\(K\\) -means clustering. Soln. 14.2 This is similar to Ex. 13.1 . (1) The log-likelihood of the data is \\[\\begin{eqnarray} l(\\mu_k, \\pi_k, \\sigma) &=& \\log\\left(\\prod_{i=1}^Ng(x_i)\\right)\\non\\\\ &=&\\sum_{i=1}^N\\log\\left(\\sum_{k=1}^K\\pi_kg_k(x_i)\\right).\\non \\end{eqnarray}\\] (2) The EM algorithm is an iterate-until-convergence process, and each iteration consists of two steps, Expectation and Maximization . For Gaussian mixture model, the Expectation step computes the responsibilities of \\(x_i\\) for class \\(k\\) as \\[\\begin{eqnarray} r_i^{(k)} &=& \\frac{\\hat\\pi_k\\hat g_k(x_i)}{\\sum_{j=1}^K\\hat\\pi_j \\hat g_j(x_i)},\\non \\end{eqnarray}\\] where \\(\\hat g_j(x) = N(\\hat \\mu_j, \\bb{I}\\cdot \\hat\\sigma^2)\\) . The Maximization step then computes the weighted means and variances as \\[\\begin{eqnarray} \\hat\\mu_k &=& \\frac{\\sum_{i=1}^Nr_i^{(k)}x_i}{\\sum_{i=1}^Nr_i^{(k)}}.\\non\\\\ \\hat\\sigma^2 &=& \\frac{\\sum_{i=1}^N\\sum_{k=1}^Kr_i^{(k)}(x_i-\\hat\\mu_k)^T(x_i-\\hat\\mu_k)}{N}\\non\\\\ \\hat\\pi_k &=& \\frac{\\sum_{i=1}^Nr_i^{(k)}}{N}.\\non \\end{eqnarray}\\] Note that for the general case where the variance \\(\\Sigma_k\\) may vary for each \\(k\\) , the estimation above becomes \\[\\begin{equation} \\Sigma_k = \\frac{\\sum_{i=1}^Nr_i^{(k)}(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^T}{\\sum_{i=1}^Nr_i^{(k)}}.\\non \\end{equation}\\] (3) It's easy to very that, as \\(\\sigma\\ra 0\\) , \\(r_i^{(k)}\\ra 1\\) for the cluster \\(k\\) that is closest to \\(x_i\\) and \\(r_i^{(k)}\\ra 0\\) for other clusters. Therefore \\(K\\) -means algorithm is recovered.","title":"Ex. 14.2"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-07/","text":"Ex. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that \\(\\hat\\mu\\) is not unique, and characterize the family of equivalent solutions. Soln. 14.7 We need to miminize the reconstruction error \\[\\begin{equation} \\min_{\\mu, \\{\\lambda_i\\}, \\bb{V}_q} \\sum_{i=1}^N\\|x_i-\\mu-\\bb{V}_q\\lambda_i\\|^2.\\non \\end{equation}\\] Taking derivatives w.r.t to \\(\\mu\\) and \\(\\lambda_i\\) and setting them to zero, we get \\[\\begin{eqnarray} &&\\sum_{i=1}^N(x_i-\\mu-\\bb{V}_q\\lambda_i) = 0\\non\\\\ &&\\bb{V}_q^T(x_i-\\mu-\\bb{V}_q\\lambda_i) = 0.\\non \\end{eqnarray}\\] Since \\(\\bb{V}_q^T\\bb{V}_q = \\bb{I}\\) , from the condition on \\(\\lambda_i\\) we have \\[\\begin{equation} \\lambda_i = \\bb{V}_q^T(x_i-\\mu),\\non \\end{equation}\\] and we plug this into the condition for \\(\\mu\\) and we get \\[\\begin{equation} (\\bb{I} - \\bb{V}_q\\bb{V}_q^T)\\sum_{i=1}^N(x_i-\\mu) = 0.\\non \\end{equation}\\] Therefore, we see that \\[\\begin{eqnarray} \\hat\\mu &=& \\bar x\\non\\\\ \\hat\\lambda_i &=&\\bb{V}_q^T(x_i-\\bar x)\\non \\end{eqnarray}\\] is a set of optimized solutions, however not unique. The family of equivalent solutions is characterized by the set of \\(\\hat\\mu\\) that yields \\(\\sum_{i=1}^N(x_i-\\hat\\mu)\\) lying in the null space of \\((\\bb{I} - \\bb{V}_q\\bb{V}_q^T)\\) .","title":"Ex. 14.7"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-08/","text":"Ex. 14.8 Derive the solution (14.57) to the Procrustes problem (14.56). Derive also the solution to the Procrustes problem with scaling (14.58). Soln. 14.8 We first derive the solution (14.57). We need to solve the following optimization problem \\[\\begin{eqnarray} &&\\min_{\\mu, \\bb{R}}\\text{trace}\\left[\\left(\\bX_2 - (\\bX_1\\bR + \\bb{1}\\mu^T)\\right)^T\\left(\\bX_2 - (\\bX_1\\bR + \\bb{1}\\mu^T)\\right)\\right]\\non\\\\ &&\\text{s.t.} \\ \\bR^T\\bR = \\bb{I}.\\non \\end{eqnarray}\\] The Lagrangian for the problem is \\[\\begin{eqnarray} L(\\mu, \\bR, \\bb{A}) &=& \\text{trace}\\left[\\left(\\bX_2 - (\\bX_1\\bR + \\bb{1}\\mu^T)\\right)^T\\left(\\bX_2 - (\\bX_1\\bR + \\bb{1}\\mu^T)\\right)\\right]\\non\\\\ &&\\ + \\text{trace}\\left[\\bb{A}(\\bR^T\\bR-\\bb{I})\\right].\\non \\end{eqnarray}\\] By properties of derivatives on trace operator, we calculate the derivatives w.r.t to \\(\\mu, \\bR\\) and set them to zero, which gives \\[\\begin{eqnarray} \\frac{\\partial L(\\mu,\\bR,\\bb{A})}{\\partial\\mu} &=& -2\\bX_2^T\\cdot \\bb{1} + \\bR^T\\bX_1^T\\cdot \\bb{1} + 2N\\mu = 0\\non\\\\ \\frac{\\partial L(\\mu,\\bR,\\bb{A})}{\\partial \\bR} &=& 2\\bX_1^T\\bX_1\\bR - 2\\bX_1^T\\bX_2-2\\bX_1^T\\bb{1}\\mu^T + \\bR (\\bb{A} + \\bb{A}^T)=0\\non \\end{eqnarray}\\] From the first equation above, we get \\[\\begin{eqnarray} \\hat\\mu &=& \\frac{1}{N}\\bX_2^T\\cdot\\bb{1} - \\frac{1}{N}\\bR^T\\cdot \\bX_1^T\\cdot\\bb{1}\\non\\\\ &=&\\bar x_2 - \\bR^T\\bar x_1.\\non \\end{eqnarray}\\] Plug above into the second condition for \\(\\bR\\) we get (by some algebra) \\[\\begin{equation} \\label{eq:14-8a} \\tilde{\\bX}_1^T \\tilde{\\bX}_2 = \\tilde{\\bX}_1^T\\tilde{\\bX}_1\\bR + \\bR \\bar{\\bb{A}}, \\end{equation}\\] where \\[\\begin{eqnarray} \\tilde{\\bX}_i &=& \\bX_i - \\bb{1}\\bar x_i^T\\non\\\\ \\bar{\\bb{A}} &=& \\frac{\\bb{A} + \\bb{A}^T}{2}.\\non \\end{eqnarray}\\] Since \\(\\bR^T\\bR=\\bb{I}\\) , multiply \\(\\bR^T\\) on both sides of \\(\\eqref{eq:14-8a}\\) we get \\[\\begin{equation} \\bR^T\\tilde{\\bX}_1^T \\tilde{\\bX}_2 = \\bR^T\\tilde{\\bX}_1^T\\tilde{\\bX}_1\\bR + \\bar{\\bb{A}}.\\non \\end{equation}\\] Since both summands on the right hand side are symmetric, we know that \\(\\bR^T\\tilde{\\bX}_1^T \\tilde{\\bX}_2=\\tilde{\\bX}_2^T\\tilde{\\bX}_1^T\\bR\\) . Now it's easy to verify that \\[\\begin{equation} \\hat{\\bR} = \\bb{U}\\bb{V}^T\\non \\end{equation}\\] satisfies the condition, where \\(\\tilde{\\bX}_1 \\tilde{\\bX}_2=\\bb{U}\\bb{D}\\bb{V}^T\\) . For the problem with scaling (14.58), the idea is the similar. The Lagrangian is \\[\\begin{eqnarray} L(\\beta, \\bR, \\bb{A}) &=& \\text{trace}[(\\bX_2 - \\beta\\bX_1\\bR)^T(\\bX_2 - \\beta\\bX_1\\bR)] + \\text{trace}[\\bb{A}(\\bb{R}^T\\bR-\\bb{I})].\\non \\end{eqnarray}\\] We have \\[\\begin{eqnarray} \\frac{\\partial L(\\beta, \\bR, \\bb{A})}{\\partial \\beta} &=& -2\\text{trace}[\\bX_2\\bR^T\\bX_1^T] + 2\\beta \\text{trace}[\\bR^T\\bX_1^T\\bX_1\\bR]\\non\\\\ &=& -2\\text{trace}[\\bX_2\\bR^T\\bX_1^T] + 2\\beta \\text{trace}[ \\bR\\bR^T\\bX_1^T\\bX_1]\\non\\\\ &=& -2\\text{trace}[\\bX_2\\bR^T\\bX_1^T] + 2\\beta \\text{trace}[ \\bX_1^T\\bX_1].\\non \\end{eqnarray}\\] Therefore we see \\[\\begin{equation} \\label{eq:14.8-b} \\hat\\beta = \\frac{\\text{trace}[\\bR^T\\bX_1^T\\bX_2]}{\\|\\bX_1\\|_F}. \\end{equation}\\] With \\(\\hat\\beta\\) fixed, we already derived \\(\\hat\\bR\\) in the first part of this exercise, with \\(\\tilde{\\bX}_1\\) replaced with \\(\\hat\\beta\\tilde{\\bX}_1\\) . Therefore we know \\(\\hat\\bR = \\bb{U}\\bb{V}^T\\) as before. Plug \\(\\hat\\bR\\) into \\(\\eqref{eq:14.8-b}\\) we have \\[\\begin{eqnarray} \\hat\\beta &=& \\frac{\\text{trace}[\\bb{V}\\bb{U}^T\\bb{U}\\bb{D}\\bb{V}^T]}{\\|\\bX_1\\|_F}\\non\\\\ &=&\\frac{\\text{trace}[\\bb{D}]}{\\|\\bX_1\\|_F}.\\non \\end{eqnarray}\\]","title":"Ex. 14.8"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-10/","text":"Ex. 14.10 Derive the solution to the affine-invariant average problem (14.60). Apply it to the three S\u2019s, and compare the results to those computed in Exercise 14.9. Soln. 14.10 We need to solve the problem \\[\\begin{equation} \\min_{\\{\\bb{A}_l\\}_1^L, \\bb{M}}\\sum_{l=1}^N\\|\\bX_l\\bb{A}_l - \\bM\\|_F^2,\\non \\end{equation}\\] where \\(\\bA_l\\) are \\(p\\times p\\) nonsingular matrices and \\(\\bM^T\\bM = \\bb{I}\\) . The Lagrangian for the problem is \\[\\begin{eqnarray} L(\\bA_l, \\bM, \\bA) &=& \\sum_{l=1}^L\\text{trace}[(\\bA_l^T\\bX_l^T-\\bM^T)(\\bA_l\\bX_l-\\bM)] + \\text{trace}[\\bA(\\bM^T\\bM-\\bb{I})].\\non \\end{eqnarray}\\] Taking derivative w.r.t \\(\\bA_l\\) and setting it to be zero we have \\[\\begin{eqnarray} \\frac{\\partial L(\\bA_l, \\bM, \\bA)}{\\partial \\bA_l} &=& 2(\\bX_l^T\\bX_l\\bA_l - \\bX_l^T\\bM)=0,\\non \\end{eqnarray}\\] thus we know \\(\\bA_l = (\\bX_l^T\\bX_l)^{-1}\\bX_l\\bM\\) . Denote \\(\\bb{H}_l = \\bX_l(\\bX_l^T\\bX_l)^{-1}\\bX_l\\) , and plug the solution for \\(\\bA_l\\) into the original problem, it reduces to minimize \\[\\begin{eqnarray} &&\\sum_{l=1}^L \\text{trace}[\\bM^T(\\bb{H}_l^T-\\bb{I})(\\bb{H}_l-\\bb{I})\\bM]\\non\\\\ &=&\\sum_{l=1}^L\\text{trace}[\\bM^T(\\bb{I}-\\bb{H}_l)\\bM]\\non\\\\ &=&L\\text{trace}\\left[\\bM^T\\bM - \\bM^T\\bar{\\bb{H}}\\bM\\right]\\non\\\\ &=&Lq - L\\text{trace}[\\bM^T\\bar{\\bb{H}}\\bM]. \\end{eqnarray}\\] Now we see the problem reduces to \\[\\begin{equation} \\max_{\\bM} \\text{trace}[\\bM^T\\bar{\\bb{H}}\\bM] \\end{equation}\\] under the condition that \\(\\bM^T\\bM = \\bb{I}\\) . The optimal solution is achieved when \\(\\bM\\) is an orthogonal basis of the eigenspace associated with the largest eigenvalues of \\(\\bar{\\bb{H}}\\) . That is, \\(\\bM\\) is the \\(N\\times p\\) matrix formed from the \\(p\\) largest eigenvectors of \\(\\bar{\\bb{H}}\\) . The proof follows directly from Courant-Fisher characterization. We give a short proof here. Consider the Lagrangian again and taking derivatives w.r.t \\(\\bM\\) , we get \\[\\begin{equation} \\bar{\\bb{H}}\\bM = \\bM \\bar{\\bb{A}},\\non \\end{equation}\\] for some symmetric \\(\\bar{\\bb{A}}\\) . This is an invariant subspace equation. The equations allow \\(\\bM\\) to be an arbitrary orthogonal basis for the rank- \\(p\\) subspace. It's then clear the choice of \\(\\bM\\) described earlier is an optimal solution.","title":"Ex. 14.10"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-11/","text":"Ex. 14.11 Classical multidimensional scaling . Let \\(\\bb{S}\\) be the centered inner product matrix with elements \\(\\langle x_i-\\bar x, x_j-\\bar x \\rangle\\) . Let \\(\\lambda_1 > \\lambda_2 > ... > \\lambda_k\\) be the \\(k\\) largest eigenvalues of \\(\\bb{S}\\) , with associated eigenvectors \\(\\bb{E}_k=(\\bb{e}_1, \\bb{e}_2, ..., \\bb{e}_k)\\) . Let \\(\\bb{D}_k\\) be a diagonal matrix with diagonal entries \\(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, ...., \\sqrt{\\lambda_k}\\) . Show that the solutions \\(z_i\\) to the classical scaling problem (14.100) are the rows of the \\(\\bb{E}_k\\bb{D}_k\\) . Soln. 14.11 Similar to \\(\\bS\\) , let \\(\\bM\\) be the be the centered inner product matrix with elements \\(\\langle z_i-\\bar z, z_j-\\bar z \\rangle\\) , and we can write \\[\\begin{equation} \\bM = \\begin{pmatrix} z_1^T \\\\ z_2^T \\\\ \\vdots \\\\ z_N^T \\end{pmatrix} \\begin{pmatrix} z_1 & z_2 & \\dots & z_N \\end{pmatrix}.\\non \\end{equation}\\] Then (14.100) becomes \\[\\begin{equation} \\text{trace}[(\\bS-\\bM)^T(\\bS-\\bM)] = \\|\\bS-\\bM\\|_F^2.\\non \\end{equation}\\] Note that \\(z_i\\in \\mathbb{R}^k\\) , the problem reduces to the best rank- \\(k\\) approximation for \\(\\bS\\) . Consider the eigen-decomposition of \\(\\bS=\\bb{E}\\bb{D}^2\\bb{E}^T\\) , from Ex. 13.5 , we know that \\[\\begin{eqnarray} \\bM &=& \\sum_{l=1}^k\\lambda_l\\bb{e}_l\\bb{e}_l^T\\non\\\\ &=& \\bb{E}_k\\bb{D}_k \\cdot (\\bb{E}_k\\bb{D}_k)^T\\non\\\\ &=& \\begin{pmatrix} \\sqrt{\\lambda_1}\\bb{e}_1 & \\sqrt{\\lambda_2}\\bb{e}_2 & \\dots & \\sqrt{\\lambda_k}\\bb{e}_k \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\lambda_1}\\bb{e}_1^T \\\\ \\sqrt{\\lambda_2}\\bb{e}_2^T \\\\ \\vdots \\\\ \\sqrt{\\lambda_k}\\bb{e}_k \\end{pmatrix} .\\non \\end{eqnarray}\\] thus we see solutions \\(z_i\\) correspond to the rows of \\(\\bb{E}_k\\bb{D}_k\\) . Specifically, \\(z_i^T\\in\\mathbb{R}^{1\\times k}\\) are the rows of \\(\\bb{E}_k\\bb{D}_k\\) .","title":"Ex. 14.11"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-18/","text":"Ex. 14.18 Consider the regularized log-likelihood for the density estimation problem arising in ICA, \\[\\begin{equation} \\frac{1}{N}\\sum_{i=1}^N[\\log(\\phi(s_i))+g(s_i)] - \\int \\phi(t)e^{g(t)}dt - \\lambda \\int \\{g^{'''}(t)\\}^2dt.\\non \\end{equation}\\] The solution \\(\\hat g\\) is a quartic smoothing spline, and can be written as \\(\\hat g(s) = \\hat q(s) + \\hat{q}_\\bot (s)\\) , where q is a quadratic function (in the null space of the penalty). Let \\(q(s) = \\theta_0 + \\theta_1s + \\theta_2s^2\\) . By examining the stationarity conditions for \\(\\theta^k\\) , \\(k = 1, 2, 3\\) , show that the solution \\(\\hat f = \\phi e^{\\hat g}\\) is a density, and has mean zero and variance one. If we used a second-derivative penalty \\(\\int \\{g^{''}(t)\\}^2(t)dt\\) instead, what simple modification could we make to the problem to maintain the three moment conditions? Soln. 14.18 Considering the stationarity condition, if we differentiate the log-likelihood w.r.t to \\(\\theta_0\\) , we should get zero at the optimal value \\(\\hat\\theta_0\\) . By the assumption of \\(\\hat g(s)\\) , the differential is \\[\\begin{equation} 1 - \\int \\phi(t)e^{\\hat g(t)} dt = 0,\\non \\end{equation}\\] therefore we know \\(\\hat f= \\phi e^{\\hat g}\\) is a density. Similarly for \\(\\theta_1\\) , the differential is \\[\\begin{equation} \\bar s - \\int \\phi(t)e^{\\hat g(t)}tdt=0-\\text{mean}(\\hat f)=0,\\non \\end{equation}\\] since \\(S\\) is assumed to have zero mean. At last, for \\(\\theta_2\\) , the differential is \\[\\begin{equation} \\frac{1}{N}\\sum_{i=1}^Ns_i^2 - \\int \\phi(t)e^{\\hat g(t)}t^2dt=0.\\non \\end{equation}\\] Since both \\(S\\) and \\(\\hat f\\) have zero mean, we know that the variance of \\(\\hat f\\) equals the variance of \\(S\\) , which is assumed to be 1. If we use a second-derivative penalty instead, then \\(\\hat g\\) becomes a cubic smoothing spline, and the first two conditions ( \\(\\hat f\\) is a density and has zero mean) are still preserved via the same arguments above. We can modify the problem by adding a Lagrangian multiplier to accommodate the constraint for the variance.","title":"Ex. 14.18"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-19/","text":"Ex. 14.19 If \\(\\bA\\) is \\(p\\times p\\) orthogonal, show that the first term in (14.92) on page 568 \\[\\begin{equation} \\sum_{j=1}^p\\sum_{i=1}^N\\log(\\phi(a_j^Tx_i)),\\non \\end{equation}\\] with \\(a_j\\) the \\(j\\) th column of \\(\\bA\\) , does not depend on \\(\\bA\\) . Soln. 14.19 Since \\(\\bb{A}\\) is orthogonal, we have \\[\\begin{eqnarray} \\sum_{j=1}^p\\sum_{i=1}^N\\log(\\phi(a_j^Tx_i)) &=& \\sum_{i=1}^N\\sum_{j=1}^p\\log(\\phi(a_j^Tx_i))\\non\\\\ &=&\\sum_{i=1}^N(2\\pi)^{-p/2}e^{-x_i^T\\bb{A}\\bb{A}^Tx_2/2}\\non\\\\ &=&(2\\pi)^{-p/2}\\sum_{i=1}^Ne^{-x_i^Tx_2/2}\\non \\end{eqnarray}\\] does not depend on \\(\\bb{A}\\) .","title":"Ex. 14.19"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-20/","text":"Ex. 14.20 Fixed point algorithm for ICA (Hyv \\(\\ddot{a}\\) rinen et al., 2001) Consider maximizing \\(C(a) = E\\{g(a^TX)\\}\\) with respect to \\(a\\) , with \\(\\|a\\|=1\\) and \\(\\text{Cov}(X)=I\\) . Use a Lagrange multiplier to enforce the norm constraint, and write down the first two derivatives of the modified criterion. Use the approximation \\[\\begin{equation} E\\{XX^Tg^{''}(a^TX)\\}\\approx E\\{XX^T\\}E\\{g^{''}(a^TX)\\}\\non \\end{equation}\\] to show that the Newton update can be written as the fixed-point update (14.96). Soln. 14.20 The Lagrangian multiplier is \\[\\begin{equation} L(a, \\lambda) = E[g(a^T\\bX)] + \\lambda(a^Ta-1).\\non \\end{equation}\\] We have \\[\\begin{eqnarray} \\frac{\\partial L(a, \\lambda)}{\\partial a} &=& E[g'(a^T\\bX)\\bX] + 2 \\lambda a\\non \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\frac{\\partial^2 L(a, \\lambda)}{\\partial a \\partial a^T} &=& E[g''(a^T\\bX)\\bX\\bX^T] + 2\\lambda.\\non \\end{eqnarray}\\] Newton's method performs the iteration \\[\\begin{equation} x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}.\\non \\end{equation}\\] Therefore, in this case, we know \\[\\begin{eqnarray} a_j &\\leftarrow& a_j - \\frac{E[g'(a_j^T\\bX)\\bX] + 2 \\lambda a_j}{E[g''(a_j^T\\bX)\\bX\\bX^T] + 2\\lambda}\\non\\\\ &\\approx& a_j - \\frac{E[g'(a_j^T\\bX)\\bX] + 2 \\lambda a_j}{E[g''(a_j^T\\bX)]E[\\bX\\bX^T] + 2\\lambda}\\non\\\\ &=&\\frac{a_jE[g''(a_j^T\\bX)] + 2\\lambda a_j - E[g'(a_j^T\\bX)\\bX] - 2 \\lambda a_j}{E[g''(a_j^T\\bX)] + 2\\lambda} \\ \\text{ ( note } \\text{Cov}(\\bX) = \\bb{I}\\ )\\non\\\\ &\\propto&E\\{\\bX g'(a_j^T\\bX) - E[g''(a^T_j\\bX)]a_j\\}.\\non \\end{eqnarray}\\] Therefore the Newton update can be written as the fixed-point update (14.96) in the text (followed by the orthogonalization of \\(\\bA\\) ).","title":"Ex. 14.20"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-23/","text":"Ex. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange, 2007). A function \\(g(x, y)\\) is said to minorize a function \\(f(x)\\) if \\[\\begin{equation} g(x, y) \\le f(x), \\ g(x,x) = f(x)\\non \\end{equation}\\] for all \\(x, y\\) in the domain. This is useful for maximizing \\(f(x)\\) since it is easy to show that \\(f(x)\\) is nondecreasing under the update \\[\\begin{equation} x^{s+1} = \\underset{x}{\\operatorname{argmax}} g(x,x^s).\\non \\end{equation}\\] There are analogous definitions for majorization , for minimizing a function \\(f(x)\\) . The resulting algorithms are known as MM algorithms, for minorize-maximize or majorize-minimize (Lange, 2004). It also can be shown that the EM algorithm (8.5) is an example of an MM algorithm: see Section 8.5.3 and Exercise 8.2 for details. (a) Consider maximization of the function \\(L(\\bb{W}, \\bb{H})\\) in (14.73), written here without the matrix notation \\[\\begin{equation} L(\\bb{W}, \\bb{H}) = \\sum_{i=1}^N\\sum_{j=1}^p\\left[x_{ij}\\log\\left(\\sum_{k=1}^rw_{ik}h_{kj}\\right)-\\sum_{k=1}^rw_{ik}h_{kj}\\right].\\non \\end{equation}\\] Using the concavity of \\(\\log(x)\\) , show that for any set of \\(r\\) values \\(y_k\\ge 0\\) and \\(0\\le c_k \\le 1\\) with \\(\\sum_{k=1}^r c_k = 1\\) , \\[\\begin{equation} \\log\\left(\\sum_{k=1}^r y_k\\right)\\ge \\sum_{k=1}^r c_k \\log(y_k/c_k).\\non \\end{equation}\\] Hence \\[\\begin{equation} \\log\\left(\\sum_{k=1}^r w_{ik}h_{kj}\\right)\\ge \\sum_{k=1}^r\\frac{a^s_{ikj}}{b^s_{ij}}\\log\\left(\\frac{b^s_{ij}}{a^s_{ikj}}w_{ik}h_{kj}\\right),\\non \\end{equation}\\] where \\[\\begin{equation} a^s_{ikj} = w^s_{ik}h^s_{kj} \\ \\text{and} \\ b^s_{ij} = \\sum_{k=1}^rw_{ik}^sh^s_{kj},\\non \\end{equation}\\] and \\(s\\) indicates the current iteration. (b) Hence show that, ignoring constants, the function \\[\\begin{eqnarray} g(\\bW, \\bb{H}|\\bW^s, \\bb{H}^s) &=& \\sum_{i=1}^N\\sum_{j=1}^p\\sum_{k=1}^rx_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}\\left(\\log w_{ik} + \\log h_{kj}\\right)\\non\\\\ &&\\ \\ \\ \\ - \\sum_{i=1}^N\\sum_{j=1}^p\\sum_{k=1}^rw_{ik}h_{kj}\\non \\end{eqnarray}\\] minorizes \\(L(\\bW, \\bb{H})\\) . (c) Set the partial derivatives of \\(g(\\bW, \\bb{H}|\\bW^s, \\bb{H}^s)\\) to zero and hence derive the updating steps (14.74). Soln. 14.23 (a) Suppose that \\(f(x)\\) is convex, then we claim \\[\\begin{equation} \\label{eq:14-23a} f\\left(\\sum_{i=1}^nc_ix_i\\right) \\le \\sum_{i=1}^nc_if(x_i), \\end{equation}\\] where \\(\\sum_{i=1}^nc_i = 1\\) and \\(c_i\\ge 0\\) for \\(i=1,...,n\\) . To prove it, we use mathematical induction. By definition, we know that \\(\\eqref{eq:14-23a}\\) holds when \\(n=2\\) . Suppose \\(n > 2\\) , assume that \\(0 < c_n < 1\\) and denote \\(c_0 = 1-c_n > 0\\) and \\(x_0 = \\sum_{i=1}^{n-1}c_ix_i/c_0\\) . Then we have \\[\\begin{eqnarray} f\\left(\\sum_{i=1}^nc_ix_i\\right) &=& f\\left(\\sum_{i=1}^{n-1}c_ix_i + c_n x_n\\right)\\non\\\\ &=&f(c_0x_0 + c_nx_n)\\non\\\\ &\\le& c_0f(x_0) + c_nf(x_n)\\non\\\\ &=&c_0 f\\left(\\sum_{i=1}^{n-1}c_ix_i/c_0\\right) + c_nf(x_n)\\non\\\\ &\\le& c_0 \\frac{1}{c_0}\\sum_{i=1}^{n-1}c_if(x_i) + c_n f(x_n)\\non\\\\ &=&\\sum_{i=1}^nc_if(x_i),\\non \\end{eqnarray}\\] which is \\(\\eqref{eq:14-23a}\\) . For concave function \\(f\\) , we have \\[\\begin{equation} f\\left(\\sum_{i=1}^nc_ix_i\\right) \\ge \\sum_{i=1}^nc_if(x_i).\\non \\end{equation}\\] Therefore we know the claim in (a) is correct. (b) We only need focus on the first term. Note that \\[\\begin{eqnarray} g(\\bW, \\bb{H}|\\bW^s, \\bb{H}^s) &=& \\sum_{i=1}^N\\sum_{j=1}^p\\sum_{k=1}^rx_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}\\left(\\log w_{ik} + \\log h_{kj}\\right)\\non\\\\ &=&\\sum_{i=1}^N\\sum_{j=1}^p\\sum_{k=1}^rx_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}\\log \\left(\\frac{b^s_{ij}}{a^s_{ikj}}w_{ik}h_{kj}\\right)- C,\\non \\end{eqnarray}\\] where the constant \\(C\\) (independent of \\(\\bW\\) and \\(\\bb{H}\\) ) is \\[\\begin{equation} C = \\sum_{i=1}^N\\sum_{j=1}^p\\sum_{k=1}^rx_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}\\log \\frac{b^s_{ij}}{a^s_{ikj}}.\\non \\end{equation}\\] Therefore, by (a), we have \\[\\begin{equation} g(\\bW, \\bb{H}|\\bW^s, \\bb{H}^s) \\le L(\\bb{W}, \\bb{H}),\\non \\end{equation}\\] and it's easy to verify that the equation holds when \\(\\bW^s = \\bW\\) and \\(\\bb{H}^s = \\bb{H}\\) . (c) Fix \\(i\\) and \\(k\\) , we have \\[\\begin{eqnarray} \\frac{\\partial g(\\bW, \\bb{H}|\\bW^s, \\bb{H}^s)}{\\partial w_{ik}} &=& \\sum_{j=1}^p x_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}\\frac{1}{w_{ik}} - \\sum_{j=1}^ph_{kj}\\non\\\\ &=&0,\\non \\end{eqnarray}\\] so that \\[\\begin{eqnarray} w_{ik} &=& \\frac{\\sum_{j=1}^px_{ij}\\frac{a^s_{ikj}}{b^s_{ij}}}{\\sum_{j=1}^ph_{kj}}\\non\\\\ &=&\\left(\\sum_{j=1}^px_{ij} \\frac{w^s_{ik}h^s_{kj}}{\\sum_{k'=1}^rw^s_{ik'}h^s_{k'j}}\\right)\\Bigg/\\sum_{j=1}^ph_{kj}\\non\\\\ &=&w_{ik}^s\\frac{\\sum_{j=1}^ph_{kj}x_{ij}/(\\bW\\bb{H})_{ij}}{\\sum_{j=1}^ph_{kj}},\\non \\end{eqnarray}\\] which is (14.74) in the text. The same arguments apply to the update of \\(h_{kj}\\) , thus we omit it for brevity.","title":"Ex. 14.23"},{"location":"ESL-Solution/_14-Unsupervised-Learning/ex14-24/","text":"Ex. 14.24 Consider the non-negative matrix factorization (14.72) in the rank one case ( \\(r=1\\) ). (a) Show that the updates (14.74) reduce to \\[\\begin{eqnarray} w_i &\\leftarrow& w_i\\frac{\\sum_{j=1}^px_{ij}}{\\sum_{j=1}^pw_ih_j}\\non\\\\ h_j &\\leftarrow& h_j\\frac{\\sum_{i=1}^Nx_{ij}}{\\sum_{i=1}^Nw_ih_j}\\non \\end{eqnarray}\\] where \\(w_i = w_{i1}, h_j = h_{1j}\\) . This is an example of the iterative proportional scaling procedure, applied to the independence model for a two-way contingency table (Fienberg, 1977, for example.) (b) Show that the final iterates have the explicit form \\[\\begin{equation} w_i = c\\cdot \\frac{\\sum_{j=1}^px_{ij}}{\\sum_{i=1}^N\\sum_{j=1}^px_{ij}}, \\ \\ h_k = \\frac{1}{c}\\cdot \\frac{\\sum_{i=1}^Nx_{ik}}{\\sum_{i=1}^N\\sum_{j=1}^px_{ij}}\\non \\end{equation}\\] for any constant \\(c > 0\\) . These are equivalent to the usual row and column estimates for a two-way independence model. Soln. 14.24 (a) When \\(r = 1\\) , \\((\\bW\\bb{H})_{ij} = w_ih_j\\) , therefore (14.74) becomes \\[\\begin{eqnarray} w_i &\\leftarrow& w_i\\frac{\\sum_{j=1}^ph_jx_{ij}/w_ih_j}{\\sum_{j=1}^ph_j}\\non\\\\ &=&w_i\\frac{\\sum_{j=1}^px_{ij}}{\\sum_{j=1}^pw_ih_j}.\\non \\end{eqnarray}\\] Similar arguments apply to \\(h_j\\) with \\[\\begin{equation} hj \\leftarrow h_j\\frac{\\sum_{i=1}^Nx_{ij}}{\\sum_{i=1}^Nw_ih_j}.\\non \\end{equation}\\] (b) From (a) we know that, in the final iterates, plug the update formula for \\(h_j\\) into \\(w_i\\) we get \\[\\begin{eqnarray} w_i &\\leftarrow& \\frac{\\sum_{j=1}^p x_{ij}}{\\sum_{j=1}^p h_j}\\non\\\\ &=&\\frac{\\sum_{j=1}^p x_{ij}}{\\sum_{j=1}^p\\frac{\\sum_{i=1}^Nx_{ij}}{\\sum_{i=1}^Nw_i}}\\non\\\\ &=&\\sum_{i=1}^Nw_i \\cdot \\frac{\\sum_{j=1}^p x_{ij}}{\\sum_{j=1}^p\\sum_{i=1}^Nx_{ij}}\\non\\\\ &=&c \\cdot \\frac{\\sum_{j=1}^p x_{ij}}{\\sum_{j=1}^p\\sum_{i=1}^Nx_{ij}},\\non \\end{eqnarray}\\] where \\(c>0\\) could be any constant. Similar arguments apply to \\(h_k\\) .","title":"Ex. 14.24"},{"location":"ESL-Solution/_15-Random-Forests/ex15-1/","text":"Ex. 15.1 Derive the variance formula (15.1). This appears to fail if \\(\\rho\\) is negative; diagnose the problem in this case. Soln. 15.1 We have \\[\\begin{eqnarray} \\text{Var}\\left(\\frac{\\sum_{i=1}^BX_i}{B}\\right) &=&\\frac{1}{B^2}\\sum_{i=1}^B\\text{Var}(X_i) + \\frac{1}{B^2}\\sum_{i\\neq j}^B\\text{Cov}(X_i, X_j)\\non\\\\ &=&\\frac{\\sigma^2}{B} + \\frac{B-1}{B}\\sigma^2\\rho\\non\\\\ &=&\\sigma^2\\rho + \\frac{1-\\rho}{B}\\sigma^2.\\non \\end{eqnarray}\\] The assumption implicitly assumes that \\(\\rho \\ge -\\frac{1}{B-1}\\) by noting the variance above is non-negative. When \\(B\\) is large, this (negative) lower bound is close to zero.","title":"Ex. 15.1"},{"location":"ESL-Solution/_15-Random-Forests/ex15-2/","text":"Ex. 15.2 Show that as the number of bootstrap samples \\(B\\) gets large, the oob error estimate for a random forest approaches its \\(N\\) -fold CV error estimate, and that in the limit, the identity is exact.","title":"Ex. 15.2 (TODO)"},{"location":"ESL-Solution/_15-Random-Forests/ex15-3/","text":"Ex. 15.3 Consider the simulation model used in Figure 15.7. Binary observations are generated with probabilities \\[\\begin{equation} \\text{Pr}(Y=1|X) = q + (1-2q)\\cdot 1\\left[\\sum_{j=1}^JX_j>J/2\\right],\\non \\end{equation}\\] where \\(X\\sim U[0,1]^p, 0\\le q\\le \\frac{1}{2}\\) , and \\(J\\le p\\) is some predefined (even) number. Describe this probability surface, and give the Bayes error rate. Soln. 15.3 In this data model, the true decision boundary depends on the first \\(J\\) variables. Specifically, it depends on the sum of \\(J\\) uniform variables on \\([0,1]\\) exceeds its mean \\(J/2\\) . Recall the Bayes error rate (e.g., (2.23) in the text), we know the Bayes error rate is \\[\\begin{equation} 1 - E\\left(\\max_{j\\in \\{0,1\\}} \\text{Pr}(Y=j|X)\\right) = q.\\non \\end{equation}\\]","title":"Ex. 15.3"},{"location":"ESL-Solution/_15-Random-Forests/ex15-4/","text":"Ex. 15.4 Suppose \\(x_i, i =1,...,N\\) are iid \\((\\mu, \\sigma^2)\\) . Let \\(\\bar x_1^\\ast\\) and \\(\\bar x_2^\\ast\\) be two bootstrap realizations of the sample mean. Show that the sampling correlation \\(\\text{corr}(\\bar x_1^\\ast, \\bar x_2^\\ast)=\\frac{n}{2n-1}\\approx 50\\%\\) . Along the way, derive \\(\\text{var}(\\bar x_1^\\ast)\\) and the variance of the bagged mean \\(\\bar x_{\\text{bag}}\\) . Here \\(\\bar x\\) is a linear statistic; bagging produces no reduction in variance for linear statistics. Soln. 15.4 Denote \\[\\begin{eqnarray} \\bar x_1^\\ast = \\frac{1}{n}\\sum_{i=1}^n\\hat x_i,\\ \\ \\bar x_2^\\ast = \\frac{1}{n}\\sum_{i=1}^n\\tilde x_i,\\non \\end{eqnarray}\\] where \\(\\{\\hat x_i, i=1,...,n\\}\\) and \\(\\{\\tilde x_i, i=1,...,n\\}\\) are realizations from the first and the second bootstrap respectively. Note that both \\(\\hat x_i\\) and \\(\\tilde x_i\\) are sampled from the empirical distribution of \\(\\{x_i, i=1,...,n\\}\\) . Therefore, for any \\(1\\le i\\le n\\) , we have \\[\\begin{eqnarray} E[\\hat x_i] &=& E[\\tilde x_i] = \\mu,\\non\\\\ \\text{var}(\\hat x_i) &=& \\text{var}(\\tilde x_i) = \\sigma^2.\\non \\end{eqnarray}\\] Also, for any \\(1\\le i, j\\le n\\) , we have \\[\\begin{eqnarray} \\text{cov}(\\hat x_i, \\tilde x_j) = \\frac{\\sigma^2}{n}.\\non \\end{eqnarray}\\] Then, we know \\[\\begin{eqnarray} \\text{cov}(\\bar x_1^\\ast, \\bar x_2^\\ast) = \\frac{1}{n^2}\\left(\\sum_{i,j}^n\\text{cov}(\\hat x_i, \\tilde x_j)\\right)=\\frac{\\sigma^2}{n},\\non \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\text{var}(\\bar x_1^\\ast) &=&\\text{var}\\left(\\frac{1}{n}\\sum_{i=1}^n\\tilde x_i\\right)\\non\\\\ &=&\\frac{1}{n^2}\\left(\\sum_{i=1}^n\\text{var}(x_i) + \\sum_{j\\neq k}^n\\text{cov}(\\hat x_j, \\hat x_k)\\right)\\non\\\\ &=&\\frac{1}{n^2}\\left(n\\cdot \\sigma^2 + (n^2-n)\\cdot \\frac{\\sigma^2}{n}\\right)\\non\\\\ &=&\\frac{(2n-1)\\sigma^2}{n^2}.\\non \\end{eqnarray}\\] Therefore we know that \\[\\begin{equation} \\text{corr}(\\bar x_1^\\ast, \\bar x_2^\\ast) = \\frac{\\text{cov}(\\bar x_1^\\ast, \\bar x_2^\\ast)}{\\sqrt{\\text{var}(\\bar x_1^\\ast)\\text{var}(\\bar x_2^\\ast)}} = \\frac{n}{2n-1}.\\non \\end{equation}\\] We have already derived \\(\\text{var}(\\bar x_1^\\ast)\\) above. For \\(\\bar x_{\\text{bag}}\\) , assume we have \\(B\\) realizations, then \\[\\begin{eqnarray} \\text{var}(\\bar x_{\\text{bag}}) &=& \\text{var}\\left(\\frac{1}{B}\\sum_{i=1}^B\\bar x_i^\\ast\\right)\\non\\\\ &=&\\frac{1}{B^2}\\sum_{i=1}^B\\text{var}(\\bar x_i^\\ast) + \\frac{1}{B^2}\\sum_{j\\neq k}^B\\text{cov}(\\bar x_j^\\ast, \\bar x_k^\\ast)\\non\\\\ &=&\\frac{1}{B}\\cdot\\frac{(2n-1)\\sigma^2}{n^2}+\\frac{B-1}{B}\\cdot \\frac{\\sigma^2}{n}\\non\\\\ &=&\\frac{(2n-1)+(B-1)n}{Bn^2}\\cdot \\sigma^2.\\non \\end{eqnarray}\\]","title":"Ex. 15.4"},{"location":"ESL-Solution/_15-Random-Forests/ex15-5/","text":"Ex. 15.5 Show that the sampling correlation between a pair of random forest trees at a point \\(x\\) is given by \\[\\begin{equation} \\rho(x) = \\frac{\\text{Var}_{\\bb{Z}}[E_{\\Theta|\\bb{Z}}T(x; \\Theta(\\bb{Z}))]}{\\text{Var}_{\\bb{Z}}[E_{\\Theta|\\bb{Z}}T(x; \\Theta(\\bb{Z}))] + E_\\bb{Z}\\text{Var}_{\\Theta|\\bb{Z}}[T(x;\\Theta(\\bb{Z}))]}.\\non \\end{equation}\\] The term in the numerator is \\(\\text{Var}_\\bb{Z}[\\hat f_{\\text{rf}}(x)]\\) , and the second term in the denominator is the expected conditional variance due to the randomization in random forests. Soln. 15.5 Recall (15.6) in the text, we have \\[\\begin{eqnarray} \\rho(x) &=& \\text{corr}[T(x;\\Theta_1(\\bb{Z})), T(x;\\Theta_2(\\bb{Z}))]\\non\\\\ &=&\\frac{\\text{cov}(T(x;\\Theta_1(\\bb{Z})), T(x;\\Theta_2(\\bb{Z})))}{\\sqrt{\\text{var}(T(x;\\Theta_1(\\bb{Z})))\\text{var}(T(x;\\Theta_2(\\bb{Z})))}}.\\non \\end{eqnarray}\\] Note that \\[\\begin{eqnarray} &&\\text{cov}(T(x;\\Theta_1(\\bb{Z})), T(x;\\Theta_2(\\bb{Z})))\\non\\\\ &=&E_{\\bb{Z}}[\\text{cov}_{\\Theta|\\bb{Z}}(T(x;\\Theta_1(\\bb{Z})), T(x;\\Theta_2(\\bb{Z})))]\\non\\\\ &&+\\text{cov}_{\\bb{Z}}(E_{\\Theta_1|\\bb{Z}}[T(x;\\Theta_1(\\bb{Z})],E_{\\Theta_2|\\bb{Z}}[T(x;\\Theta_2(\\bb{Z})])\\non\\\\ &=&0+\\text{cov}_{\\bb{Z}}(E_{\\Theta_1|\\bb{Z}}[T(x;\\Theta_1(\\bb{Z})],E_{\\Theta_2|\\bb{Z}}[T(x;\\Theta_2(\\bb{Z})])\\non\\\\ &=&\\text{Var}_{\\bb{Z}}[E_{\\Theta|\\bb{Z}}T(x; \\Theta(\\bb{Z}))],\\non \\end{eqnarray}\\] where the last equation follows from \\(T(x;\\Theta_1(\\bb{Z}))\\) and \\(T(x;\\Theta_2(\\bb{Z}))\\) are independent and have the same distribution. This is the numerator in the formula. The denominator follows directly from (15.9) in the text.","title":"Ex. 15.5"},{"location":"ESL-Solution/_15-Random-Forests/ex15-6/","text":"Ex. 15.6 Fit a series of random-forest classifiers to the spam data, to explore the sensitivity to the parameter \\(m\\) . Plot both the oob error as well as the test error against a suitably chosen range of values for \\(m\\) . Soln. 15.6 Figure 1 below shows the sensitivity to the parameter \\(m\\) , whose values are chosen from 1 to \\(\\sqrt{p}\\approx 8\\) (the square root of the number of variables). We see that OOB errors are consistent greater than test errors but have a similar trend. Figure 1: OOB and Test Errors for Random Forests","title":"Ex. 15.6"},{"location":"ESL-Solution/_15-Random-Forests/ex15-7/","text":"Ex. 15.7 Suppose we fit a linear regression model to \\(N\\) observations with response \\(y_i\\) and predictors \\(x_{i1}, ..., x_{ip}\\) . Assume that all variables are standardized to have mean zero and standard deviation one. Let \\(RSS\\) be the mean-squared residual on the training data, and \\(\\hat\\beta\\) the estimated coefficient. Denote by \\(RSS_j^\\ast\\) the mean-squared residual on the training data using the same \\(\\hat\\beta\\) , but with the \\(N\\) values for the \\(j\\) th variable randomly permuted before the predictions are calculated. Show that \\[\\begin{equation} E_P[RSS_j^\\ast-RSS] = 2\\hat\\beta_j^2,\\non \\end{equation}\\] where \\(E_P\\) denotes expectation with respect to the permutation distribution. Argue that this is approximately true when the evaluations are done using an independent test set. Soln. 15.7 Denote by \\(\\bb{X}_j\\) the values of \\(X\\) but with \\(j\\) -th variable randomly permuted. Note that \\(X_j\\) is random. We have \\[\\begin{eqnarray} \\text{RSS} &=& (\\bb{Y}-\\bb{X}\\hat\\beta)^T(\\bb{Y}-\\bb{X}\\hat\\beta)\\non\\\\ \\text{RSS}_j^\\ast &=& (\\bb{Y}-\\bb{X}_j\\hat\\beta)^T(\\bb{Y}-\\bb{X}_j\\hat\\beta).\\non \\end{eqnarray}\\] Therefore, \\[\\begin{eqnarray} \\text{RSS}_j^\\ast - \\text{RSS} = 2\\bb{Y}^T(\\bb{X}-\\bb{X}_j)\\hat\\beta + \\hat\\beta^T(\\bb{X}_j^T\\bb{X}_j-\\bb{X}^T\\bb{X})\\hat\\beta.\\non \\end{eqnarray}\\] Note that \\(\\bb{X}_j\\) has the same elements as \\(\\bb{X}\\) except in their \\(j\\) -th column, thus we can rewrite \\[\\begin{equation} 2\\bb{Y}^T(\\bb{X}-\\bb{X}_j)\\hat\\beta=2\\hat\\beta_j\\bb{Y}^T(x_j-x^\\ast_j),\\non \\end{equation}\\] where \\(x_j^\\ast\\) and \\(x_j\\) represent the \\(j\\) -th column in \\(\\bb{X}_j\\) and \\(\\bb{X}\\) , respectively. That is, \\(x_j^\\ast\\) is a permutation of \\(x_j\\) . We need to assume that \\(\\bb{X}^T\\bb{X}=\\bb{I}.\\) It's easy to see that \\(E_p[x_j^\\ast] = \\bar x_j=\\bb{0}\\) , which is a zero-vector. Also, by definition of \\(\\hat\\beta=(\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\bb{Y}=\\bb{X}^T\\bb{Y}\\) , we have \\[\\begin{eqnarray} E_P[2\\hat\\beta_j\\bb{Y}^T(x_j-x_j^\\ast)] &=& 2\\hat\\beta_j \\bb{Y}^Tx_j\\non\\\\ &=&2\\hat\\beta_j \\cdot \\hat\\beta_j\\non\\\\ &=&2\\hat\\beta_j^2.\\non \\end{eqnarray}\\] On the other hand, it is easy to verify that \\[\\begin{equation} E_p[\\bb{X}_j^T\\bb{X}_j-\\bb{X}^T\\bb{X}]=0\\non \\end{equation}\\] under the assumption \\(\\bb{X}^T\\bb{X}=\\bb{I}\\) . The proof is complete.","title":"Ex. 15.7"},{"location":"ESL-Solution/_16-Ensemble-Learning/ex16-1/","text":"Ex. 16.1 Describe exactly how to generate the block correlated data used in the simulation in Section 16.2.3. Soln. 16.1 First we need to simulate \\(\\bb{X}\\) from a multi-dimensional Gaussian distribution with mean zero and covariance matrix \\(C\\in \\mathbb{R}^{1000\\times 1000}\\) . Specifically, \\(C\\) is a blockwise matrix \\[\\begin{equation} C = \\begin{pmatrix} C_1 & \\bb{0} & \\cdots & \\bb{0}\\\\ \\bb{0} & C_2 & \\cdots & \\bb{0}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\bb{0} & \\bb{0} & \\cdots & C_{20} \\end{pmatrix},\\non \\end{equation}\\] where each \\(C_i\\) being the same \\[\\begin{equation} C_i = \\begin{pmatrix} 1 & \\rho & \\cdots & \\rho\\\\ \\rho & 1 & \\cdots & \\rho\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\rho & \\rho & \\cdots & 1 \\end{pmatrix}.\\non \\end{equation}\\] Note that \\(\\rho=0.95\\) in this example. Recall that in Ex. 15.1 we proved that \\(-\\frac{1}{50-1}\\le \\rho\\le 1\\) . Given simulated \\(\\bb{X}\\in \\mathbb{R}^{1000\\times 1}\\) , write \\(\\bb{X}^T=(x_1, x_2,...,x_{1000})\\) . We randomly pick \\(x_1^\\ast\\) from \\(x_1,...,x_{20}\\) , and then randomly pick \\(x_2^\\ast\\) from \\(x_{21}, ..., x_{40}\\) , and so on and so forth. We end up with \\((x_1^\\ast, x_2^\\ast,...,x_{50}^\\ast)\\) being our variables. Next we simulate \\(\\beta\\in\\mathbb{R}^{50\\times 1}\\) from a standard multi-dimensional Gaussian distribution, and \\(\\epsilon\\in\\mathbb{R}^{50\\times 1}\\) from a multi-dimensional Gaussian distribution with mean zero and covariance matrix \\(\\sigma^2\\bb{I}_{50}\\) . The data model can be written as \\[\\begin{equation} Y = f(X^\\ast) + \\epsilon,\\non \\end{equation}\\] with \\(f(X)=\\beta^TX^\\ast\\) . The value of \\(\\sigma^2\\) is determined by the noise-to-signal ratio \\(\\frac{\\sigma^2}{\\text{Var}(f(X^\\ast))}=0.72\\) (see, e.g., (11.18) in the text), so that \\(\\sigma^2=50 * 0.72= 36\\) .","title":"Ex. 16.1"},{"location":"ESL-Solution/_16-Ensemble-Learning/ex16-2/","text":"Ex. 16.2 Let \\(\\alpha(t)\\in\\mathbb{R}^p\\) be a piecewise-differentiable and continuous coefficient profile, with \\(\\alpha(0)=0\\) . The \\(L_1\\) arc-length of \\(\\alpha\\) from time 0 to \\(t\\) is defined by \\[\\begin{equation} \\Lambda(t) = \\int_0^t |\\dot{\\alpha}(s)|_1ds.\\non \\end{equation}\\] Show that \\(\\Lambda(t)\\ge |\\alpha(t)|_1\\) , with equality iff \\(\\alpha(t)\\) is monotone. Soln. 16.2 The inequality follows directly by noting \\[\\begin{equation} \\alpha(t) = \\int_0^t\\dot{\\alpha}(s)ds.\\non \\end{equation}\\] When \\(\\alpha(t)\\) is monotone, we know that \\(\\dot{\\alpha(s)}\\) is either non-positive or non-negative, so that the equality holds. On the other hand, suppose that equality holds but \\(\\alpha(t)\\) is not monotone. Without loss of generality, we may assume that there exist \\(0 < t_1 < t\\) such that \\(\\alpha(t_1) > \\alpha(0)\\) and \\(\\alpha(t_1) > \\alpha(t)\\) . It is then not difficult (via classical real analysis alike arguments) to show that \\(\\Lambda(t) > |\\alpha(t)|_1\\) , which is a contradiction.","title":"Ex. 16.2"},{"location":"ESL-Solution/_16-Ensemble-Learning/ex16-3/","text":"Ex. 16.3 Show that fitting a linear regression model using rules 1, 4, 5 and 6 in equation (16.14) gives the same fit as the regression tree corresponding to this tree. Show the same is true for classification, if a logistic regression model is fit. Soln. 16.3 The claims follow by noting that rule 1, 4, 5 and 6 consist of the leaves of the tree. Recall discussions on regression tree (see, e.g., (9.10) - (9.14) in the text), fitting a regression tree reduces to fitting a linear regression once the leaves are finalized. As for classification, recall that logistic regression has the linear fit in its numerator and the same denominator for all classes, it's essentially the same as fitting a linear regression in this case.","title":"Ex. 16.3"},{"location":"ESL-Solution/_16-Ensemble-Learning/ex16-4/","text":"Ex. 16.4 Program and run the simulation study described in Figure 16.2.","title":"Ex. 16.4 (TODO)"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-01/","text":"Ex. 17.1 For the Markov graph of Figure 17.8, list all of the implied conditional independence relations and find the maximal cliques. Soln. 17.1 Recall that a clique is a complete subgraph (every pair of vertices joined by an edge). A clique is called maximal if no other vertices can be added into it and still yields a clique. In the Figure 17.8 the maximal cliques are \\(\\{X_1, X_2, X_6\\}, \\{X_3, X_4\\}, \\{X_5\\}\\) . We check each pair of vertices and list the implied conditional independence below. \\(X_1 \\bot X_3|X_4\\) \\(X_1 \\bot X_5|X_2, X_6\\) \\(X_2 \\bot X_3|X_1, X_4, X_6\\) \\(X_2 \\bot X_4|X_1, X_6\\) \\(X_2 \\bot X_5|X_1, X_6\\) \\(X_3 \\bot X_5|\\text{rest}\\) \\(X_3 \\bot X_6|X_1, X_2, X_4\\) \\(X_4 \\bot X_5|X_1, X_2, X_6\\) \\(X_4 \\bot X_6|X_1, X_2\\)","title":"Ex. 17.1"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-02/","text":"Ex. 17.2 Consider random variables \\(X_1, X_2, X_3, X_4\\) . In each of the following cases draw a graph that has the given independence relations: (a) \\(X_1\\bot X_3|X_2\\) and \\(X_2\\bot X_4|X_3\\) . (b) \\(X_1\\bot X_4|X_2, X_3\\) and \\(X_2\\bot X_4|X_1, X_3\\) . (c) \\(X_1\\bot X_4|X_2, X_3, X_1\\bot X_3|X_2, X_4\\) and \\(X_3\\bot X_4|X_1, X_2\\) . Soln. 17.2 (a) (b) (c)","title":"Ex. 17.2"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-03/","text":"Ex. 17.3 Let \\(\\bm{\\Sigma}\\) be the covariance matrix of a set of \\(p\\) variables \\(X\\) . Consider the partial covariance matrix \\(\\bm{\\Sigma}_{a.b} = \\bm{\\Sigma}_{aa} - \\bm{\\Sigma}_{aa} - \\bm{\\Sigma}_{ab}\\bm{\\Sigma}_{bb}^{-1}\\bm{\\Sigma}_{ba}\\) between the two subsets of variables \\(X_a = (X_1, X_2)\\) consisting of the first two, and \\(X_b\\) the rest. This is the covariance matrix between these two variables, after linear adjustment for all the rest. In the Gaussian distribution, this is the covariance matrix of the conditional distribution of \\(X_a|X_b\\) . The partial correlation coefficient \\(\\rho_{jk}|\\text{rest}\\) between the pair \\(X_a\\) conditional on the rest \\(X_b\\) , is simply computed from this partial covariance. Define \\(\\bm{\\Theta}=\\bm{\\Sigma}^{-1}\\) . Show that \\(\\bm{\\Sigma}_{a.b}=\\bm{\\Theta}_{aa}^{-1}\\) . Show that if any off-diagonal element of \\(\\bm{\\Theta}\\) is zero, then the partial correlation coefficient between the corresponding variables is zero. Show that if we treat \\(\\bm{\\Theta}\\) as if it were a covariance matrix, and compute the corresponding correlation matrix \\[\\begin{equation} \\bb{R} = \\text{diag}(\\bm{\\Theta})^{-1/2}\\cdot\\bm{\\Theta}\\cdot \\text{diag}(\\bm{\\Theta})^{-1/2},\\non \\end{equation}\\] then \\(r_{jk} = -\\rho_{jk}|\\text{rest}\\) . Soln. 17.3 (1) We write \\[\\begin{equation} \\bm{\\Sigma} = \\begin{pmatrix} \\bm{\\Sigma}_{aa} & \\bm{\\Sigma}_{ab}\\\\ \\bm{\\Sigma}_{ba} & \\bm{\\Sigma}_{bb} \\end{pmatrix}.\\non \\end{equation}\\] Then the desired result \\(\\bm{\\Sigma}_{a.b} = \\bm{\\Theta}_{aa}^{-1}\\) follows directly from the block matrix inverse (e.g., see 8.4.3 in \\cite{matbook}). (2) Assume that \\(\\theta_{jk} = 0\\) for some \\(j < k\\) . Note \\(X_a = (X_j, X_k)\\) in such case. Then by results above we have \\[\\begin{equation} \\bm{\\Sigma}_{a.b} = \\bm{\\Theta}_{aa}^{-1} = \\begin{pmatrix} \\theta_{jj}^{-1} & 0 \\\\ 0 & \\theta_{kk}^{-1} \\end{pmatrix}.\\non \\end{equation}\\] Therefore the partial correlation coefficient is \\[\\begin{equation} \\rho_{jk|\\text{rest}} = \\frac{\\left(\\bm{\\Sigma}_{a.b}\\right)_{12}}{\\sqrt{\\theta_{jj}^{-1}\\theta_{kk}^{-1}}}=0.\\non \\end{equation}\\] (3) Following the notations above, we have \\[\\begin{equation} \\bm{\\Sigma}_{a.b} = \\bm{\\Theta}_{aa}^{-1} = \\begin{pmatrix} \\theta_{jj} & \\theta_{jk} \\\\ \\theta_{kj} & \\theta_{kk} \\end{pmatrix}^{-1} = \\frac{1}{\\theta_{jj}\\theta_{kk}-\\theta_{jk}\\theta_{kj}} \\begin{pmatrix} \\theta_{kk} & -\\theta_{jk}\\\\ -\\theta_{kj} & \\theta_{jj} \\end{pmatrix}.\\non \\end{equation}\\] Then we have \\[\\begin{eqnarray} \\rho_{jk|\\text{rest}} &=& -\\frac{\\theta_{jk}}{\\theta_{jj}\\theta_{kk}-\\theta_{jk}\\theta_{kj}}\\Bigg/\\sqrt{\\frac{\\theta_{kk}}{\\theta_{jj}\\theta_{kk}-\\theta_{jk}\\theta_{kj}}\\cdot\\frac{\\theta_{jj}}{\\theta_{jj}\\theta_{kk}-\\theta_{jk}\\theta_{kj}}}\\non\\\\ &=&-\\frac{\\theta_{jk}}{\\sqrt{\\theta_{jj}\\theta_{kk}}}\\non\\\\ &=&-r_{jk},\\non \\end{eqnarray}\\] where the last equation follows from the definition of \\(r_{jk}\\) .","title":"Ex. 17.3"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-04/","text":"Ex. 17.4 Denote by \\(f(X_1|X_2, X_3,...,X_p)\\) the conditional density of \\(X_1\\) given \\(X_2, X_3,...,X_p\\) . If \\[\\begin{equation} f(X_1|X_2,X_3,...,X_p) = f(X_1|X_3,...,X_p),\\non \\end{equation}\\] show that \\(X_1\\bot X_2|X_3,...,X_p\\) . Soln. 17.4 It's easy to see that \\[\\begin{eqnarray} f(X_1, X_2|X_3,...,X_p) &=& f(X_1|X_2,X_3,...,X_p) f(X_2|X_3,...,X_p)\\non\\\\ &=&f(X_1|X_3,...,X_p)f(X_2|X_3,...,X_p).\\non \\end{eqnarray}\\] Therefore we have \\(X_1\\bot X_2|X_3,...,X_p\\) .","title":"Ex. 17.4"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-05/","text":"Ex. 17.5 Consider the setup in Section 17.4.1 with no missing edges. Show that \\[\\begin{equation} \\bb{S}_{11}\\beta - s_{12} = 0 \\non \\end{equation}\\] are the estimating equations for the multiple regression coefficients of the last variable on the rest. Soln. 17.5 When no edge is missing, the log-likelihood reduces to the form of (17.11) in the text, \\[\\begin{equation} l(\\bm{\\Theta}) = \\log(\\text{det}(\\bm{\\Theta})) - \\text{trace}(\\bb{S}\\bm{\\Theta}),\\non \\end{equation}\\] and \\(\\bm{\\Gamma}=0\\) in (17.13). Therefore, following the same arguments which leads to (17.19) in the text, we get \\[\\begin{equation} \\bb{S}_{11}\\beta - s_{12} = 0\\non \\end{equation}\\] are the estimating equations.","title":"Ex. 17.5"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-06/","text":"Ex. 17.6 Recovery of \\(\\hat{\\bm{\\Theta}}=\\hat{\\bm{\\Sigma}}^{-1}\\) from Algorithm 17.1 . Use expression (17.16) to derive the standard partitioned inverse expressions \\[\\begin{eqnarray} \\theta_{12} &=& -\\bb{W}^{-1}_{11}w_{12}\\theta_{22}\\non\\\\ \\theta_{22} &=&1/(w_{22}-w_{12}^T\\bb{W}_{11}^{-1}w_{12}).\\non \\end{eqnarray}\\] Since \\(\\hat\\beta = \\bb{W}_{11}^{-1}w_{12}\\) , show that \\(\\hat\\theta_{22} = 1/(w_{22}-w_{12}^T\\hat\\beta)\\) and \\(\\hat\\theta_{12} = -\\hat\\beta\\hat\\theta_{22}\\) . Thus \\(\\hat\\theta_{12}\\) is a simply rescaling of \\(\\hat\\beta\\) by \\(-\\hat\\theta_{22}\\) . Soln. 17.6 From (17.16) in the text we have \\[\\begin{equation} \\label{eq:17-6a} \\theta_{12} = -\\bb{W}^{-1}_{11}w_{12}\\theta_{22}. \\end{equation}\\] From (17.15) in the text, we have \\[\\begin{equation} w_{12}^T\\theta_{12} + w_{22}\\theta_{22} = 1.\\non \\end{equation}\\] Plug \\(\\eqref{eq:17-6a}\\) into it, we get \\[\\begin{equation} -w_{12}^T \\bb{W}^{-1}_{11}w_{12}\\theta_{22} + w_{22}\\theta_{22} = 1,\\non \\end{equation}\\] which gives \\[\\begin{equation} \\theta_{22} = 1/(w_{22}-w_{12}^T\\bb{W}_{11}^{-1}w_{12}).\\non \\end{equation}\\] The rest follows directly from \\(\\hat\\beta = \\bb{W}_11^{-1}w_{12}\\) .","title":"Ex. 17.6"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-07/","text":"Ex. 17.7 Write a program to implement the modified regression procedure (17.1) for fitting the Gaussian graphical model with pre-specified edges missing. Test it on the flow cytometry data from the book website, using the graph of Figure 17.1. Code: 17.7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import pandas as pd import numpy as np import pathlib from GraphicalGaussian import GraphicalGaussian # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # covariance data S = pd . read_csv ( DATA_PATH . joinpath ( \"cytometry.csv\" ), header = 0 ) S = np . asarray ( S ) \"\"\" Gamma: represents network in Figure 17.1 X1 - X11 are in the following order Raf, Mek, Plcg, PIP2, PIP3, Erk, Akt, PKA, PKC, P38, Jnk -0.55 & 0.36 & 0 & 0 & 0 & 0 & -0.0048 & 0.00046 & 0 & -6.5 & 0 if two nodes i, j are connected, Gamma[i][j] = 0, else Gamma[i][j] = 1 \"\"\" Gamma = np . array ([ [ 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 1 ], [ 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 1 , 0 , 0 ], [ 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ], [ 1 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 0 , 1 ], [ 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 0 , 1 ], [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 ], [ 1 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ], [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 ] ], dtype = float ) # test if a matrix is symmetric def is_symmetric ( a , tol = 1e-3 ): return ( np . abs ( a - a . T ) <= tol ) . all () assert is_symmetric ( S ) assert is_symmetric ( Gamma ) model = GraphicalGaussian ( verbose = True ) cov = model . fit ( S , Gamma ) . covariance_ theta = model . theta_ Code: GraphicalGaussian 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 import numpy as np from sklearn.base import BaseEstimator def _partition ( X , idx ): \"\"\" Partition the matrix X into part 1: all but the idx-th row and column, and part 2: the idx-th row and column Parameters ---------- X : array-like of shape (n_features, n_features) idx: the index for partion Returns ------- X11: the upper left sub-matrix X12: the upper right vector X21: the lower left vector X22: X[j][j] \"\"\" n_features = X . shape [ 0 ] indices = np . arange ( n_features ) X11 = X [ indices != idx , :] X11 = X11 [:, indices != idx ] X22 = X [ idx ][ idx ] X21 = X [ idx , indices != idx ] X21 = X21 . reshape (( 1 , n_features - 1 )) X12 = X [ indices != idx , idx ] X12 = X12 . reshape (( n_features - 1 , 1 )) return [ X11 , X12 , X21 , X22 ] def _solve ( W , S , Gamma , idx ): W11 = _partition ( W , idx )[ 0 ] Gamma12 = _partition ( Gamma , idx )[ 1 ] S12 = _partition ( S , idx )[ 1 ] zero_indices = np . where ( Gamma12 == 0 )[ 0 ] S12_new = S12 [ zero_indices ] W11_new = W11 [ zero_indices , :] W11_new = W11_new [:, zero_indices ] beta_ast = np . linalg . inv ( W11_new ) @ S12_new beta = np . zeros ( Gamma12 . shape ) beta [ zero_indices ] = beta_ast return beta def _update ( W , idx , beta ): n_features = W . shape [ 0 ] indices = np . arange ( n_features ) W11 = _partition ( W , idx )[ 0 ] updated_W12 = W11 @ beta W [ indices != idx , idx ] = updated_W12 . ravel () W [ idx , indices != idx ] = updated_W12 . ravel () return def _update_theta ( Theta , Gamma , W , S , idx ): beta = _solve ( W , S , Gamma , idx ) S22 = _partition ( S , idx )[ 3 ] W12 = _partition ( W , idx )[ 1 ] try : theta22 = 1 / ( S22 - W12 . T @ beta ) theta12 = ( - theta22 ) * beta except FloatingPointError as e : e . args = ( e . args [ 0 ] + '. Error happened, check for details.' ) raise e Theta [ idx , idx ] = theta22 n_feature = W . shape [ 0 ] indices = np . arange ( n_feature ) Theta [ idx , indices != idx ] = theta12 . ravel () return class GraphicalGaussian ( BaseEstimator ): def __init__ ( self , tol = 1e-4 , max_iter = 100 , verbose = False ): self . tol = tol self . max_iter = max_iter self . verbose = verbose self . stop_reason = None self . n_iter = None self . theta_ = None self . covariance_ = None def fit ( self , S , Gamma ): # Covariance does not make sense for a single feature S = self . _validate_data ( S , ensure_min_features = 2 , ensure_min_samples = 2 , estimator = self ) # Adjacent matrix does not make sense for a single feature Gamma = self . _validate_data ( Gamma , ensure_min_features = 2 , ensure_min_samples = 2 , estimator = self ) n_feature = S . shape [ 0 ] W = S . copy () for n_iter in range ( self . max_iter ): if self . verbose : print ( 'executing {} th iteration' . format ( n_iter + 1 )) W_last = W . copy () for idx in range ( n_feature ): if self . verbose : print ( 'executing for {} th variable' . format ( idx + 1 )) beta = _solve ( W , S , Gamma , idx ) _update ( W , idx , beta ) if np . linalg . norm ( W - W_last ) < self . tol : self . stop_reason = 'Covariance estimation converged' break if n_iter + 1 == self . max_iter : self . stop_reason = 'Maximum iteration reached' # final cycle Theta = np . zeros ( S . shape ) for idx in range ( n_feature ): _update_theta ( Theta , Gamma , W , S , idx ) self . theta_ = Theta self . covariance_ = W self . n_iter = n_iter return self # S = np.array([ # [10, 1, 5, 4], # [1, 10, 2, 6], # [5, 2, 10, 3], # [4, 6, 3, 10] # ], dtype=float) # # Gamma = np.array([ # [0, 0, 1, 0], # [0, 0, 0, 1], # [1, 0, 0, 0], # [0, 1, 0, 0] # ], dtype=float) # # model = GraphicalGaussian(verbose=True) # model.fit(S, Gamma) # # print(1) def _missing_indices ( X , i ): return np . argwhere ( np . isnan ( X [ i ])) . ravel () def _observed_indices ( X , i ): return np . argwhere ( ~ np . isnan ( X [ i ])) . ravel () class GraphicalGaussianEM ( BaseEstimator ): def __init__ ( self , graph_Gaussian_obj = GraphicalGaussian (), init_mean = None , init_cov = None , tol = 1e-4 , max_iter = 100 , verbose = False ): self . init_mean = init_mean self . init_cov = init_cov self . tol = tol self . max_iter = max_iter self . verbose = verbose self . graph_Gaussian_Obj = graph_Gaussian_obj self . covariance_ = None self . mean_ = None self . imputed_X = None def _initCov ( self , X ): filled_X = X . copy () inds = np . where ( np . isnan ( filled_X )) filled_X [ inds ] = np . take ( self . mean_ , inds [ 1 ]) self . covariance_ = np . cov ( filled_X , rowvar = False ) def _init ( self , X , init_mean = None , init_cov = None ): if init_mean is None : self . mean_ = np . nanmean ( X , axis = 0 ) if init_cov is None : self . _initCov ( X ) def _e_step ( self , X ): n_samples = X . shape [ 0 ] for i in range ( n_samples ): if self . verbose : print ( 'executing {} -th sample' . format ( i + 1 )) mi , oi = _missing_indices ( X , i ), _observed_indices ( X , i ) if len ( mi ) == 0 : continue sigma_mi_oi , sigma_oi_oi = self . covariance_ [ np . ix_ ( mi , oi )], self . covariance_ [ np . ix_ ( oi , oi )] sigma_oi_oi_inv = np . linalg . inv ( sigma_oi_oi ) imputed = self . mean_ [ mi ] + sigma_mi_oi @ sigma_oi_oi_inv @ ( X [ i , oi ] - self . mean_ [ oi ]) self . imputed_X [ i , mi ] = imputed . ravel () def _m_step ( self , Gamma ): \"\"\" Use Modified Regression to Estimated Sigma Parameters ---------- X Gamma Returns ------- \"\"\" self . mean_ = np . nanmean ( self . imputed_X , axis = 0 ) self . covariance_ = self . graph_Gaussian_Obj . fit ( self . covariance_ , Gamma ) . covariance_ def _gap ( self , mean_old , cov_old ): return np . linalg . norm ( self . mean_ - mean_old ) + np . linalg . norm ( self . covariance_ - cov_old ) def fit ( self , X , Gamma ): self . _init ( X , init_mean = self . init_mean , init_cov = self . init_cov ) self . imputed_X = X . copy () for n_iter in range ( self . max_iter ): if self . verbose : print ( 'executing {} -th iteration' . format ( n_iter + 1 )) mean_old = self . mean_ . copy () cov_old = self . covariance_ . copy () self . _e_step ( X ) self . _m_step ( Gamma ) if self . _gap ( mean_old , cov_old ) < self . tol : if self . verbose : print ( 'stop because convergence criteria met' ) break return self # S = np.array([ # [10, 1, 5, 4], # [1, 10, 2, 6], # [5, 2, 10, 3], # [4, 6, 3, 10] # ], dtype=float) # # X = np.array([ # [1, np.nan, 3, 4], # [1, 10, 2, 6], # [5, 1, np.nan, 3], # [4, 6, 3, 10] # ], dtype=float) # # Gamma = np.array([ # [0, 0, 1, 0], # [0, 0, 0, 1], # [1, 0, 0, 0], # [0, 1, 0, 0] # ], dtype=float) # # model = GraphicalGaussianEM(verbose=True) # model.fit(X, Gamma) # # print(1)","title":"Ex. 17.7"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-08/","text":"Ex. 17.8 (a) Write a program to fit the lasso using the coordinate descent procedure (17.26). Compare its results to those from the lars program or some other convex optimizer, to check that it is working correctly. (b) Using the program from (a), write code to implement the graphical lasso (Algorithm 17.2). Apply it to the flow cytometry data from the book website. Vary the regularization parameter and examine the resulting networks. Soln. 17.8 We've already implemented Algorithm 17.1 in Ex. 17.7 . The graphical lasso implementation only needs two minor changes on top of that. The first is the initialization of \\(\\bb{W} = \\bb{S} + \\lambda \\bb{I}\\) . The second is in step 2 (b), instead of solving \\(\\beta\\) using ordinary least squares, we use Lasso instead. For this exercise, we will reuse the GraphicalLasso module in sklearn . Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import pandas as pd import numpy as np import pathlib from sklearn.covariance import GraphicalLassoCV # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # get data X = pd . read_csv ( DATA_PATH . joinpath ( \"cytometry_data.csv\" ), header = 0 ) nodes = list ( X . columns ) X = X . to_numpy ( dtype = float ) res = GraphicalLassoCV ( cv = 10 , alphas = 36 ) . fit ( X ) Sigma = res . covariance_ Theta = np . linalg . inv ( Sigma )","title":"Ex. 17.8"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-09/","text":"Ex. 17.9 Suppose that we have a Gaussian graphical model in which some or all of the data at some vertices are missing. (a) Consider the EM algorithm for a dataset of N i.i.d. multivariate observations \\(x_i\\in\\mathbb{R}^p\\) with mean \\(\\mu\\) and covariance matrix \\(\\bm{\\Sigma}\\) . For each sample \\(i\\) , let \\(o_i\\) and \\(m_i\\) index the predictors that are observed and missing, respectively. Show that in the E step, the observations are imputed from the current estimates of \\(\\mu\\) and \\(\\bm{\\Sigma}\\) : \\[\\begin{equation} \\hat x_{i,m_i} = E(x_{i,m_i}|x_{i, o_i}, \\theta) = \\hat{\\mu}_{m_i} + \\hat\\Sigma_{m_i,o_i}\\hat\\Sigma_{o_i,o_i}^{-1}(x_{i,o_i}-\\hat\\mu_{o_i})\\non \\end{equation}\\] while in the M step, \\(\\mu\\) and \\(\\bm{\\Sigma}\\) are re-estimated from the empirical mean and (modified) covariance of the imputed data: \\[\\begin{eqnarray} \\hat\\mu_{j} &=& \\sum_{i=1}^N\\hat x_{ij}/N\\non\\\\ \\hat\\Sigma_{jj'} &=& \\sum_{i=1}^N[(\\hat x_{ij}-\\hat\\mu_{ij})(\\hat x_{ij'}-\\hat \\mu_{j'}) + c_{i,jj'}]/N\\non \\end{eqnarray}\\] where \\(c_{i,jj'}=\\hat\\Sigma_{jj'}\\) if \\(j, j'\\in m_i\\) and zero otherwise. Explain the reason for the correction term \\(c_{i, jj'}\\) (\\cite{little2019statistical}). (b) Implement the EM algorithm for the Gaussian graphical model using the modified regression procedure from Exercise 17.7 for the M-step. (c) For the flow cytometry data on the book website, set the data for the last protein Jnk in the first 1000 observations to missing, fit the model of Figure 17.1, and compare the predicted values to the actual values for Jnk . Compare the results to those obtained from a regression of Jnk on the other vertices with edges to Jnk in Figure 17.1, using only the non-missing data. Soln. 17.9 (a) In the E-Step, the imputed estimate for missing variables follows directly from equation (17.16) in the text. In the M-Step, it's easy to see that \\(\\hat\\mu\\) is the average of imputed observations. Specifically, if \\(x_{ij}\\) is not missing, \\(\\hat x_{ij} = x_{ij}\\) , otherwise, the imputed (conditional mean) \\(E[x_{ij}|x_{i, o}, \\theta]\\) is used. For \\(\\Sigma\\) , recall (17.11) in the text, the maximum likelihood estimate is simply the (modified) covariance \\(\\bb{S}\\) (see, e.g., (17.10)), where the additional correction term \\(c_{i, jk}\\) results from the imputation of missing values by their conditional expectations. (b) Note that if the graph is complete, then we could just use equations derived in (a) for the EM algorithm. When the graph is not complete, we need to use Algorithm 17.1 implemented in Ex. 17.7 to estimate \\(\\Sigma\\) . (c) The imputed 1000 values from EM algorithm has a mean of -38.65, while the true mean is -33.20, the mean square error is around 1972.32. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import pandas as pd import numpy as np import pathlib from sklearn.metrics import mean_squared_error from GraphicalGaussian import GraphicalGaussian , GraphicalGaussianEM # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # data X = pd . read_csv ( DATA_PATH . joinpath ( \"cytometry_data.csv\" ), header = 0 ) X = np . asarray ( X ) \"\"\" Gamma: represents network in Figure 17.1 X1 - X11 are in the following order Raf, Mek, Plcg, PIP2, PIP3, Erk, Akt, PKA, PKC, P38, Jnk -0.55 & 0.36 & 0 & 0 & 0 & 0 & -0.0048 & 0.00046 & 0 & -6.5 & 0 if two nodes i, j are connected, Gamma[i][j] = 0, else Gamma[i][j] = 1 \"\"\" Gamma = pd . read_csv ( DATA_PATH . joinpath ( \"cytometry_gamma.csv\" ), header = 0 ) Gamma = np . asarray ( Gamma ) # set first 1000 Jnk as NaN X_jnk = X . copy () X_jnk [ np . arange ( 1000 ), - 1 ] = np . nan model_EM = GraphicalGaussianEM () model_EM . fit ( X_jnk , Gamma ) cov_EM = model_EM . covariance_ est_X = model_EM . imputed_X est_values , true_values = est_X [ np . arange ( 1000 ), - 1 ], X [ np . arange ( 1000 ), - 1 ] print ( 'Estimated mean is {} vs actual mean is {} ' . format ( np . mean ( est_values ), np . mean ( true_values ))) print ( 'MSE is {} ' . format ( mean_squared_error ( est_values , true_values ))) # drop missing value X_drop = X . copy () X_drop = X [ 1000 :, :] model = GraphicalGaussian ( verbose = True ) S = np . cov ( X_drop , rowvar = False ) model . fit ( S , Gamma ) print ( 'The norm of the difference between to estimated covariance matrix is {} ' . format ( np . linalg . norm ( model . covariance_ - cov_EM )))","title":"Ex. 17.9"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-10/","text":"Ex. 17.10 Using a simple binary graphical model with just two variables, show why it is essential to include a constant node \\(X_0\\equiv 1\\) in the model. Soln. 17.10 Suppose we only have two variables \\(X_1\\) and \\(X_2\\) . When \\(X_1\\) and \\(X_2\\) are not connected, the joint probability in (17.28) reduces to a constant 1. When a constant node \\(X_0\\equiv 1\\) is included and is always connected to \\(X_1\\) and \\(X_2\\) , then the join probability becomes \\[\\begin{eqnarray} p(\\bb{X},\\bm{\\Theta}) &=& \\exp\\left(\\theta_{01}X_1+\\theta_{02}X_2 - \\log\\left(\\exp(\\theta_{01}) + \\exp(\\theta_{02} + \\exp(\\theta_{01} + \\theta_{02}))\\right)\\right)\\non\\\\ &=&\\exp\\left(\\theta_{01}X_1 + \\theta_{02}X_2\\right)/[1+\\exp(\\theta_{01})+\\exp(\\theta_{02}) + \\exp(\\theta_{01} + \\theta_{02})].\\non \\end{eqnarray}\\]","title":"Ex. 17.10"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-11/","text":"Ex. 17.11 Show that the Ising model (17.28) (17.28) for the joint probabilities in a discrete graphical model implies that the conditional distributions have the logistic form (17.30). Soln. 17.11 The claim follows from Bayesian formula. Note that we include constant node \\(X_0\\equiv 1\\) which connects to each \\(X_i\\) . By (17.28) in the text, We have \\[\\begin{eqnarray} &&\\text{Pr}(X_j=1|X_{-j}=x_{-j})\\non\\\\ &=& \\frac{\\text{Pr}(X_j=1, X_{-j}=x_{-j})}{\\text{Pr}(X_j=1, X_{-j}=x_{-j}) + \\text{Pr}(X_j=0, X_{-j}=x_{-j})}\\non\\\\ &=&\\frac{\\exp\\left(\\theta_{j0} + \\sum_{(j,k)\\in E}\\theta_{jk}x_k + \\sum_{\\substack{(i,k)\\in E\\\\ i, k \\neq j}}\\theta_{ik}x_ix_k\\right)}{\\exp\\left(\\theta_{j0} + \\sum_{(j,k)\\in E}\\theta_{jk}x_k + \\sum_{\\substack{(i,k)\\in E\\\\ i, k \\neq j}}\\theta_{ik}x_ix_k\\right) + \\exp\\left(\\sum_{\\substack{(i,k)\\in E\\\\ i, k \\neq j}}\\theta_{ik}x_ix_k\\right)}\\non\\\\ &=&\\frac{1}{1+\\exp\\left(-\\theta_{j0}-\\sum_{(j,k)\\in E}\\theta_{jk}x_k\\right)}.\\non \\end{eqnarray}\\]","title":"Ex. 17.11"},{"location":"ESL-Solution/_17-Undirected-Graphical-Models/ex17-12/","text":"Ex. 17.12 Consider a Poisson regression problem with \\(p\\) binary variables \\(x_{ij}, j = 1,...,p\\) and response variable \\(y_i\\) which measures the number of observations with predictor \\(x_i\\in \\{0,1\\}^p\\) . The design is balanced, in that all \\(n = 2p\\) possible combinations are measured. We assume a log-linear model for the Poisson mean in each cell \\[\\begin{equation} \\log(\\mu(X)) = \\theta_{00} + \\sum_{(i,j)\\in E}x_{ij}x_{ik}\\theta_{jk},\\non \\end{equation}\\] using the same notation as in Section 17.4.1 (including the constant variable \\(x_{i0}=1\\ \\forall i\\) ). We assume the response is distributed as \\[\\begin{equation} \\text{Pr}(Y=y|X=x) = \\frac{e^{-\\mu(x)\\mu(x)^y}}{y!}.\\non \\end{equation}\\] Write down the conditional log-likelihood for the observed responses \\(y_i\\) , and compute the gradient. (a) Show that the gradient equation for \\(\\theta_{00}\\) computes the partition function (17.29). (b) Show that the gradient equations for the remainder of the parameters are equivalent to the gradient (17.34). Soln. 17.12 The conditional log-likelihood is \\[\\begin{eqnarray} l(\\bm{\\Theta}) &=& \\sum_{i=1}^N\\log\\frac{e^{-\\mu(x_i)}\\mu(x_i)^y}{y_i!}\\non\\\\ &=&\\sum_{i=1}^N\\left[-\\mu(x_i) + y_i\\log(\\mu(x_i))-\\log(y_i!)\\right].\\non \\end{eqnarray}\\] Note that \\[\\begin{equation} \\mu(x_i) = \\exp\\left(\\theta_{00} + \\sum_{(j,k)\\in E}x_{ij}x_{ik}\\theta_{jk}\\right).\\non \\end{equation}\\] (a) We have \\[\\begin{eqnarray} \\frac{\\partial l(\\bm{\\Theta})}{\\partial \\theta_{00}} &=& \\sum_{i=1}^N(-\\mu(x_i) + y_i),\\non \\end{eqnarray}\\] so that \\[\\begin{eqnarray} \\sum_{i=1}^Ny_i &=& \\sum_{i=1}^N\\exp\\left(\\theta_{00} + \\sum_{(i,j)\\in E}x_{ij}x_{ik}\\theta_{jk}\\right)\\non\\\\ &=&\\exp(\\theta_{00})\\sum_{i=1}^N\\exp\\left(\\sum_{(i,j)\\in E}x_{ij}x_{ik}\\theta_{jk}\\right).\\non \\end{eqnarray}\\] Solve for \\(\\theta_{00}\\) we get \\[\\begin{equation} \\theta_{00} = \\log\\left(\\sum_{i=1}^Ny_i\\right) - \\Phi(\\bm{\\Theta}).\\non \\end{equation}\\] Note that \\(N=2^p\\) , so that \\(\\sum_{x\\in \\mathcal{X}}\\) in (17.29) is the same as \\(\\sum_{i=1}^{N=2^p}\\) . (b) From (a) we get the solution for \\(\\theta_{00}\\) and thus know that \\[\\begin{eqnarray} \\mu(x_i) &=& \\exp\\left( \\log\\left(\\sum_{l=1}^Ny_l\\right) - \\Phi(\\bm{\\Theta}) + \\sum_{(j,k)\\in E}x_{ij}x_{ik}\\theta_{jk}\\right)\\non\\\\ &=&\\left(\\sum_{l=1}^Ny_l\\right)p(x_i,\\bm{\\Theta}),\\non \\end{eqnarray}\\] where \\(p(x,\\bm{\\Theta})\\) is defined in (17.28). Taking the derivative of \\(l(\\bm{\\Theta})\\) w.r.t to \\(\\theta_{jk}\\) and setting it to be zero we get \\[\\begin{eqnarray} 0&=&\\sum_{i=1}^N(y_i-\\mu(x_i))x_{ij}x_{ik}\\non\\\\ &=&\\sum_{i=1}^N\\left(y_i-\\left(\\sum_{l=1}^Ny_l\\right)p(x_i,\\bm{\\Theta})\\right)x_{ij}x_{ik}.\\non \\end{eqnarray}\\] Therefore we have equivalent form of (17.34) in the text \\[\\begin{equation} \\hat E(X_jX_k) - E_{\\bm{\\Theta}}(X_jX_K)=0\\non \\end{equation}\\] where \\[\\begin{equation} \\hat E(X_jX_k) = \\sum_{i=1}^N\\frac{y_i}{\\sum_{l=1}^Ny_l}x_{ij}x_{ik}\\non \\end{equation}\\] and as in (17.33) \\[\\begin{equation} E_{\\bm{\\Theta}}(X_jX_K) = \\sum_{i=1}^Nx_{ij}x_{ik}\\cdot p(x_i, \\bm{\\Theta}).\\non \\end{equation}\\]","title":"Ex. 17.12"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-01/","text":"Ex. 18.1 For a coefficient estimate \\(\\hat\\beta_j\\) , let \\(\\hat\\beta_j/\\|\\hat\\beta_j\\|_2\\) be the normalized version. Show that as \\(\\lambda\\ra\\infty\\) , the normalized ridge-regression estimates converge to the renormalized partial-least-squares one-component estimates. Soln. 18.1 Recall the SVD decomposition of \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T\\) . Here \\(\\bb{U}\\) and \\(\\bb{V}\\) are \\(N\\times N\\) and \\(p\\times N\\) orthogonal matrices, and \\(\\bb{D}=\\text{diag}(d_1, d_2,...,d_p)\\) is a \\(p\\times p\\) diagonal matrix. Denote \\(\\bb{V} = \\{v_{ij}\\}\\) and write \\[\\begin{equation} \\bb{U} = \\begin{pmatrix} \\bb{u}_1 & \\bb{u}_2 & \\dots & \\bb{u}_p \\end{pmatrix}. \\non \\end{equation}\\] We have \\[\\begin{eqnarray} \\hat\\beta &=& (\\bX^T\\bX + \\lambda\\bI)^{-1}\\bX^T\\by\\non\\\\ &=&\\left(\\bb{V}\\bb{D}^2\\bb{V}^T + \\lambda\\bI\\right)^{-1}\\bb{V}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&\\left(\\bb{V}(\\bb{D}^2 + \\lambda\\bI\\right)\\bb{V}^T)^{-1}\\bb{V}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&\\bb{V}^T(\\bb{D}^2 + \\lambda\\bI)^{-1}\\bb{D}\\bb{U}^T\\by.\\non \\end{eqnarray}\\] Thus we can write \\[\\begin{equation} \\hat\\beta_j = \\sum_{i=1}^p\\frac{d_iv_{ji}\\bb{u}_i^Ty}{d_i^2+\\lambda}.\\non \\end{equation}\\] On the other hand, \\[\\begin{eqnarray} \\|\\hat\\beta\\|_2^2 &=& \\by^T\\bb{U}\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-1}(\\bb{D}^2+\\lambda\\bI)^{-1}\\bb{D}\\bb{U}^T\\by\\non\\\\ &=&(\\bb{U}^T\\by)^T[\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-2}\\bb{D}](\\bb{U}^T\\by)\\non\\\\ &=&\\sum_{j=1}^p\\frac{d_j^2(\\bb{U}^T\\by)_j^2}{(d_j^2 + \\lambda)^2}.\\non \\end{eqnarray}\\] where \\(\\bb{D}(\\bb{D}^2+\\lambda\\bI)^{-2}\\bb{D}\\) represents a diagonal matrix with elements \\(\\frac{d_j^2}{(d_j^2 + \\lambda)^2}\\) . Thus, as \\(\\lambda\\ra\\infty\\) , we have \\[\\begin{eqnarray} \\frac{\\hat\\beta_j}{\\|\\hat\\beta_j\\|_2} &=& \\frac{\\sum_{i=1}^p\\frac{d_iv_{ji}\\bb{u}_i^Ty}{d_i^2+\\lambda}}{\\frac{d_j(\\bb{u}_j^T\\by)}{(d_j^2 + \\lambda)}}\\non\\\\ &=&\\frac{1}{d_j\\bb{u}_j\\by}\\cdot \\sum_{i=1}^p(d_iv_{ji}\\bb{u}_i^T\\by)\\frac{d_j^2+\\lambda}{d_i^2+\\lambda}\\non\\\\ &\\ra& \\frac{1}{d_j\\bb{u}_j^T\\by}\\cdot \\sum_{i=1}^p(d_iv_{ji}\\bb{u}_i^T\\by)\\non\\\\ &=&\\frac{1}{d_j\\bb{u}^T_j\\by} (\\bb{V}\\bb{D}\\bb{U}^T\\by)_j\\non\\\\ &=&\\frac{(\\bX^T\\by)_j}{d_j\\bb{u}_j^T\\by},\\non \\end{eqnarray}\\] which is re-normalized partial-least-squares one-component estimate \\(\\bX^T\\by\\) .","title":"Ex. 18.1"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-02/","text":"Ex. 18.2 Nearest shrunken centroids and the lasso. Consider a (naive Bayes) Gaussian model for classification in which the features \\(j=1,2,...,p\\) are assumed to be independent within each class \\(k=1,2,...,K\\) . With observations \\(i=1,2,...,N\\) and \\(C_k\\) equal to the set of indices of the \\(N_k\\) observations in class \\(k\\) , we observe \\(x_{ij}\\sim N(\\mu_j + \\mu_{jk}, \\sigma^2_j)\\) for \\(i\\in C_k\\) with \\(\\sum_{k=1}^K\\mu_{jk}=0\\) . Set \\(\\hat\\sigma_j^2=s_j^2\\) , the pooled within-class variance for feature \\(j\\) , and consider the lasso-style minimization problem \\[\\begin{equation} \\min_{\\{\\mu_j, \\mu_{jk}\\}}\\left\\{\\frac{1}{2}\\sum_{j=1}^p\\sum_{k=1}^K\\sum_{i\\in C_k}\\frac{(x_{ij}-\\mu_j-\\mu_{jk})^2}{s_j^2}+\\lambda\\sqrt{N_k}\\sum_{j=1}^p\\sum_{k=1}^K|\\frac{\\mu_{jk}}{s_j}|\\right\\}.\\non \\end{equation}\\] Show that the solution is equivalent to the nearest shrunken centroid estimator (18.5), with \\(s_0\\) set to zero, and \\(M_k\\) equal to \\(1/N_k\\) instead of \\(1/N_k-1/N\\) as before. Soln. 18.2 Denote the objective function as \\(L(\\mu_j, \\mu_{jk})\\) , we take first-order derivative w.r.t to \\(\\mu_j\\) and \\(\\mu_{jk}\\) and set them to be zero. For \\(\\mu_j\\) , note that \\(\\sum_{k=1}^K\\mu_{jk}=0\\) , we have \\[\\begin{eqnarray} \\frac{\\partial L(\\mu_j, \\mu_{jk})}{\\partial \\mu_j} &=& - \\sum_{k=1}^K\\sum_{i\\in C_k}\\frac{x_{ij}-\\mu_j-\\mu_{jk}}{s_j^2}\\non\\\\ &=&\\frac{1}{s_j^2}\\sum_{k=1}^K\\sum_{i\\in C_k}\\mu_j -\\frac{1}{s_j^2}\\sum_{k=1}^K\\sum_{i\\in C_k}x_{ij}\\non\\\\ &=&\\frac{1}{s_j^2}\\left(N\\mu_j - \\sum_{k=1}^K\\sum_{i\\in C_k}x_{ij}\\right)\\non\\\\ &=&0, \\non \\end{eqnarray}\\] thus we get \\[\\begin{equation} \\mu_j = \\frac{1}{N}\\sum_{k=1}^K\\sum_{i\\in C_k}x_{ij} = \\bar x_j.\\non \\end{equation}\\] For \\(\\mu_{jk}\\) , we have \\[\\begin{eqnarray} \\frac{\\partial L(\\mu_j, \\mu_{jk})}{\\partial \\mu_{jk}} &=& -\\sum_{i\\in C_k}\\frac{(x_{ij}-\\mu_j-\\mu_{jk})}{s_j^2} + \\frac{\\lambda\\sqrt{N_k}}{s_j}\\text{sign}(\\mu_{jk})\\non\\\\ &=&s_j^2\\left[N_k\\mu_{jk}+N_k\\mu_j-\\sum_{i\\in C_k}x_{ij}+\\lambda\\sqrt{N_k}s_j\\text{sign}(\\mu_{jk})\\right]\\non\\\\ &=&0.\\non \\end{eqnarray}\\] Thus we have \\[\\begin{eqnarray} \\mu_{jk} &=& \\frac{s_j}{\\sqrt{N_k}}\\left[\\frac{\\frac{1}{N_k}\\sum_{i\\in C_k}x_{ij}-\\mu_j}{s_j/\\sqrt{N_k}}-\\lambda \\text{sign}(\\mu_{jk})\\right]\\non\\\\ &=&\\frac{s_j}{\\sqrt{N_k}}\\left[d_{jk}-\\lambda \\text{sign}(\\mu_{jk})\\right] \\label{eq:18-2a} \\end{eqnarray}\\] where \\[\\begin{equation} d_{jk} = \\frac{\\frac{1}{N_k}\\sum_{i\\in C_k}x_{ij}-\\mu_j}{s_j/\\sqrt{N_k}}.\\non \\end{equation}\\] We claim that \\[\\begin{equation} \\label{eq:18-2b} \\mu_{jk} = \\frac{1}{\\sqrt{N_k}}s_j\\text{sign}(d_{jk})(|d_{jk}|-\\lambda)_+. \\end{equation}\\] To verify that, when \\(\\mu_{jk} > 0\\) , \\(\\eqref{eq:18-2a}\\) becomes \\[\\begin{equation} \\mu_{jk} = \\frac{s_j}{\\sqrt{N_k}}\\left[d_{jk}-\\lambda\\right] > 0,\\non \\end{equation}\\] so that \\(d_{jk} > \\lambda > 0\\) and \\(\\eqref{eq:18-2b}\\) is the same as the expression above. Similar arguments goes for the case when \\(\\mu_{jk} \\le 0\\) . From \\(\\eqref{eq:18-2b}\\) it's easy to see the proof is complete.","title":"Ex. 18.2"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-03/","text":"Ex. 18.3 Show that the fitted coefficients for the regularized multiclass logistic regression problem (18.10) satisfy \\(\\sum_{k=1}^K\\hat\\beta_{kj}=0, j=1,...,p.\\) What about the \\(\\hat\\beta_{k0}\\) ? Discuss issues with these constant parameters, and how they can be resolved. Soln. 18.3 The objective function can be written as \\[\\begin{eqnarray} L(\\bm{\\beta}) &=& \\sum_{i=1}^N\\left[\\sum_{k=1}^K\\bb{1}(g_i=k)(\\beta_{k0}+x_i^T\\beta_k) - \\log\\left(\\sum_{l=1}^K\\exp(\\beta_{l0}+x_i^T\\beta_l)\\right) - \\frac{\\lambda}{2}\\sum_{k=1}^K\\|\\beta_k\\|_2^2 \\right].\\non \\end{eqnarray}\\] Taking first-order derivative w.r.t \\(\\beta_k\\) and setting it to zero, we get \\[\\begin{eqnarray} \\frac{\\partial L(\\bm{\\beta})}{\\partial \\beta_k} &=&\\sum_{i=1}^N\\left[x_i\\bb{1}(g_i=k)-\\frac{\\exp(\\beta_{k0}+x_i^T\\beta_{k})}{\\sum_{l=1}^K\\exp(\\beta_{l0}+x_i^T\\beta_l)}\\cdot x_i - \\lambda \\beta_k\\right]\\non\\\\ &=&0.\\label{eq:18-3a} \\end{eqnarray}\\] By the fact that \\(\\sum_{k=1}^K\\frac{\\partial L(\\bm{\\beta})}{\\partial \\beta_k} = 0\\) , it's easy to see that \\(\\eqref{eq:18-3a}\\) leads to \\[\\begin{equation} N\\sum_{k=1}^L\\hat\\beta_k = \\bb{0}\\non. \\end{equation}\\] For constant parameters \\(\\hat\\beta_{k0}\\) , they are not differentiable, in the sense that if we add a common constant \\(\\alpha\\) to each of \\(\\hat\\beta_{k0}\\) , then the derived probabilities are not changed. Therefore, we need to impose an additional regularization for \\(\\hat\\beta_{k0}\\) , e.g, \\[\\begin{equation} \\sum_{k=1}^K\\hat\\beta_{k0}=0.\\non \\end{equation}\\]","title":"Ex. 18.3"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-04/","text":"Ex. 18.4 Derive the computational formula (18.15) for ridge regression. [ Hint : Use the first derivative of the penalized sum-of-squares criterion to show that if \\(\\lambda > 0\\) , then \\(\\hat\\beta = \\bX^Ts\\) for some \\(s\\in\\mathbb{R}^N\\) .] Soln. 18.4 By SVD decomposition of \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T=\\bb{R}\\bb{V}^T\\) , we have \\(\\bb{V}^T\\bb{V}=\\bb{I}\\) . We need to show that \\[\\begin{equation} \\label{eq:18-4a} \\hat\\beta = \\bb{V}(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1}\\bb{R}^T\\by. \\end{equation}\\] We know that \\(\\hat\\beta\\) solves the equation \\[\\begin{equation} -\\bX^T(\\by-\\bX\\hat\\beta) + \\lambda \\hat\\beta = 0.\\non \\end{equation}\\] Therefore, it suffices to plug \\(\\eqref{eq:18-4a}\\) into the equation above and verify the equation indeed holds. To that end, let's write \\[\\begin{eqnarray} \\bX^T(\\by-\\bX\\hat\\beta) &=& \\bX^T(\\by-\\bX\\bb{V}(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1}\\bb{R}^T\\by) \\non\\\\ &=& \\bb{V}\\bb{R}^T(\\by - \\bR\\bV^T\\bb{V}(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1}\\bb{R}^T\\by)\\non\\\\ &=&\\bV(\\bR^T\\by-\\bR^T\\bR(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1}\\bb{R}^T\\by)\\non\\\\ &=&\\bV(\\bI-\\bR^T\\bR(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1})\\bR^T\\by\\non\\\\ &=&\\bV(\\bI-(\\bR^T\\bR+\\lambda \\bI - \\lambda \\bI)(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1})\\bR^T\\by\\non\\\\ &=&\\bV(\\lambda \\bI(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1})\\bR^T\\by\\non\\\\ &=&\\lambda \\bV(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1})\\bR^T\\by\\non\\\\ &=&\\lambda \\hat\\beta.\\non \\end{eqnarray}\\] Therefore the proof is complete.","title":"Ex. 18.4"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-05/","text":"Ex. 18.5 Prove the theorem (18.16)\u2013(18.17) in Section 18.3.5, by decomposing \\(\\beta\\) and the rows of \\(\\bX\\) into their projections into the column space of \\(\\bb{V}\\) and its complement in \\(\\mathbb{R}^p\\) . Soln. 18.5 Intuitively the proof follows by change of variates \\(\\beta = \\bb{V}\\theta\\) . Here we recap the proof detailed in \\cite{hastie2004efficient}. Let \\(\\bV_\\bot\\) be the \\(p\\times (p-n)\\) and span the complementary subspace in \\(\\mathbb{R}^p\\) to \\(\\bV\\) . Then \\(\\bb{Q} = (\\bV : \\bV_\\bot)\\) is a \\(p\\times p\\) orthogonal matrix. Let \\(x_i^\\ast = \\bb{Q}^Tx_i\\) and \\(\\beta^\\ast = \\bb{Q}^T\\beta\\) . Then \\[\\begin{eqnarray} &&(x_i^\\ast)^T\\beta^\\ast = x_i^T\\bb{Q}\\bb{Q}^T\\beta = x_i^T\\beta,\\non\\\\ &&(\\beta^\\ast)^T\\beta^\\ast = \\beta^T\\bb{Q}\\bb{Q}^T\\beta=\\beta^T\\beta.\\non \\end{eqnarray}\\] Hence (18.16) is invariant under orthogonal transformation. There is a one-to-one mapping between the locations of their minima, so we can focus on \\(\\beta^\\ast\\) rather than \\(\\beta\\) . By definition of \\(\\bV\\) in \\(\\bX = \\bR\\bV^T\\) , we know \\((x_i^\\ast)^T\\beta^\\ast=r_i^T\\beta_1^\\ast\\) where \\(\\beta_1^\\ast\\) consists of the first \\(n\\) elements of \\(\\beta^\\ast\\) . Hence the loss part of (18.16) involves \\(\\beta_0\\) and \\(\\beta_1^\\ast\\) . We can similarly factor the quadratic penalty into two terms \\(\\lambda(\\beta_1^\\ast)^T\\beta_1^\\ast + \\lambda(\\beta_2^\\ast)^T\\beta_2^\\ast\\) , and rewrite (18.16) as \\[\\begin{equation} \\left[\\sum_{i=1}^NL(y_i, \\beta_0 + r_i^T\\beta_1^\\ast)+\\lambda (\\beta_1^\\ast)^T\\beta_1^\\ast\\right] + [\\lambda (\\beta_2^\\ast)^T\\beta_2^\\ast],\\non \\end{equation}\\] which we can minimize separately. The second part is minimized at \\(\\beta_2^\\ast=0\\) , and the result follows by noting that the first part is identical to (18.17) with \\(\\theta_0=\\beta_0\\) and \\(\\theta = \\beta_1^\\ast\\) . From the equivalence, \\[\\begin{equation} \\hat\\beta = \\bb{Q}\\hat{\\beta^\\ast} = \\begin{pmatrix} \\bV & \\bV_\\bot \\end{pmatrix}\\begin{pmatrix} \\hat\\theta\\\\ 0 \\end{pmatrix}=\\bV\\hat\\theta.\\non \\end{equation}\\]","title":"Ex. 18.5"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-06/","text":"Ex. 18.6 Show how the theorem in Section 18.3.5 can be applied to regularized discriminant analysis [Section 4.14 and Equation (18.9)]. Soln. 18.6 Here we recap Section 4.5 in \\cite{hastie2004efficient}, where the authors discussed applying the theorem to regularized discriminant analysis in detail. Recall that LDA model (Section 4.3) assumes features come from a multivariate Gaussian distribution in each class \\(k=1,...,K\\) . Each class has mean vector \\(\\mu_k\\) , but shares a common covariance matrix \\(\\Sigma\\) . The discriminant function of class \\(k\\) is \\[\\begin{equation} \\label{eq:18-6a} \\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log \\pi_k \\end{equation}\\] where \\(\\pi_k\\) is the prior probability of class \\(k\\) . Parameters are estimated as \\[\\begin{eqnarray} \\hat\\pi_k &=& \\frac{n_k}{n}\\non\\\\ \\hat\\mu_k &=& \\frac{1}{n_k}\\sum_{y_i=k}x_i\\non\\\\ \\hat\\Sigma &=&\\frac{1}{n-k}\\sum_{k=1}^K\\sum_{y_i=k}(x_i-\\hat\\mu_k)(x_i-\\hat\\mu_k)^T.\\non \\end{eqnarray}\\] These estimates are plugged into \\(\\eqref{eq:18-6a}\\) , which requires the inversion of a \\(p\\times p\\) singular matrix \\(\\hat\\Sigma\\) when \\(p\\gg N\\) . Regularized overcomes the issue by replacing \\(\\hat\\Sigma\\) with \\[\\begin{equation} \\hat\\Sigma(\\gamma) = \\gamma\\hat\\Sigma + (1-\\gamma)\\text{diag}(\\hat\\Sigma), \\non \\end{equation}\\] for \\(\\gamma \\in [0,1]\\) . Following the same arguments in Ex. 18.5 , it's easy to show that \\(\\eqref{eq:18-6a}\\) and its regularized version are invariant under a coordinate rotation. Hence we can once again use the SVD construction and replace the training \\(x_i\\) by their corresponding \\(r_i\\) , and fit the regularized model in the lower-dimensional space. Again the \\(n\\) -dimensional linear coefficients \\[\\begin{equation} \\hat\\beta_k^\\ast = (\\gamma\\hat\\Sigma^\\ast + (1-\\gamma)\\text{diag}(\\hat\\Sigma^\\ast))^{-1}\\hat\\mu_k^\\ast\\non \\end{equation}\\] are mapped back to \\(p\\) -dimensions via \\(\\hat\\beta_k = \\bV\\hat\\beta_k^\\ast\\) .","title":"Ex. 18.6"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-07/","text":"Ex. 18.7 Consider a linear regression problem where \\(p\\gg N\\) , and assume the rank of \\(\\bX\\) is \\(N\\) . Let the SVD of \\(\\bX=\\bb{U}\\bb{D}\\bb{V}^T = \\bb{R}\\bb{V}^T\\) ,where \\(\\bb{R}\\) is \\(N\\times N\\) nonsingular, and \\(\\bb{V}\\) is \\(p\\times N\\) with orthonormal columns. (a) Show that there are infinitely many least-squares solutions all with zero residuals. (b) Show that the ridge-regression estimate for \\(\\beta\\) can be written \\[\\begin{equation} \\hat\\beta_\\lambda = \\bb{V}(\\bb{R}^T\\bb{R}+\\lambda \\bb{I})^{-1}\\bb{R}^T\\by\\non \\end{equation}\\] (c) Show that when \\(\\lambda=0\\) , the solution \\(\\hat\\beta_0 = \\bb{V}\\bb{D}^{-1}\\bb{U}^T\\by\\) has residuals all equal to zero, and is unique in that it has the smallest Euclidean norm amongst all zero-residual solutions. Soln. 18.7 (a) Since \\(\\bX\\in\\mathbb{R}^{p\\times N}\\) has rank \\(N\\le p\\) , we know there exists \\(\\alpha\\neq 0\\) such that \\(\\bX\\alpha = 0\\) . If \\(\\hat\\beta_0\\) has zero residuals, so does \\(\\hat\\beta_0 + k\\alpha\\) for any \\(k\\in\\mathbb{R}\\) . Therefore there are infinitely many least-squares solutions all with zero residuals. (b) This is the same as Ex. 18.4 . (c) Note that \\[\\begin{equation} \\bX\\hat\\beta_0 = \\bU\\bD\\bV^T\\bV\\bD^{-1}\\bU^T\\by = \\by,\\non \\end{equation}\\] so \\(\\hat\\beta_0\\) has zero residual. Suppose that \\(\\hat\\beta_0+\\beta\\) has zero residual for some \\(\\beta \\neq \\bb{0}\\) , that is, \\(\\bX(\\hat\\beta_0+\\beta) = \\by\\) . Since \\(\\hat\\beta_0\\) has zero residual, we know \\[\\begin{equation} \\bX\\beta = \\bR\\bV^T\\beta=0.\\non \\end{equation}\\] Note that \\(\\bR\\) is \\(N\\times N\\) nonsingular, so we have \\(\\bV^T\\beta = 0\\) . Now consider the Euclidean norm of \\(\\hat\\beta_0+\\beta\\) , we have \\[\\begin{eqnarray} &&(\\hat\\beta_0+\\beta)^T(\\hat\\beta_0+\\beta)\\non\\\\ &=&\\hat\\beta_0^T\\hat\\beta_0 + \\beta^T\\beta + 2 \\hat\\beta_0^T\\beta\\non\\\\ &=&\\hat\\beta_0^T\\hat\\beta_0 + \\beta^T\\beta + 2\\by^T\\bU\\bD^{-1}\\bV^T\\beta\\non\\\\ &=&\\hat\\beta_0^T\\hat\\beta_0 + \\beta^T\\beta+0.\\non \\end{eqnarray}\\] Since \\(\\beta^T\\beta > 0\\) , we know that \\(\\hat\\beta_0\\) has the smallest Euclidean norm.","title":"Ex. 18.7"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-08/","text":"Ex. 18.8 Data Piling . Exercise 4.2 shows that the two-class LDA solution can be obtained by a linear regression of a binary response vector \\(\\by\\) consisting of -1s and +1s. The prediction \\(\\hat\\beta^Tx\\) for any \\(x\\) is (up to a scale and shift) the LDA score \\(\\delta(x)\\) . Suppose now that \\(p\\gg N\\) . (a) Consider the linear regression model \\(f(x)=\\alpha + \\beta^Tx\\) fit to a binary response \\(Y\\in \\{-1, +1\\}\\) . Using Exercise 18.7 , show that there are infinitely many directions defined by \\(\\hat\\beta\\) in \\(\\mathbb{R}^p\\) onto which the data project to exactly two points, one for each class. There are known as data piling directions (Ahn and Marron, 2005) (b) Show that the distance between the projected points is \\(2/\\|\\hat\\beta\\|\\) , and hence these directions define separating hyperplanes with that margin. (c) Argue that there is a single maximal data piling direction for which this distance is largest, and is defined by \\(\\hat\\beta_0=\\bV\\bD^{-1}\\bU^T\\by=\\bX^{-}\\by\\) , where \\(\\bX=\\bU\\bD\\bV^T\\) is the SVD of \\(\\bX\\) . Soln. 18.8 (a) This follows from Ex. 18.7 (a) directly. (b) Suppose that \\(x_1\\) and \\(x_2\\) are the closest point from each class, that is, \\[\\begin{eqnarray} \\hat\\alpha + \\hat\\beta^Tx_1 &=& 1\\non\\\\ \\hat\\alpha + \\hat\\beta^Tx_2 &=& -1.\\non \\end{eqnarray}\\] Then the result follows directly from (4.40) in Section 4.5 (Separating Hyperplanes). (c) This follows from Ex. 18.7 (c), where we proved that \\(\\hat\\beta_0\\) has smallest Euclidean norm among all zero-residual solutions, thus largest distance (margin) by (b).","title":"Ex. 18.8"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-09/","text":"Ex. 18.9 Compare the data piling direction of Exercise 18.8 to the direction of the optimal separating hyperplane (Section 4.5.2) qualitatively. Which makes the widest margin, and why? Use a small simulation to demonstrate the difference. Soln. 18.9 The optimal separating hyperplane makes the widest margin, since its objective is to maximize the margin. It's thus expected that data piling direction produces a narrower margin. We simulated a matrix \\(\\bb{X}\\) with \\(N=40\\) and \\(p=1000\\) , our simulation shows that margin from optimal separating hyperplane is 258, which is indeed greater than 252 from data piling. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import numpy as np from sklearn import svm from sklearn.datasets import make_blobs # we create 40 separable points n_samples_raw = 400 X_raw , y_raw , centers = make_blobs ( n_samples = n_samples_raw , n_features = 1000 , centers = 2 , random_state = 2 , return_centers = True ) y_raw [ y_raw == 0 ] = - 1 c0 = centers [ 0 ] c1 = centers [ 1 ] dist = np . linalg . norm ( c0 - c1 ) n_samples = 20 indices , indices_1 = [], [] r0 , r1 = 0 , 0 for i in range ( n_samples_raw ): if y_raw [ i ] == - 1 : cur_dist = np . linalg . norm ( X_raw [ i ] - c0 ) if cur_dist < dist / 3 and len ( indices ) < n_samples : indices . append ( i ) r0 = max ( r0 , cur_dist ) else : cur_dist_1 = np . linalg . norm ( X_raw [ i ] - c1 ) if cur_dist_1 < dist / 3 and len ( indices_1 ) < n_samples : indices_1 . append ( i ) r1 = max ( r1 , cur_dist_1 ) X = X_raw [ indices + indices_1 ] y = y_raw [ indices + indices_1 ] clf = svm . SVC ( kernel = 'linear' , C = 1000 ) clf . fit ( X , y ) margin_svm = 2 / np . linalg . norm ( clf . coef_ ) U , D , V_T = np . linalg . svd ( X , full_matrices = False ) D = np . diag ( D ) V = V_T . T beta = V @ np . linalg . inv ( D ) @ U . T @ y margin_data_piling = 2 / np . linalg . norm ( beta ) print ( 'Margins from optimal hyperplane (SVM) and data piling are {} and {} , respectively.' . format ( margin_svm , margin_data_piling ))","title":"Ex. 18.9"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-10/","text":"Ex. 18.10 When \\(p\\gg N\\) , linear discriminant analysis (see Section 4.3) is degenerate because the within-class covariance matrix \\(\\bb{W}\\) is singular. One version of regularized discriminant analysis (4.14) replaces \\(\\bW\\) by a ridged version \\(\\bW + \\lambda \\bI\\) . leading to a regularized discriminant function \\(\\delta_\\lambda(x)=x^T(\\bW + \\lambda\\bI)^{-1}(\\bar x_1 - \\bar x_{-1})\\) . Show that \\(\\delta_0(x)=\\lim_{\\lambda\\downarrow 0}\\delta_\\lambda(x)\\) corresponds to the maximal data piling direction defined in Exercise 18.8 . Soln. 18.10 Note we need to assume that \\(\\bb{X}\\) is centered. By Ex. 18.8 (c) and Ex. 18.7 (b), the maximal data piling direction \\(\\hat\\beta= \\bb{V}(\\bb{R}^T\\bb{R})^{-1}\\bb{R}^T\\by\\) , which further equals to \\((\\bb{X}^T\\bb{X})^{-1}\\bb{X}^T\\by\\) by Ex. 18.4 . See (18.14)-(18.15) in the text for related discussions. Note that in the binary case, where \\(y=1\\) or -1, \\(\\bX^T\\by\\) has the same direction as \\((\\bar x_1-\\bar x_{-1})\\) . With \\(\\bb{W} = \\bb{X}\\bb{X}^T\\) , it suffices to show that \\[\\begin{equation} \\lim_{\\delta\\ra 0}(\\bb{W}+\\lambda \\bb{I})^{-1} = \\bb{W}^{-1} = (\\bb{X}\\bb{X}^T)^{-1}.\\non \\end{equation}\\] To see that, we start with the spectral decomposition of \\(\\bb{W}\\) \\[\\begin{equation} \\bb{W}^{-1} = \\sum_{i=1}^p\\frac{\\bb{v}_i\\bb{v}_i^T}{\\bb{e}_i}\\non \\end{equation}\\] where \\(\\bb{v}_i\\) is the \\(i\\) -th eigenvector of \\(\\bb{W}\\) and \\(\\bb{e}_i\\) is its corresponding eigenvalue. Then \\[\\begin{equation} (\\bb{W}+\\lambda \\bb{I})^{-1} = \\sum_{i=1}^p\\frac{\\bb{v}_i\\bb{v}_i^T}{\\bb{e}_i+\\lambda}.\\non \\end{equation}\\] It's now easy to see the proof is complete.","title":"Ex. 18.10"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-11/","text":"Ex. 18.11 Suppose you have a sample of \\(N\\) pairs \\((x_i, y_i)\\) , with \\(y_i\\) binary and \\(x_i\\in \\mathbb{R}^1\\) . Suppose also that the two classes are separable; e.g., for each pair \\(i, i'\\) with \\(y_i=0\\) and \\(y_{i'}=1\\) , \\(x_{i'}-x_i\\ge C\\) for some \\(C > 0\\) . You wish to fit a linear logistic regression model logitPr \\((Y=1|X)=\\alpha + \\beta X\\) by maximum-likelihood. Show that \\(\\hat\\beta\\) is undefined. Soln. 18.11 Linear logistic regression is discussed in Section 4.4. Recall (4.20) in the text, the likelihood we need to maximize is \\[\\begin{eqnarray} l(\\alpha, \\beta) &=& \\sum_{i=1}^N\\{y_i(\\alpha+\\beta x_i)-\\log(1+\\exp(\\alpha + \\beta x_i))\\}\\non\\\\ &=&\\sum_{i:y_i = 1}(\\alpha+\\beta x_i - \\log(1+\\exp(\\alpha + \\beta x_i) ) - \\sum_{i: y_i=0}\\log(1+\\exp(\\alpha + \\beta x_i).\\non \\end{eqnarray}\\] Without loss of generality, we assume that \\(x_i\\ge C\\) for when \\(y_i=1\\) and \\(x_i < 0\\) when \\(y_i=0\\) . This can be achieved by shifting the constant \\(\\alpha\\) . Consider the first summation above, take the derivative with respect to \\(\\beta\\) , we get \\[\\begin{equation} \\sum_{i:y_i=1} \\frac{x_i}{1+e^{\\alpha+\\beta x_i}} > 0.\\non \\end{equation}\\] Therefore the first summation is increasing in \\(\\beta\\) . Similarly, it's easy to verify that the second summation is decreasing in \\(\\beta\\) . Therefore the likelihood is increasing in \\(\\beta\\) , so the optimal \\(\\hat\\beta\\) is undefined.","title":"Ex. 18.11"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-12/","text":"Ex. 18.12 Suppose we wish to select the ridge parameter \\(\\lambda\\) by 10-fold cross-validation in a \\(p\\gg N\\) situation (for any linear model). We wish to use the computational shortcuts described in Section 18.3.5. Show that we need only to reduce the \\(N\\times p\\) matrix \\(\\bX\\) to the \\(N\\times N\\) matrix \\(\\bR\\) once , and can use it in all the cross-validation runs. Soln. 18.12 The \\(N\\times N\\) matrix \\(\\bb{R}\\) is constructed via SVD of \\(\\bb{X}\\) in (18.13). For each observation \\(x_i, i=1,...,N\\) , (18.13) defines a corresponding \\(r_i, i=1,...,N\\) . To perform 10-fold cross-validation, we divide the training sample \\(\\bb{X}\\) into 10 subsets \\(N_i, i=1,...,10\\) with size \\(N/10\\) . Correspondingly, we divide the matrix \\(\\bb{R}\\) into 10 subsets with the same division indices as \\(\\bb{X}\\) . We separate each subset \\(N_i\\) aside and train on the remaining subsets. Recall the theorem described in (18.16)-(18.17) in the text, each training session (indexed by \\(j=1,...,10\\) ) essential becomes solving \\[\\begin{equation} \\underset{\\beta_0, \\beta}{\\operatorname{argmin}}\\sum_{i\\notin N_j}L(y_i, \\beta_0+x_i^T\\beta) + \\lambda \\beta^T\\beta,\\non \\end{equation}\\] which has the same optimal solution if we solve for \\(r_i\\) for \\(i\\notin N_j\\) like (18.17). Therefore, we only need to construct \\(\\bb{R}\\) once.","title":"Ex. 18.12"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-13/","text":"Ex. 18.13 Suppose our \\(p>N\\) predictors are presented as an \\(N\\times N\\) inner-product matrix \\(\\bb{K}=\\bb{X}\\bb{X}^T\\) , and we wish to fit the equivalent of a linear logistic regression model in the original features with quadratic regularization. Our predictions are also to be made using inner products; a new \\(x_0\\) is presented as \\(k_0=\\bX x_0\\) . Let \\(\\bb{K}=\\bU\\bD^2\\bU^T\\) be the eigen-decomposition of \\(\\bb{K}\\) . Show that the predictions are given by \\(\\hat f_0 = k_0^T\\hat\\alpha\\) , where (a) \\(\\hat\\alpha = \\bU\\bD^{-1}\\hat\\beta\\) , and (b) \\(\\hat\\beta\\) is the ridged logistic regression estimate with input matrix \\(\\bb{R}=\\bU\\bD\\) . Argue that the same approach can be used for any appropriate kernel matrix \\(\\bb{K}\\) . Soln. 18.13 Let \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) be defined as in (a) and (b). Note that \\[\\begin{eqnarray} \\hat f_0 &=& k_0^T\\hat\\alpha\\non\\\\ &=& x_0^T\\bb{X}^T\\bU\\bD^{-1}\\hat\\beta\\non\\\\ &=&x_0^T\\bb{V}\\bb{D}\\bb{U}^T\\bb{U}\\bb{D}^{-1}\\hat\\beta\\non\\\\ &=&x_0^T\\bb{V}\\hat\\beta,\\non \\end{eqnarray}\\] which is exact the same as prediction given by (18.15) in the text (see Ex. 18.4 for proof). For any kernel matrix \\(\\bb{K}\\) , the proof follows the same logic except that we replace the explicit \\(\\bb{X}\\bb{X}^T\\) by \\(\\bb{K}\\) . See Ex. 5.16 for more details.","title":"Ex. 18.13"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-14/","text":"Ex. 18.14 Distance weighted 1-NN classification . Consider the 1-nearest-neighbor method (Section 13.3) in a two-class classification problem. Let \\(d_+(x_0)\\) be the shortest distance to a training observation in class +1, and likewise \\(d_-(x_0)\\) the shortest distance for class -1. Let \\(N_-\\) be the number of samples in class -1, \\(N_+\\) the number in class \\(+1\\) , and \\(N=N_- + N_+\\) . (a) Show that \\[\\begin{equation} \\delta(x_0) = \\log \\frac{d_-(x_0)}{d_+(x_0)}\\non \\end{equation}\\] can be viewed as a nonparametric discriminant function corresponding to \\(1\\) -NN classification. [ Hint : Show that \\(\\hat f_+(x_0)=\\frac{1}{N_+d_+(x_0)}\\) can be viewed as a nonparametric estimate of the density in class +1 at \\(x_0\\) ]. (b) How would you modify this function to introduce class prior probabilities \\(\\pi_+\\) and \\(\\pi_-\\) different from the sample-priors \\(N_+/N\\) and \\(N_-/N\\) ? (c) How would you generalize this approach for \\(K\\) -NN classification? Soln. 18.14 (a) Note that \\(\\delta(x_0) > 0\\) is equivalent to \\(d_-(x_0) > d_+(x_0)\\) , thus we can assign \\(x_0\\) to class \\(+1\\) . Therefore \\(\\delta (x_0)\\) can be viewed as a nonparametric discriminant function corresponding to \\(1\\) -NN classification. (b) Note that by Bayes formula, we have \\[\\begin{eqnarray} P(x \\in \\text{class 1}|x=x_0) &=& \\frac{P(x=x_0, x\\in \\text{class 1})}{P(x=x_0)} \\non\\\\ &=&\\frac{\\hat f_+(x_0)\\pi_+}{P(x=x_0)}\\non\\\\ &=& \\frac{\\pi_+/N_+d_+(x_0)}{P(x=x_0)}.\\non \\end{eqnarray}\\] Therefore we have \\[\\begin{eqnarray} \\delta(x_0) &=& \\log \\left(\\frac{\\pi_+N_-}{\\pi_-N_+}\\cdot \\frac{d_-(x_0)}{d_+(x_0)}\\right).\\non \\end{eqnarray}\\] (c) For \\(k\\) -NN, given \\(x_0\\) , compute the distances \\(d(x_0, x_k)\\) between \\(x_0\\) and \\(K\\) closest training samples \\(x_k\\) for \\(k=1,...,K\\) . Then we can choose nonparametric discriminant function as \\[\\begin{equation} \\delta(x_0) = \\frac{\\sum_{k=1}^Kw_k y_k}{\\sum_{k=1}^Kw_k}\\non \\end{equation}\\] where \\(w_k\\) represents the weights and in our case \\(w_k = \\frac{1}{d(x_0, x_k)}\\) for \\(k=1,...,K\\) .","title":"Ex. 18.14"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-15/","text":"Ex. 18.15 Kernel PCA . In Section 18.5.2 we show how to compute the principal component variables \\(\\bb{Z}\\) from an uncentered inner-product matrix \\(\\bb{K}\\) . We compute the eigen-decomposition \\((\\bI-\\bb{M})\\bb{K}(\\bI-\\bb{M})=\\bU\\bD^2\\bU^T\\) , with \\(\\bb{M}=\\bb{1}\\bb{1}^T/N\\) , and then \\(\\bb{Z}=\\bU\\bD\\) . Suppose we have the inner-product vector \\(\\bb{k}_0\\) , containing the \\(N\\) inner-products between a new point \\(x_0\\) and each of the \\(x_i\\) in our training set. Show that the (centered) projections of \\(x_0\\) onto the principal-component directions are given by \\[\\begin{equation} \\bb{z}_0=\\bD^{-1}\\bU^T(\\bI-\\bb{M})[\\bb{k}_0-\\bb{K}\\bb{1}/N].\\non \\end{equation}\\] Soln. 18.15 Note that \\(\\tilde{\\mathbb{X}} = (\\bI-\\bb{M})\\bX = \\bb{U}\\bb{D}\\bb{V}^T\\) , we have \\((\\bI-\\bb{M})\\bX\\bb{V} = \\bb{U}\\bb{D}\\) since \\(\\bb{V}^T\\bb{V}=\\bb{I}\\) . Thus we can write \\[\\begin{equation} \\bb{z}_0 = \\bb{V}^T(\\bb{x}_0-\\bar x)\\in \\mathbb{R}^{N\\times 1}.\\non \\end{equation}\\] On the other hand, we have \\[\\begin{eqnarray} \\bb{U}\\bb{D}\\bb{z}_0 &=& \\bb{U}\\bb{D}\\bb{V}^T(\\bb{x}_0-\\bar x)\\non\\\\ &=& (\\bI-\\bb{M})\\bX (\\bb{x}_0-\\bar x)\\non\\\\ & =& (\\bI-\\bb{M})(\\bb{k}_0-\\bb{K}\\bb{1}/N).\\non \\end{eqnarray}\\] Therefore we obtain \\[\\begin{equation} \\bb{z}_0=\\bD^{-1}\\bU^T(\\bI-\\bb{M})[\\bb{k}_0-\\bb{K}\\bb{1}/N].\\non \\end{equation}\\]","title":"Ex. 18.15"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-16/","text":"Ex. 18.16 Bonferroni method for multiple comparisons . Suppose we are in a multiple-testing scenario with null hypotheses \\(H_{0j}, j=1,2,...,M,\\) and corresponding \\(p\\) -values \\(p_j, j=1,2,...,M\\) . Let \\(A\\) be the event that at least one null hypothesis is falsely rejected, and let \\(A_j\\) be the event that the \\(j\\) th null hypothesis is falsely rejected. Suppose that we use the Bonferroni method, rejecting the \\(j\\) th null hypothesis if \\(p_j < \\alpha/M\\) . (a) Show that \\(\\text{Pr}(A)\\le \\alpha\\) . [ Hint : \\(\\text{Pr}(A_j\\cup A_{j'})=\\text{Pr}(A_j) + \\text{Pr}(A_{j'}) - \\text{Pr}(A_j\\cap A_{j'})\\) ] (b) If the hypothesess \\(H_{0j}, j=1,2,...,M\\) , are independent, then \\(\\text{Pr}(A)=1-\\text{Pr}(A^C)=1-\\prod_{j=1}^M\\text{Pr}(A_j^C)=1-(1-\\alpha/M)^M\\) . Use this to show that \\(\\text{Pr}(A)\\approx \\alpha\\) in this case. Soln. 18.16 (a) We have \\[\\begin{eqnarray} \\text{Pr}(A) = \\text{Pr}\\left(\\cup_{j=1}^M A_j\\right) \\le \\sum_{j=1}^M\\text{Pr}(A_j) = M \\cdot \\frac{\\alpha}{M} = \\alpha.\\non \\end{eqnarray}\\] (b) If follows directly from the fact that \\((1-\\alpha/M)^M\\approx 1-\\alpha\\) when \\(\\alpha\\) is small or \\(M\\) is large.","title":"Ex. 18.16"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-17/","text":"Ex. 18.17 Equivalence between Benjamini\u2013Hochberg and plug-in methods. (a) In the notation of Algorithm 18.2, show that for rejection threshold \\(p_0=p_{(L)}\\) , a proportion of at most \\(p_0\\) of the permuted values \\(t_j^k\\) exceed \\(|T|_{(L)}\\) where \\(|T|_{(L)}\\) is the \\(L\\) th largest value among the \\(|t_j|\\) . Hence show that the plug-in FDR estimate \\(\\widehat{\\text{FDR}}\\) is less than or equal to \\(p_0\\cdot M/L = \\alpha\\) . (b) Show that the cut-point \\(|T|_{(L+1)}\\) produces a test with estimated FDR greater than \\(\\alpha\\) . Soln. 18.17 (a) Note that \\(p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(M)}\\) and the definition of \\(p_j\\) in (18.41), we know \\(p_{(L)}\\) corresponds to \\(T_{(L)}\\) , that is, \\[\\begin{equation} p_0 = p_{(L)} = \\frac{1}{MK}\\sum_{j' = 1}^M \\sum_{k=1}^K \\bb{I}(|t_{j'}^k| > t_{(L)}).\\non \\end{equation}\\] Therefore, the proportion of the permuted values \\(t_j^k\\) exceed \\(|T|_{(L)}\\) is at most \\(p_0\\) . Recall (18.46) in Algorithm 18.3, we have \\[\\begin{eqnarray} \\widehat{\\text{FDR}} &=& \\frac{\\widehat{E(V)}}{R_{\\text{obs}}}\\non\\\\ &=&\\frac{\\frac{1}{K}\\sum_{j=1}^M\\sum_{k=1}^K\\bb{I}(|t_j^k| > |T|_{(L)})}{\\sum_{j=1}^M\\bb{I}(|t_j| > |T|_{(L)})}\\non\\\\ &\\le &\\frac{p_0\\cdot M}{L-1}\\non\\\\ &=&\\alpha. \\label{eq:18-17a} \\end{eqnarray}\\] Note that the last equality assumes \\(\\alpha = \\frac{p_0\\cdot M}{L-1}\\) instead of \\(\\frac{p_0\\cdot M}{L}\\) defined in the text. Otherwise I don't see how to prove the claimed result. (b) It follows the same arguments as \\(\\eqref{eq:18-17a}\\) by noting the the proportion of the permuted values \\(t_j^k\\) exceed \\(|T|_{(L+1)}\\) is greater than \\(p_0\\) .","title":"Ex. 18.17"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-18/","text":"Ex. 18.18 Use result (18.53) to show that \\[\\begin{equation} \\text{pFDR} = \\frac{\\pi_0\\cdot \\{\\text{Type I error of } \\Gamma\\} }{\\pi_0\\cdot \\{\\text{Type I error of } \\Gamma\\} + \\pi_1\\cdot\\{\\text{Power of }\\Gamma\\}}\\non \\end{equation}\\] (Storey, 2003 \\cite{storey2003positive}). Soln. 18.18 From (18.53) in the text we obtain \\[\\begin{eqnarray} \\text{pFDR}(\\Gamma) &=& \\text{Pr}(Z_j=0|t_j\\in \\Gamma)\\non\\\\ &=&\\frac{\\pi_0\\cdot\\text{Pr}(t_j\\in \\Gamma|Z_j=0)}{\\pi_0\\cdot\\text{Pr}(t_j\\in \\Gamma|Z_j=0) + \\pi_1\\cdot\\text{Pr}(t_j\\in \\Gamma|Z_j=1)}\\non\\\\ &=&\\frac{\\pi_0\\cdot \\{\\text{Type I error of } \\Gamma\\} }{\\pi_0\\cdot \\{\\text{Type I error of } \\Gamma\\} + \\pi_1\\cdot\\{\\text{Power of }\\Gamma\\}}.\\non \\end{eqnarray}\\]","title":"Ex. 18.18"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-19/","text":"Ex. 18.19 Consider the data in Table 18.4 of Section (18.7), available from the book website. (a) Using a symmetric two-sided rejection region based on the \\(t\\) -statistic, compute the plug-in estimate of the FDR for various values of the cut-point. (b) Carry out the BH procedure for various FDR levels \\(\\alpha\\) and show the equivalence of your results, with those from part (a). (c) Let \\((q_{.25}, q_{.75})\\) be the quartiles of the \\(t\\) -statistics from the permuted datasets. Let \\(\\hat\\pi_0 = \\{\\# t_j\\in (q_{.25}, q_{.75})\\}/(.5M)\\) , and set \\(\\hat\\pi_0=\\min(\\hat\\pi_0, 1)\\) . Multiply the FDR estimates from (a) by \\(\\hat\\pi_0\\) and examine the results. (d) Give a motivation for the estimate in part (c). (Storey, 2003 \\cite{storey2003positive}). Soln. 18.19 See Code below for (a)- (c). (d) This is an estimate of \\(M_0/M\\) in (18.45). Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import pandas as pd import numpy as np from scipy import stats import statsmodels.stats.multitest as mt import pathlib import timeit # get relative data folder PATH = pathlib . Path ( __file__ ) . resolve () . parents [ 1 ] DATA_PATH = PATH . joinpath ( \"data\" ) . resolve () # data X = pd . read_csv ( DATA_PATH . joinpath ( \"microarray.csv\" ), header = 0 ) def t_wrapper ( arr , ind1 = None , ind2 = None ): s1 = np . asarray ( arr [ ind1 ]) s2 = np . asarray ( arr [ ind2 ]) return stats . ttest_ind ( s1 , s2 ) . statistic def perm_t_wrapper ( arr , perms ): res = [] for perm in perms : ind1 = perm [: 44 ] ind2 = perm [ 44 :] t = t_wrapper ( arr , ind1 = ind1 , ind2 = ind2 ) res . append ( t ) return res def get_t_stats ( X , ind1 , ind2 ): res = X . apply ( t_wrapper , axis = 1 , ind1 = ind1 , ind2 = ind2 ) return np . asarray ( res ) def get_tjk ( X , K = 1000 ): perms = [] for _ in range ( K ): perms . append ( np . random . permutation ( 58 )) res = X . apply ( perm_t_wrapper , axis = 1 , perms = perms ) return res def get_FDR ( t_orig , tjk , C , K ): R_obs = ( abs ( t_orig ) > C ) . sum () E_V = ( abs ( tjk ) > C ) . sum () / K FDR = E_V / R_obs return FDR def get_p_value ( t_orig , tjk ): p , K = tjk . shape p_values = np . arange ( p ) for p_ in range ( p ): p_values [ p_ ] = np . count_nonzero ( tjk [ p_ ] > t_orig [ p_ ]) / K p_values . sort () return p_values normal_indices = np . arange ( 44 ) sensitive_indices = np . arange ( 44 , 58 ) t_orig = get_t_stats ( X , normal_indices , sensitive_indices ) tjk = get_tjk ( X , K = 1000 ) # ~ 20mins C = [ 4 , 4.101 , 4.2 ] plugin_FDR = [] for c in C : plugin_FDR . append ( get_FDR ( t_orig , np . asarray ( tjk ), c )) p_values = get_p_value ( t_orig , np . asarray ( tjk )) alphas = plugin_FDR L = [] for alpha in alphas : res = mt . multipletests ( p_values , alpha , is_sorted = True ) L . append ( np . count_nonzero ( res . reject )) print ( L )","title":"Ex. 18.19"},{"location":"ESL-Solution/_18-High-Dimensional-Problems/ex18-20/","text":"Ex. 18.20 Proof of result (18.53) . Write \\[\\begin{eqnarray} \\text{pFDR} &=& E\\left(\\frac{V}{R}|R > 0\\right)\\non\\\\ &=&\\sum_{k=1}^ME\\left[\\frac{V}{R}|R=k\\right]\\text{Pr}(R=k|R > 0)\\non \\end{eqnarray}\\] Use the fact that given \\(R=k\\) , \\(V\\) is a binomial random variable, with \\(k\\) trials and probability of success \\(\\text{Pr}(H=0|T\\in \\Gamma)\\) , to complete the proof. Soln. 18.20 Note that given \\(R=k\\) , \\(V\\) is a binomial random variable, with \\(k\\) trials and probability of success \\(\\text{Pr}(H=0|T\\in \\Gamma)\\) , we have \\[\\begin{equation} E[V|R=k] = k \\cdot \\text{Pr}(H=0|T\\in \\Gamma).\\non \\end{equation}\\] Therefore, \\[\\begin{eqnarray} \\text{pFDR}(\\Gamma) &=&\\sum_{k=1}^M\\frac{1}{k} \\cdot k \\cdot \\text{Pr}(H=0|T\\in \\Gamma)\\text{Pr}(R=k|R > 0)\\non\\\\ &=&\\text{Pr}(H=0|T\\in \\Gamma)\\sum_{k=1}^M\\text{Pr}(R=k|R > 0)\\non\\\\ &=&\\text{Pr}(H=0|T\\in \\Gamma).\\non \\end{eqnarray}\\] Note the notation \\(H\\) comes from \\cite{storey2003positive}, and it should be \\(Z\\) defined (18.51) in our context.","title":"Ex. 18.20"}]}